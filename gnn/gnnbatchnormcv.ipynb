{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:27.973596Z",
     "iopub.status.busy": "2025-02-22T09:26:27.973124Z",
     "iopub.status.idle": "2025-02-22T09:26:33.213758Z",
     "shell.execute_reply": "2025-02-22T09:26:33.212641Z",
     "shell.execute_reply.started": "2025-02-22T09:26:27.973562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.10)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.9.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:33.215801Z",
     "iopub.status.busy": "2025-02-22T09:26:33.215534Z",
     "iopub.status.idle": "2025-02-22T09:26:37.699680Z",
     "shell.execute_reply": "2025-02-22T09:26:37.698742Z",
     "shell.execute_reply.started": "2025-02-22T09:26:33.215775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv, BatchNorm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import networkx as nx\n",
    "from torch import nn\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score, train_test_split\n",
    "from sklearn.metrics import f1_score, homogeneity_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:37.701471Z",
     "iopub.status.busy": "2025-02-22T09:26:37.700861Z",
     "iopub.status.idle": "2025-02-22T09:26:37.768283Z",
     "shell.execute_reply": "2025-02-22T09:26:37.767349Z",
     "shell.execute_reply.started": "2025-02-22T09:26:37.701436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding = np.load(\"/Users/arjuns/Downloads/code_res_paper/paper-results-code/gnn/ds/v1/graph2vec_embeddings.npy\")\n",
    "labels = np.load(\"/Users/arjuns/Downloads/code_res_paper/paper-results-code/gnn/ds/v1/graph2vec_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:37.769554Z",
     "iopub.status.busy": "2025-02-22T09:26:37.769230Z",
     "iopub.status.idle": "2025-02-22T09:26:37.775626Z",
     "shell.execute_reply": "2025-02-22T09:26:37.774807Z",
     "shell.execute_reply.started": "2025-02-22T09:26:37.769525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:37.776730Z",
     "iopub.status.busy": "2025-02-22T09:26:37.776457Z",
     "iopub.status.idle": "2025-02-22T09:26:37.811278Z",
     "shell.execute_reply": "2025-02-22T09:26:37.810635Z",
     "shell.execute_reply.started": "2025-02-22T09:26:37.776696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def delete_all_contents(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: {folder_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    for item in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        \n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.unlink(item_path)  # Delete file or symlink\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)  # Delete directory and its contents\n",
    "            print(f\"Deleted: {item_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {item_path}: {e}\")\n",
    "\n",
    "delete_all_contents(\"/kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:37.813512Z",
     "iopub.status.busy": "2025-02-22T09:26:37.813264Z",
     "iopub.status.idle": "2025-02-22T09:26:37.822445Z",
     "shell.execute_reply": "2025-02-22T09:26:37.821635Z",
     "shell.execute_reply.started": "2025-02-22T09:26:37.813490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:37.830636Z",
     "iopub.status.busy": "2025-02-22T09:26:37.830294Z",
     "iopub.status.idle": "2025-02-22T09:26:37.845256Z",
     "shell.execute_reply": "2025-02-22T09:26:37.844585Z",
     "shell.execute_reply.started": "2025-02-22T09:26:37.830603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NodeClassificationGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_channel, num_classes, dropout=0.3, activation=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        self.activation = activation()  # Initialize activation function\n",
    "        self.conv1 = SAGEConv(input_dim, hidden_channel)\n",
    "        self.bn1 = BatchNorm(hidden_channel)  # BatchNorm for first layer\n",
    "        self.conv2 = SAGEConv(hidden_channel, hidden_channel)\n",
    "        self.bn2 = BatchNorm(hidden_channel)  # BatchNorm for second layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # MLP for final classification\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_channel, hidden_channel),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channel, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First convolutional layer with batch normalization and activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second convolutional layer with batch normalization and activation\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through the MLP for final predictions\n",
    "        x = self.mlp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:44.720192Z",
     "iopub.status.busy": "2025-02-22T09:26:44.719861Z",
     "iopub.status.idle": "2025-02-22T09:26:44.729096Z",
     "shell.execute_reply": "2025-02-22T09:26:44.728314Z",
     "shell.execute_reply.started": "2025-02-22T09:26:44.720160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_similarity_graph(embeddings, threshold):\n",
    "    num_nodes = embeddings.shape[0]\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for i in range(num_nodes):\n",
    "        G.add_node(i, embedding=embeddings[i])\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            if similarity_matrix[i, j] >= threshold:\n",
    "                G.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "\n",
    "    return G\n",
    "\n",
    "def convert_to_torch_geometric(graph):\n",
    "    node_features = np.array([graph.nodes[n]['embedding'] for n in graph.nodes])\n",
    "    edge_index = np.array(list(graph.edges)).T\n",
    "    edge_weights = np.array([graph[u][v]['weight'] for u, v in graph.edges])\n",
    "\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights)\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_data(data, labels, train_ratio=0.7, val_ratio=0.2, random_state=42):\n",
    "    train_idx, temp_idx, train_labels, temp_labels = train_test_split(\n",
    "        np.arange(len(labels)), labels, test_size=(1 - train_ratio), stratify=labels,  random_state=random_state\n",
    "    )\n",
    "    val_size = val_ratio / (1 - train_ratio)\n",
    "    val_idx, test_idx, val_labels, test_labels = train_test_split(\n",
    "        temp_idx, temp_labels, test_size=(1 - val_size), stratify=temp_labels, random_state=random_state\n",
    "    )\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "def load_best_model(model, threshold, save_path='checkpoints'):\n",
    "    checkpoint = torch.load(os.path.join(save_path, f'best_model_threshold_{threshold:.2f}.pt'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return checkpoint['best_val_loss']\n",
    "\n",
    "def save_training_history(history, threshold, save_path='training_history'):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    with open(os.path.join(save_path, f'history_threshold_{threshold:.2f}.json'), 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:45.512228Z",
     "iopub.status.busy": "2025-02-22T09:26:45.511884Z",
     "iopub.status.idle": "2025-02-22T09:26:45.520640Z",
     "shell.execute_reply": "2025-02-22T09:26:45.519693Z",
     "shell.execute_reply.started": "2025-02-22T09:26:45.512196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(optimizer, criterion, model, data, _, threshold, save_path='checkpoints', num_epochs=300):\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                         factor=0.5, patience=10, \n",
    "                                                         min_lr=1e-6, verbose=True)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = int(pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
    "        train_acc = correct / data.train_mask.sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data.x, data.edge_index)\n",
    "            val_loss = criterion(val_out[data.val_mask], data.y[data.val_mask])\n",
    "            val_pred = val_out.argmax(dim=1)\n",
    "            val_correct = int(val_pred[data.val_mask].eq(data.y[data.val_mask]).sum().item())\n",
    "            val_acc = val_correct / data.val_mask.sum().item()\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, os.path.join(save_path, f'best_model_threshold_{threshold:.2f}.pt'))\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
    "                  f\"Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "    save_training_history(history, threshold)\n",
    "    \n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return best_val_loss, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:46.764292Z",
     "iopub.status.busy": "2025-02-22T09:26:46.763994Z",
     "iopub.status.idle": "2025-02-22T09:26:46.776325Z",
     "shell.execute_reply": "2025-02-22T09:26:46.775504Z",
     "shell.execute_reply.started": "2025-02-22T09:26:46.764268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_graphs_and_setup_model_cv(embedding, labels, thresholds=np.arange(0.05, 0.16, 0.01), n_splits=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    best_threshold = None\n",
    "    best_avg_test_acc = 0\n",
    "    all_results = {}\n",
    "\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        np.arange(len(labels)), test_size=0.3, stratify=labels, random_state=42\n",
    "    )\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nTraining with threshold: {threshold}\")\n",
    "        similarity_graph = build_similarity_graph(embedding, threshold=threshold)\n",
    "        data = convert_to_torch_geometric(similarity_graph)\n",
    "        data.y = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        test_mask = torch.zeros(data.num_nodes, dtype=torch.bool).to(device)\n",
    "        test_mask[test_idx] = True\n",
    "        data.test_mask = test_mask\n",
    "        data = data.to(device)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        fold_results = []\n",
    "\n",
    "        for fold, (train_sub_idx, val_idx) in enumerate(skf.split(train_idx, labels[train_idx])):\n",
    "            print(f\"  Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "            train_mask = torch.zeros(data.num_nodes, dtype=torch.bool).to(device)\n",
    "            val_mask = torch.zeros(data.num_nodes, dtype=torch.bool).to(device)\n",
    "            train_mask[train_idx[train_sub_idx]] = True\n",
    "            val_mask[train_idx[val_idx]] = True\n",
    "            data.train_mask = train_mask\n",
    "            data.val_mask = val_mask\n",
    "\n",
    "            input_dim = 128\n",
    "            hidden_channels = 256\n",
    "            out_ch = len(np.unique(labels))\n",
    "            model = NodeClassificationGNN(\n",
    "                input_dim=input_dim, hidden_channel=hidden_channels, num_classes=out_ch, dropout=0.7\n",
    "            ).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            _, history = train(optimizer, criterion, model, data, labels, threshold)\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_out = model(data.x, data.edge_index)\n",
    "                test_pred = test_out.argmax(dim=1).cpu().numpy()\n",
    "                test_true = data.y[data.test_mask].cpu().numpy()\n",
    "                test_pred_filtered = test_pred[data.test_mask.cpu().numpy()]\n",
    "\n",
    "                test_correct = (test_pred_filtered == test_true).sum()\n",
    "                test_acc = test_correct / len(test_true)\n",
    "                test_f1 = f1_score(test_true, test_pred_filtered, average='weighted')\n",
    "                test_homogeneity = homogeneity_score(test_true, test_pred_filtered)\n",
    "                test_nmi = normalized_mutual_info_score(test_true, test_pred_filtered)\n",
    "                test_ari = adjusted_rand_score(test_true, test_pred_filtered)\n",
    "\n",
    "            print(f\"Fold Test Accuracy: {test_acc:.4f}, F1-score: {test_f1:.4f}, Homogeneity: {test_homogeneity:.4f}, \"\n",
    "                  f\"NMI: {test_nmi:.4f}, ARI: {test_ari:.4f}\")\n",
    "\n",
    "            fold_results.append({\n",
    "                'test_accuracy': test_acc,\n",
    "                'f1_score': test_f1,\n",
    "                'homogeneity': test_homogeneity,\n",
    "                'nmi': test_nmi,\n",
    "                'ari': test_ari,\n",
    "            })\n",
    "\n",
    "        avg_results = {metric: sum(fold[metric] for fold in fold_results) / n_splits for metric in fold_results[0]}\n",
    "        print(f\"  Average Metrics for Threshold {threshold:.2f}: {avg_results}\")\n",
    "\n",
    "        all_results[threshold] = avg_results\n",
    "        if avg_results['test_accuracy'] > best_avg_test_acc:\n",
    "            best_avg_test_acc = avg_results['test_accuracy']\n",
    "            best_threshold = threshold\n",
    "\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "\n",
    "    with open('results/cv_results.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'best_threshold': best_threshold,\n",
    "            'best_avg_test_accuracy': best_avg_test_acc,\n",
    "            'all_thresholds_results': {str(k): v for k, v in all_results.items()}\n",
    "        }, f, indent=4)\n",
    "\n",
    "    print(f\"\\nBest Threshold: {best_threshold:.2f}, Best Average Test Accuracy: {best_avg_test_acc:.4f}\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:26:54.283634Z",
     "iopub.status.busy": "2025-02-22T09:26:54.283243Z",
     "iopub.status.idle": "2025-02-22T10:33:58.299816Z",
     "shell.execute_reply": "2025-02-22T10:33:58.299064Z",
     "shell.execute_reply.started": "2025-02-22T09:26:54.283602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with threshold: 0.05\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7029, Val Loss: 2.5638, Acc: 0.0760, Val Acc: 0.0824\n",
      "Epoch 10, Loss: 2.4625, Val Loss: 2.4312, Acc: 0.1620, Val Acc: 0.2301\n",
      "Epoch 20, Loss: 2.1954, Val Loss: 2.1549, Acc: 0.2345, Val Acc: 0.2585\n",
      "Epoch 30, Loss: 1.9779, Val Loss: 1.8776, Acc: 0.2843, Val Acc: 0.3267\n",
      "Epoch 40, Loss: 1.7424, Val Loss: 1.6431, Acc: 0.3276, Val Acc: 0.3892\n",
      "Epoch 50, Loss: 1.5365, Val Loss: 1.4133, Acc: 0.4115, Val Acc: 0.5170\n",
      "Epoch 60, Loss: 1.3705, Val Loss: 1.2065, Acc: 0.4733, Val Acc: 0.6136\n",
      "Epoch 70, Loss: 1.2163, Val Loss: 1.0476, Acc: 0.5259, Val Acc: 0.6420\n",
      "Epoch 80, Loss: 1.0577, Val Loss: 0.9087, Acc: 0.5714, Val Acc: 0.6761\n",
      "Epoch 90, Loss: 0.9194, Val Loss: 0.7965, Acc: 0.6382, Val Acc: 0.6960\n",
      "Epoch 100, Loss: 0.8325, Val Loss: 0.7263, Acc: 0.6532, Val Acc: 0.7358\n",
      "Epoch 110, Loss: 0.7607, Val Loss: 0.6871, Acc: 0.7015, Val Acc: 0.7216\n",
      "Epoch 120, Loss: 0.6837, Val Loss: 0.6853, Acc: 0.7264, Val Acc: 0.6960\n",
      "Epoch 130, Loss: 0.6069, Val Loss: 0.6774, Acc: 0.7541, Val Acc: 0.6960\n",
      "Epoch 140, Loss: 0.6061, Val Loss: 0.6233, Acc: 0.7441, Val Acc: 0.7244\n",
      "Epoch 150, Loss: 0.5407, Val Loss: 0.5729, Acc: 0.7889, Val Acc: 0.7670\n",
      "Epoch 160, Loss: 0.5485, Val Loss: 0.5587, Acc: 0.7882, Val Acc: 0.7727\n",
      "Epoch 170, Loss: 0.5013, Val Loss: 0.5354, Acc: 0.8017, Val Acc: 0.7727\n",
      "Epoch 180, Loss: 0.5053, Val Loss: 0.5374, Acc: 0.8109, Val Acc: 0.7812\n",
      "Epoch 190, Loss: 0.4718, Val Loss: 0.5306, Acc: 0.8209, Val Acc: 0.7869\n",
      "Epoch 200, Loss: 0.4757, Val Loss: 0.5291, Acc: 0.8138, Val Acc: 0.7727\n",
      "Epoch 210, Loss: 0.4576, Val Loss: 0.5182, Acc: 0.8202, Val Acc: 0.7812\n",
      "Epoch 220, Loss: 0.4391, Val Loss: 0.5145, Acc: 0.8252, Val Acc: 0.7955\n",
      "Epoch 230, Loss: 0.4316, Val Loss: 0.5227, Acc: 0.8358, Val Acc: 0.7670\n",
      "Epoch 240, Loss: 0.4384, Val Loss: 0.5139, Acc: 0.8330, Val Acc: 0.7898\n",
      "Epoch 250, Loss: 0.4077, Val Loss: 0.5122, Acc: 0.8522, Val Acc: 0.7926\n",
      "Epoch 260, Loss: 0.4282, Val Loss: 0.5094, Acc: 0.8429, Val Acc: 0.7926\n",
      "Epoch 270, Loss: 0.4048, Val Loss: 0.5074, Acc: 0.8451, Val Acc: 0.7955\n",
      "Epoch 280, Loss: 0.4433, Val Loss: 0.5039, Acc: 0.8216, Val Acc: 0.7926\n",
      "Epoch 290, Loss: 0.4008, Val Loss: 0.5040, Acc: 0.8458, Val Acc: 0.7869\n",
      "Best Validation Loss: 0.4990\n",
      "Fold Test Accuracy: 0.8329, F1-score: 0.8320, Homogeneity: 0.8112, NMI: 0.8167, ARI: 0.7124\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7067, Val Loss: 2.5618, Acc: 0.0675, Val Acc: 0.0710\n",
      "Epoch 10, Loss: 2.4522, Val Loss: 2.4272, Acc: 0.1599, Val Acc: 0.2415\n",
      "Epoch 20, Loss: 2.1887, Val Loss: 2.1236, Acc: 0.2416, Val Acc: 0.3523\n",
      "Epoch 30, Loss: 1.9564, Val Loss: 1.8355, Acc: 0.2765, Val Acc: 0.3438\n",
      "Epoch 40, Loss: 1.7493, Val Loss: 1.5720, Acc: 0.3255, Val Acc: 0.4205\n",
      "Epoch 50, Loss: 1.5260, Val Loss: 1.3550, Acc: 0.4030, Val Acc: 0.5170\n",
      "Epoch 60, Loss: 1.3523, Val Loss: 1.1750, Acc: 0.4634, Val Acc: 0.5852\n",
      "Epoch 70, Loss: 1.1915, Val Loss: 1.0157, Acc: 0.5160, Val Acc: 0.6534\n",
      "Epoch 80, Loss: 1.0505, Val Loss: 0.8633, Acc: 0.5672, Val Acc: 0.7188\n",
      "Epoch 90, Loss: 0.9302, Val Loss: 0.7713, Acc: 0.6340, Val Acc: 0.7159\n",
      "Epoch 100, Loss: 0.8401, Val Loss: 0.7044, Acc: 0.6581, Val Acc: 0.7557\n",
      "Epoch 110, Loss: 0.7645, Val Loss: 0.6560, Acc: 0.6780, Val Acc: 0.7557\n",
      "Epoch 120, Loss: 0.6991, Val Loss: 0.5886, Acc: 0.7157, Val Acc: 0.7756\n",
      "Epoch 130, Loss: 0.6321, Val Loss: 0.5542, Acc: 0.7420, Val Acc: 0.7699\n",
      "Epoch 140, Loss: 0.5955, Val Loss: 0.5409, Acc: 0.7591, Val Acc: 0.7614\n",
      "Epoch 150, Loss: 0.5466, Val Loss: 0.5008, Acc: 0.7875, Val Acc: 0.7812\n",
      "Epoch 160, Loss: 0.5221, Val Loss: 0.4600, Acc: 0.8003, Val Acc: 0.8040\n",
      "Epoch 170, Loss: 0.4693, Val Loss: 0.4166, Acc: 0.8095, Val Acc: 0.8068\n",
      "Epoch 180, Loss: 0.4388, Val Loss: 0.3884, Acc: 0.8273, Val Acc: 0.8352\n",
      "Epoch 190, Loss: 0.4269, Val Loss: 0.3913, Acc: 0.8436, Val Acc: 0.8295\n",
      "Epoch 200, Loss: 0.3671, Val Loss: 0.4105, Acc: 0.8564, Val Acc: 0.8210\n",
      "Epoch 210, Loss: 0.3691, Val Loss: 0.3704, Acc: 0.8664, Val Acc: 0.8352\n",
      "Epoch 220, Loss: 0.3446, Val Loss: 0.3738, Acc: 0.8756, Val Acc: 0.8466\n",
      "Epoch 230, Loss: 0.3163, Val Loss: 0.3774, Acc: 0.8820, Val Acc: 0.8381\n",
      "Epoch 240, Loss: 0.3173, Val Loss: 0.3533, Acc: 0.8827, Val Acc: 0.8494\n",
      "Epoch 250, Loss: 0.3066, Val Loss: 0.3485, Acc: 0.8920, Val Acc: 0.8523\n",
      "Epoch 260, Loss: 0.2994, Val Loss: 0.3515, Acc: 0.8977, Val Acc: 0.8608\n",
      "Epoch 270, Loss: 0.3052, Val Loss: 0.3527, Acc: 0.8913, Val Acc: 0.8608\n",
      "Epoch 280, Loss: 0.3329, Val Loss: 0.3560, Acc: 0.8806, Val Acc: 0.8523\n",
      "Epoch 290, Loss: 0.2957, Val Loss: 0.3581, Acc: 0.8920, Val Acc: 0.8494\n",
      "Best Validation Loss: 0.3473\n",
      "Fold Test Accuracy: 0.8422, F1-score: 0.8394, Homogeneity: 0.8324, NMI: 0.8355, ARI: 0.7395\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6946, Val Loss: 2.5647, Acc: 0.0725, Val Acc: 0.1051\n",
      "Epoch 10, Loss: 2.4658, Val Loss: 2.4229, Acc: 0.1493, Val Acc: 0.2017\n",
      "Epoch 20, Loss: 2.2010, Val Loss: 2.1305, Acc: 0.2189, Val Acc: 0.3011\n",
      "Epoch 30, Loss: 1.9507, Val Loss: 1.8279, Acc: 0.3049, Val Acc: 0.4290\n",
      "Epoch 40, Loss: 1.6945, Val Loss: 1.5296, Acc: 0.3582, Val Acc: 0.4716\n",
      "Epoch 50, Loss: 1.4827, Val Loss: 1.2892, Acc: 0.4165, Val Acc: 0.5682\n",
      "Epoch 60, Loss: 1.3134, Val Loss: 1.1009, Acc: 0.4790, Val Acc: 0.6250\n",
      "Epoch 70, Loss: 1.1629, Val Loss: 0.9495, Acc: 0.5402, Val Acc: 0.6648\n",
      "Epoch 80, Loss: 1.0271, Val Loss: 0.8157, Acc: 0.5928, Val Acc: 0.6989\n",
      "Epoch 90, Loss: 0.9011, Val Loss: 0.7380, Acc: 0.6418, Val Acc: 0.7244\n",
      "Epoch 100, Loss: 0.8162, Val Loss: 0.6693, Acc: 0.6716, Val Acc: 0.7301\n",
      "Epoch 110, Loss: 0.7313, Val Loss: 0.6402, Acc: 0.7107, Val Acc: 0.7500\n",
      "Epoch 120, Loss: 0.6753, Val Loss: 0.6234, Acc: 0.7221, Val Acc: 0.7415\n",
      "Epoch 130, Loss: 0.6365, Val Loss: 0.5486, Acc: 0.7399, Val Acc: 0.7955\n",
      "Epoch 140, Loss: 0.5732, Val Loss: 0.5980, Acc: 0.7733, Val Acc: 0.7557\n",
      "Epoch 150, Loss: 0.5392, Val Loss: 0.5865, Acc: 0.7903, Val Acc: 0.7557\n",
      "Epoch 160, Loss: 0.5168, Val Loss: 0.4838, Acc: 0.7861, Val Acc: 0.8182\n",
      "Epoch 170, Loss: 0.4959, Val Loss: 0.4786, Acc: 0.8053, Val Acc: 0.8182\n",
      "Epoch 180, Loss: 0.5162, Val Loss: 0.4734, Acc: 0.8095, Val Acc: 0.8097\n",
      "Epoch 190, Loss: 0.4534, Val Loss: 0.4797, Acc: 0.8280, Val Acc: 0.8040\n",
      "Epoch 200, Loss: 0.4485, Val Loss: 0.4705, Acc: 0.8195, Val Acc: 0.8125\n",
      "Epoch 210, Loss: 0.4233, Val Loss: 0.4613, Acc: 0.8365, Val Acc: 0.8153\n",
      "Epoch 220, Loss: 0.4325, Val Loss: 0.4537, Acc: 0.8387, Val Acc: 0.8267\n",
      "Epoch 230, Loss: 0.4100, Val Loss: 0.4495, Acc: 0.8415, Val Acc: 0.8267\n",
      "Epoch 240, Loss: 0.4025, Val Loss: 0.4416, Acc: 0.8586, Val Acc: 0.8295\n",
      "Epoch 250, Loss: 0.4066, Val Loss: 0.4416, Acc: 0.8522, Val Acc: 0.8267\n",
      "Epoch 260, Loss: 0.3924, Val Loss: 0.4408, Acc: 0.8358, Val Acc: 0.8267\n",
      "Epoch 270, Loss: 0.3976, Val Loss: 0.4404, Acc: 0.8571, Val Acc: 0.8267\n",
      "Epoch 280, Loss: 0.3677, Val Loss: 0.4271, Acc: 0.8564, Val Acc: 0.8295\n",
      "Epoch 290, Loss: 0.3961, Val Loss: 0.4256, Acc: 0.8571, Val Acc: 0.8324\n",
      "Best Validation Loss: 0.4248\n",
      "Fold Test Accuracy: 0.8316, F1-score: 0.8300, Homogeneity: 0.8128, NMI: 0.8156, ARI: 0.7152\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7165, Val Loss: 2.5634, Acc: 0.0753, Val Acc: 0.1193\n",
      "Epoch 10, Loss: 2.4558, Val Loss: 2.4214, Acc: 0.1635, Val Acc: 0.2500\n",
      "Epoch 20, Loss: 2.1700, Val Loss: 2.1145, Acc: 0.2452, Val Acc: 0.3068\n",
      "Epoch 30, Loss: 1.9359, Val Loss: 1.8451, Acc: 0.2942, Val Acc: 0.4176\n",
      "Epoch 40, Loss: 1.7223, Val Loss: 1.5608, Acc: 0.3483, Val Acc: 0.4631\n",
      "Epoch 50, Loss: 1.4772, Val Loss: 1.3091, Acc: 0.4222, Val Acc: 0.5739\n",
      "Epoch 60, Loss: 1.3303, Val Loss: 1.1362, Acc: 0.4783, Val Acc: 0.6193\n",
      "Epoch 70, Loss: 1.1223, Val Loss: 0.9982, Acc: 0.5622, Val Acc: 0.6449\n",
      "Epoch 80, Loss: 1.0129, Val Loss: 0.8637, Acc: 0.6127, Val Acc: 0.6676\n",
      "Epoch 90, Loss: 0.8911, Val Loss: 0.7562, Acc: 0.6425, Val Acc: 0.6989\n",
      "Epoch 100, Loss: 0.7920, Val Loss: 0.7723, Acc: 0.6887, Val Acc: 0.6676\n",
      "Epoch 110, Loss: 0.7386, Val Loss: 0.6713, Acc: 0.6958, Val Acc: 0.7188\n",
      "Epoch 120, Loss: 0.6787, Val Loss: 0.6609, Acc: 0.7129, Val Acc: 0.7188\n",
      "Epoch 130, Loss: 0.6133, Val Loss: 0.5938, Acc: 0.7420, Val Acc: 0.7443\n",
      "Epoch 140, Loss: 0.5606, Val Loss: 0.5562, Acc: 0.7761, Val Acc: 0.7812\n",
      "Epoch 150, Loss: 0.5428, Val Loss: 0.5437, Acc: 0.7960, Val Acc: 0.7898\n",
      "Epoch 160, Loss: 0.4719, Val Loss: 0.5541, Acc: 0.8159, Val Acc: 0.7614\n",
      "Epoch 170, Loss: 0.4540, Val Loss: 0.5268, Acc: 0.8316, Val Acc: 0.7841\n",
      "Epoch 180, Loss: 0.4384, Val Loss: 0.5065, Acc: 0.8337, Val Acc: 0.8011\n",
      "Epoch 190, Loss: 0.4158, Val Loss: 0.5040, Acc: 0.8344, Val Acc: 0.7983\n",
      "Epoch 200, Loss: 0.4143, Val Loss: 0.4881, Acc: 0.8436, Val Acc: 0.8125\n",
      "Epoch 210, Loss: 0.3895, Val Loss: 0.4882, Acc: 0.8465, Val Acc: 0.8125\n",
      "Epoch 220, Loss: 0.3613, Val Loss: 0.4777, Acc: 0.8692, Val Acc: 0.8011\n",
      "Epoch 230, Loss: 0.3497, Val Loss: 0.4853, Acc: 0.8621, Val Acc: 0.8125\n",
      "Epoch 240, Loss: 0.3706, Val Loss: 0.4748, Acc: 0.8657, Val Acc: 0.8097\n",
      "Epoch 250, Loss: 0.3649, Val Loss: 0.4698, Acc: 0.8692, Val Acc: 0.8097\n",
      "Epoch 260, Loss: 0.3402, Val Loss: 0.4588, Acc: 0.8792, Val Acc: 0.8182\n",
      "Epoch 270, Loss: 0.3642, Val Loss: 0.4547, Acc: 0.8621, Val Acc: 0.8239\n",
      "Epoch 280, Loss: 0.3037, Val Loss: 0.4541, Acc: 0.8948, Val Acc: 0.8182\n",
      "Epoch 290, Loss: 0.3220, Val Loss: 0.4595, Acc: 0.8884, Val Acc: 0.8239\n",
      "Best Validation Loss: 0.4506\n",
      "Fold Test Accuracy: 0.8263, F1-score: 0.8264, Homogeneity: 0.7961, NMI: 0.7994, ARI: 0.6955\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7511, Val Loss: 2.5575, Acc: 0.0732, Val Acc: 0.1054\n",
      "Epoch 10, Loss: 2.4879, Val Loss: 2.4576, Acc: 0.1634, Val Acc: 0.2536\n",
      "Epoch 20, Loss: 2.1939, Val Loss: 2.1192, Acc: 0.2408, Val Acc: 0.3504\n",
      "Epoch 30, Loss: 1.9668, Val Loss: 1.8125, Acc: 0.2876, Val Acc: 0.3903\n",
      "Epoch 40, Loss: 1.7285, Val Loss: 1.5457, Acc: 0.3246, Val Acc: 0.5043\n",
      "Epoch 50, Loss: 1.5281, Val Loss: 1.3151, Acc: 0.4084, Val Acc: 0.5385\n",
      "Epoch 60, Loss: 1.3043, Val Loss: 1.1072, Acc: 0.4858, Val Acc: 0.6239\n",
      "Epoch 70, Loss: 1.1687, Val Loss: 0.9493, Acc: 0.5249, Val Acc: 0.6809\n",
      "Epoch 80, Loss: 1.0032, Val Loss: 0.8219, Acc: 0.5966, Val Acc: 0.7179\n",
      "Epoch 90, Loss: 0.9111, Val Loss: 0.7375, Acc: 0.6271, Val Acc: 0.7293\n",
      "Epoch 100, Loss: 0.8248, Val Loss: 0.6734, Acc: 0.6676, Val Acc: 0.7436\n",
      "Epoch 110, Loss: 0.7161, Val Loss: 0.6312, Acc: 0.7237, Val Acc: 0.7436\n",
      "Epoch 120, Loss: 0.6529, Val Loss: 0.5902, Acc: 0.7379, Val Acc: 0.7607\n",
      "Epoch 130, Loss: 0.6359, Val Loss: 0.5627, Acc: 0.7393, Val Acc: 0.7635\n",
      "Epoch 140, Loss: 0.5886, Val Loss: 0.4892, Acc: 0.7670, Val Acc: 0.8262\n",
      "Epoch 150, Loss: 0.5517, Val Loss: 0.4701, Acc: 0.7805, Val Acc: 0.8348\n",
      "Epoch 160, Loss: 0.5318, Val Loss: 0.4636, Acc: 0.7976, Val Acc: 0.8177\n",
      "Epoch 170, Loss: 0.5238, Val Loss: 0.4587, Acc: 0.7862, Val Acc: 0.8148\n",
      "Epoch 180, Loss: 0.4936, Val Loss: 0.4421, Acc: 0.8033, Val Acc: 0.8234\n",
      "Epoch 190, Loss: 0.4750, Val Loss: 0.4250, Acc: 0.8104, Val Acc: 0.8433\n",
      "Epoch 200, Loss: 0.4682, Val Loss: 0.4113, Acc: 0.8182, Val Acc: 0.8433\n",
      "Epoch 210, Loss: 0.4796, Val Loss: 0.4090, Acc: 0.8132, Val Acc: 0.8405\n",
      "Epoch 220, Loss: 0.4268, Val Loss: 0.3942, Acc: 0.8345, Val Acc: 0.8519\n",
      "Epoch 230, Loss: 0.4108, Val Loss: 0.3942, Acc: 0.8438, Val Acc: 0.8433\n",
      "Epoch 240, Loss: 0.4074, Val Loss: 0.3834, Acc: 0.8388, Val Acc: 0.8575\n",
      "Epoch 250, Loss: 0.3898, Val Loss: 0.3689, Acc: 0.8686, Val Acc: 0.8689\n",
      "Epoch 260, Loss: 0.3829, Val Loss: 0.3660, Acc: 0.8565, Val Acc: 0.8632\n",
      "Epoch 270, Loss: 0.3855, Val Loss: 0.3641, Acc: 0.8509, Val Acc: 0.8575\n",
      "Epoch 280, Loss: 0.3665, Val Loss: 0.3657, Acc: 0.8651, Val Acc: 0.8632\n",
      "Epoch 290, Loss: 0.3859, Val Loss: 0.3624, Acc: 0.8608, Val Acc: 0.8604\n",
      "Best Validation Loss: 0.3609\n",
      "Fold Test Accuracy: 0.8329, F1-score: 0.8266, Homogeneity: 0.8179, NMI: 0.8234, ARI: 0.7235\n",
      "  Average Metrics for Threshold 0.05: {'test_accuracy': 0.83315649867374, 'f1_score': 0.8308637093296243, 'homogeneity': 0.8141059242287847, 'nmi': 0.8181186599402427, 'ari': 0.7172208226752821}\n",
      "\n",
      "Training with threshold: 0.060000000000000005\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6915, Val Loss: 2.5605, Acc: 0.0789, Val Acc: 0.1307\n",
      "Epoch 10, Loss: 2.4484, Val Loss: 2.4194, Acc: 0.1642, Val Acc: 0.2699\n",
      "Epoch 20, Loss: 2.1891, Val Loss: 2.1221, Acc: 0.2416, Val Acc: 0.3267\n",
      "Epoch 30, Loss: 1.9610, Val Loss: 1.8672, Acc: 0.2722, Val Acc: 0.3665\n",
      "Epoch 40, Loss: 1.7113, Val Loss: 1.6149, Acc: 0.3426, Val Acc: 0.4489\n",
      "Epoch 50, Loss: 1.5178, Val Loss: 1.3790, Acc: 0.4264, Val Acc: 0.5199\n",
      "Epoch 60, Loss: 1.3551, Val Loss: 1.1691, Acc: 0.4613, Val Acc: 0.6108\n",
      "Epoch 70, Loss: 1.2034, Val Loss: 1.0128, Acc: 0.4996, Val Acc: 0.6534\n",
      "Epoch 80, Loss: 1.0258, Val Loss: 0.9001, Acc: 0.5977, Val Acc: 0.6676\n",
      "Epoch 90, Loss: 0.9209, Val Loss: 0.7787, Acc: 0.6233, Val Acc: 0.7074\n",
      "Epoch 100, Loss: 0.8296, Val Loss: 0.6966, Acc: 0.6638, Val Acc: 0.7330\n",
      "Epoch 110, Loss: 0.7483, Val Loss: 0.6930, Acc: 0.7001, Val Acc: 0.7216\n",
      "Epoch 120, Loss: 0.6765, Val Loss: 0.6313, Acc: 0.7342, Val Acc: 0.7301\n",
      "Epoch 130, Loss: 0.6363, Val Loss: 0.5900, Acc: 0.7484, Val Acc: 0.7500\n",
      "Epoch 140, Loss: 0.6174, Val Loss: 0.5758, Acc: 0.7612, Val Acc: 0.7756\n",
      "Epoch 150, Loss: 0.5477, Val Loss: 0.5703, Acc: 0.7818, Val Acc: 0.7642\n",
      "Epoch 160, Loss: 0.5691, Val Loss: 0.5479, Acc: 0.7783, Val Acc: 0.7642\n",
      "Epoch 170, Loss: 0.5507, Val Loss: 0.5263, Acc: 0.7783, Val Acc: 0.7869\n",
      "Epoch 180, Loss: 0.5140, Val Loss: 0.5041, Acc: 0.8031, Val Acc: 0.7841\n",
      "Epoch 190, Loss: 0.4913, Val Loss: 0.4955, Acc: 0.8109, Val Acc: 0.7983\n",
      "Epoch 200, Loss: 0.4848, Val Loss: 0.4852, Acc: 0.8124, Val Acc: 0.8068\n",
      "Epoch 210, Loss: 0.4483, Val Loss: 0.4727, Acc: 0.8181, Val Acc: 0.8097\n",
      "Epoch 220, Loss: 0.4495, Val Loss: 0.4702, Acc: 0.8280, Val Acc: 0.8068\n",
      "Epoch 230, Loss: 0.4313, Val Loss: 0.4635, Acc: 0.8380, Val Acc: 0.8125\n",
      "Epoch 240, Loss: 0.4299, Val Loss: 0.4685, Acc: 0.8422, Val Acc: 0.8040\n",
      "Epoch 250, Loss: 0.4129, Val Loss: 0.4667, Acc: 0.8479, Val Acc: 0.8011\n",
      "Epoch 260, Loss: 0.4106, Val Loss: 0.4545, Acc: 0.8394, Val Acc: 0.8097\n",
      "Epoch 270, Loss: 0.4113, Val Loss: 0.4486, Acc: 0.8443, Val Acc: 0.8125\n",
      "Epoch 280, Loss: 0.4137, Val Loss: 0.4483, Acc: 0.8472, Val Acc: 0.8125\n",
      "Epoch 290, Loss: 0.3930, Val Loss: 0.4440, Acc: 0.8550, Val Acc: 0.8153\n",
      "Best Validation Loss: 0.4422\n",
      "Fold Test Accuracy: 0.8422, F1-score: 0.8416, Homogeneity: 0.8208, NMI: 0.8240, ARI: 0.7302\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6992, Val Loss: 2.5624, Acc: 0.0896, Val Acc: 0.0710\n",
      "Epoch 10, Loss: 2.4548, Val Loss: 2.4263, Acc: 0.1571, Val Acc: 0.2500\n",
      "Epoch 20, Loss: 2.1546, Val Loss: 2.0993, Acc: 0.2559, Val Acc: 0.2727\n",
      "Epoch 30, Loss: 1.9116, Val Loss: 1.7962, Acc: 0.3092, Val Acc: 0.3523\n",
      "Epoch 40, Loss: 1.6954, Val Loss: 1.5053, Acc: 0.3475, Val Acc: 0.4858\n",
      "Epoch 50, Loss: 1.4863, Val Loss: 1.2862, Acc: 0.4229, Val Acc: 0.5909\n",
      "Epoch 60, Loss: 1.3095, Val Loss: 1.1135, Acc: 0.4840, Val Acc: 0.6534\n",
      "Epoch 70, Loss: 1.1536, Val Loss: 0.9515, Acc: 0.5238, Val Acc: 0.7017\n",
      "Epoch 80, Loss: 0.9997, Val Loss: 0.8286, Acc: 0.6041, Val Acc: 0.7415\n",
      "Epoch 90, Loss: 0.8993, Val Loss: 0.7507, Acc: 0.6375, Val Acc: 0.7443\n",
      "Epoch 100, Loss: 0.8033, Val Loss: 0.6842, Acc: 0.6603, Val Acc: 0.7642\n",
      "Epoch 110, Loss: 0.7045, Val Loss: 0.6445, Acc: 0.7143, Val Acc: 0.7585\n",
      "Epoch 120, Loss: 0.6747, Val Loss: 0.6265, Acc: 0.7257, Val Acc: 0.7472\n",
      "Epoch 130, Loss: 0.6005, Val Loss: 0.5603, Acc: 0.7576, Val Acc: 0.7812\n",
      "Epoch 140, Loss: 0.5541, Val Loss: 0.4985, Acc: 0.7797, Val Acc: 0.8040\n",
      "Epoch 150, Loss: 0.5131, Val Loss: 0.4777, Acc: 0.7903, Val Acc: 0.8210\n",
      "Epoch 160, Loss: 0.4887, Val Loss: 0.4658, Acc: 0.8131, Val Acc: 0.8267\n",
      "Epoch 170, Loss: 0.4580, Val Loss: 0.4610, Acc: 0.8401, Val Acc: 0.8153\n",
      "Epoch 180, Loss: 0.4070, Val Loss: 0.4397, Acc: 0.8436, Val Acc: 0.8210\n",
      "Epoch 190, Loss: 0.4180, Val Loss: 0.4089, Acc: 0.8294, Val Acc: 0.8324\n",
      "Epoch 200, Loss: 0.4016, Val Loss: 0.4064, Acc: 0.8408, Val Acc: 0.8352\n",
      "Epoch 210, Loss: 0.3930, Val Loss: 0.4300, Acc: 0.8571, Val Acc: 0.8324\n",
      "Epoch 220, Loss: 0.3897, Val Loss: 0.3978, Acc: 0.8408, Val Acc: 0.8381\n",
      "Epoch 230, Loss: 0.3640, Val Loss: 0.3912, Acc: 0.8643, Val Acc: 0.8381\n",
      "Epoch 240, Loss: 0.3556, Val Loss: 0.3896, Acc: 0.8678, Val Acc: 0.8409\n",
      "Epoch 250, Loss: 0.3553, Val Loss: 0.3928, Acc: 0.8671, Val Acc: 0.8352\n",
      "Epoch 260, Loss: 0.3289, Val Loss: 0.4006, Acc: 0.8685, Val Acc: 0.8352\n",
      "Epoch 270, Loss: 0.3647, Val Loss: 0.4019, Acc: 0.8657, Val Acc: 0.8381\n",
      "Epoch 280, Loss: 0.3400, Val Loss: 0.3990, Acc: 0.8714, Val Acc: 0.8381\n",
      "Epoch 290, Loss: 0.3360, Val Loss: 0.3951, Acc: 0.8763, Val Acc: 0.8381\n",
      "Best Validation Loss: 0.3889\n",
      "Fold Test Accuracy: 0.8316, F1-score: 0.8310, Homogeneity: 0.8104, NMI: 0.8127, ARI: 0.7135\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7330, Val Loss: 2.5618, Acc: 0.0597, Val Acc: 0.0739\n",
      "Epoch 10, Loss: 2.4498, Val Loss: 2.4263, Acc: 0.1606, Val Acc: 0.2557\n",
      "Epoch 20, Loss: 2.1741, Val Loss: 2.0952, Acc: 0.2473, Val Acc: 0.3153\n",
      "Epoch 30, Loss: 1.9234, Val Loss: 1.8235, Acc: 0.3056, Val Acc: 0.3977\n",
      "Epoch 40, Loss: 1.7182, Val Loss: 1.5787, Acc: 0.3490, Val Acc: 0.4261\n",
      "Epoch 50, Loss: 1.5191, Val Loss: 1.3439, Acc: 0.4101, Val Acc: 0.5085\n",
      "Epoch 60, Loss: 1.3618, Val Loss: 1.1544, Acc: 0.4478, Val Acc: 0.5824\n",
      "Epoch 70, Loss: 1.1816, Val Loss: 0.9996, Acc: 0.5409, Val Acc: 0.6364\n",
      "Epoch 80, Loss: 1.0489, Val Loss: 0.8639, Acc: 0.5935, Val Acc: 0.6790\n",
      "Epoch 90, Loss: 0.9198, Val Loss: 0.7621, Acc: 0.6304, Val Acc: 0.7131\n",
      "Epoch 100, Loss: 0.8164, Val Loss: 0.7069, Acc: 0.6866, Val Acc: 0.7045\n",
      "Epoch 110, Loss: 0.7261, Val Loss: 0.6388, Acc: 0.6979, Val Acc: 0.7301\n",
      "Epoch 120, Loss: 0.6830, Val Loss: 0.5957, Acc: 0.7342, Val Acc: 0.7443\n",
      "Epoch 130, Loss: 0.6212, Val Loss: 0.6184, Acc: 0.7541, Val Acc: 0.7415\n",
      "Epoch 140, Loss: 0.5778, Val Loss: 0.5798, Acc: 0.7719, Val Acc: 0.7557\n",
      "Epoch 150, Loss: 0.5404, Val Loss: 0.5193, Acc: 0.7818, Val Acc: 0.8040\n",
      "Epoch 160, Loss: 0.5351, Val Loss: 0.5118, Acc: 0.7761, Val Acc: 0.7983\n",
      "Epoch 170, Loss: 0.4687, Val Loss: 0.5025, Acc: 0.8216, Val Acc: 0.7869\n",
      "Epoch 180, Loss: 0.5019, Val Loss: 0.4932, Acc: 0.8031, Val Acc: 0.7869\n",
      "Epoch 190, Loss: 0.4790, Val Loss: 0.4819, Acc: 0.8053, Val Acc: 0.7983\n",
      "Epoch 200, Loss: 0.4590, Val Loss: 0.4776, Acc: 0.8316, Val Acc: 0.7983\n",
      "Epoch 210, Loss: 0.4610, Val Loss: 0.4683, Acc: 0.8195, Val Acc: 0.8011\n",
      "Epoch 220, Loss: 0.4610, Val Loss: 0.4700, Acc: 0.8337, Val Acc: 0.8068\n",
      "Epoch 230, Loss: 0.4357, Val Loss: 0.4712, Acc: 0.8415, Val Acc: 0.8011\n",
      "Epoch 240, Loss: 0.4290, Val Loss: 0.4627, Acc: 0.8401, Val Acc: 0.8040\n",
      "Epoch 250, Loss: 0.4465, Val Loss: 0.4597, Acc: 0.8358, Val Acc: 0.8068\n",
      "Epoch 260, Loss: 0.4417, Val Loss: 0.4635, Acc: 0.8323, Val Acc: 0.8125\n",
      "Epoch 270, Loss: 0.4486, Val Loss: 0.4617, Acc: 0.8365, Val Acc: 0.8153\n",
      "Epoch 280, Loss: 0.4461, Val Loss: 0.4624, Acc: 0.8358, Val Acc: 0.8097\n",
      "Epoch 290, Loss: 0.4305, Val Loss: 0.4644, Acc: 0.8472, Val Acc: 0.8125\n",
      "Best Validation Loss: 0.4577\n",
      "Fold Test Accuracy: 0.8117, F1-score: 0.8054, Homogeneity: 0.8013, NMI: 0.8106, ARI: 0.6902\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6926, Val Loss: 2.5584, Acc: 0.0675, Val Acc: 0.0966\n",
      "Epoch 10, Loss: 2.4533, Val Loss: 2.4253, Acc: 0.1713, Val Acc: 0.3040\n",
      "Epoch 20, Loss: 2.1893, Val Loss: 2.1030, Acc: 0.2480, Val Acc: 0.3267\n",
      "Epoch 30, Loss: 1.9355, Val Loss: 1.8174, Acc: 0.3042, Val Acc: 0.3892\n",
      "Epoch 40, Loss: 1.7178, Val Loss: 1.5682, Acc: 0.3539, Val Acc: 0.4318\n",
      "Epoch 50, Loss: 1.5150, Val Loss: 1.3518, Acc: 0.4087, Val Acc: 0.5568\n",
      "Epoch 60, Loss: 1.3232, Val Loss: 1.1501, Acc: 0.4890, Val Acc: 0.6278\n",
      "Epoch 70, Loss: 1.1966, Val Loss: 0.9951, Acc: 0.5309, Val Acc: 0.6591\n",
      "Epoch 80, Loss: 1.0303, Val Loss: 0.8509, Acc: 0.5856, Val Acc: 0.6989\n",
      "Epoch 90, Loss: 0.9305, Val Loss: 0.7509, Acc: 0.6162, Val Acc: 0.7159\n",
      "Epoch 100, Loss: 0.8378, Val Loss: 0.6766, Acc: 0.6617, Val Acc: 0.7415\n",
      "Epoch 110, Loss: 0.7539, Val Loss: 0.7684, Acc: 0.6894, Val Acc: 0.6733\n",
      "Epoch 120, Loss: 0.6940, Val Loss: 0.6520, Acc: 0.7122, Val Acc: 0.7301\n",
      "Epoch 130, Loss: 0.6841, Val Loss: 0.6070, Acc: 0.7122, Val Acc: 0.7472\n",
      "Epoch 140, Loss: 0.6421, Val Loss: 0.6050, Acc: 0.7456, Val Acc: 0.7472\n",
      "Epoch 150, Loss: 0.6070, Val Loss: 0.5820, Acc: 0.7477, Val Acc: 0.7642\n",
      "Epoch 160, Loss: 0.6237, Val Loss: 0.5568, Acc: 0.7392, Val Acc: 0.7841\n",
      "Epoch 170, Loss: 0.5854, Val Loss: 0.5479, Acc: 0.7555, Val Acc: 0.7926\n",
      "Epoch 180, Loss: 0.5834, Val Loss: 0.5374, Acc: 0.7548, Val Acc: 0.7955\n",
      "Epoch 190, Loss: 0.5684, Val Loss: 0.5351, Acc: 0.7953, Val Acc: 0.8011\n",
      "Epoch 200, Loss: 0.5497, Val Loss: 0.5370, Acc: 0.7740, Val Acc: 0.7955\n",
      "Epoch 210, Loss: 0.5466, Val Loss: 0.5224, Acc: 0.7839, Val Acc: 0.8040\n",
      "Epoch 220, Loss: 0.5280, Val Loss: 0.5178, Acc: 0.7790, Val Acc: 0.8097\n",
      "Epoch 230, Loss: 0.5215, Val Loss: 0.5104, Acc: 0.7974, Val Acc: 0.8097\n",
      "Epoch 240, Loss: 0.5172, Val Loss: 0.4977, Acc: 0.7882, Val Acc: 0.8125\n",
      "Epoch 250, Loss: 0.4917, Val Loss: 0.4913, Acc: 0.8088, Val Acc: 0.8182\n",
      "Epoch 260, Loss: 0.4798, Val Loss: 0.5041, Acc: 0.8230, Val Acc: 0.8153\n",
      "Epoch 270, Loss: 0.4762, Val Loss: 0.4972, Acc: 0.8216, Val Acc: 0.8153\n",
      "Epoch 280, Loss: 0.5044, Val Loss: 0.4964, Acc: 0.7982, Val Acc: 0.8068\n",
      "Epoch 290, Loss: 0.4701, Val Loss: 0.4936, Acc: 0.8109, Val Acc: 0.8097\n",
      "Best Validation Loss: 0.4913\n",
      "Fold Test Accuracy: 0.8064, F1-score: 0.8001, Homogeneity: 0.8042, NMI: 0.8096, ARI: 0.6945\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7255, Val Loss: 2.5660, Acc: 0.0881, Val Acc: 0.0684\n",
      "Epoch 10, Loss: 2.4677, Val Loss: 2.4447, Acc: 0.1506, Val Acc: 0.2678\n",
      "Epoch 20, Loss: 2.2149, Val Loss: 2.1419, Acc: 0.2138, Val Acc: 0.2764\n",
      "Epoch 30, Loss: 1.9588, Val Loss: 1.8446, Acc: 0.2876, Val Acc: 0.3761\n",
      "Epoch 40, Loss: 1.7445, Val Loss: 1.5756, Acc: 0.3132, Val Acc: 0.4074\n",
      "Epoch 50, Loss: 1.5542, Val Loss: 1.3713, Acc: 0.3857, Val Acc: 0.5157\n",
      "Epoch 60, Loss: 1.3756, Val Loss: 1.1943, Acc: 0.4482, Val Acc: 0.6040\n",
      "Epoch 70, Loss: 1.2344, Val Loss: 1.0305, Acc: 0.5000, Val Acc: 0.6638\n",
      "Epoch 80, Loss: 1.0886, Val Loss: 0.8721, Acc: 0.5724, Val Acc: 0.7179\n",
      "Epoch 90, Loss: 0.9673, Val Loss: 0.7783, Acc: 0.5973, Val Acc: 0.7236\n",
      "Epoch 100, Loss: 0.8467, Val Loss: 0.7085, Acc: 0.6541, Val Acc: 0.7322\n",
      "Epoch 110, Loss: 0.7928, Val Loss: 0.6814, Acc: 0.6776, Val Acc: 0.7208\n",
      "Epoch 120, Loss: 0.7362, Val Loss: 0.6816, Acc: 0.7109, Val Acc: 0.7236\n",
      "Epoch 130, Loss: 0.6921, Val Loss: 0.5616, Acc: 0.7237, Val Acc: 0.7863\n",
      "Epoch 140, Loss: 0.6426, Val Loss: 0.5400, Acc: 0.7500, Val Acc: 0.7949\n",
      "Epoch 150, Loss: 0.6040, Val Loss: 0.5233, Acc: 0.7635, Val Acc: 0.7892\n",
      "Epoch 160, Loss: 0.5982, Val Loss: 0.5024, Acc: 0.7557, Val Acc: 0.7949\n",
      "Epoch 170, Loss: 0.5856, Val Loss: 0.4934, Acc: 0.7614, Val Acc: 0.8091\n",
      "Epoch 180, Loss: 0.5658, Val Loss: 0.4768, Acc: 0.7905, Val Acc: 0.8091\n",
      "Epoch 190, Loss: 0.5616, Val Loss: 0.4758, Acc: 0.7784, Val Acc: 0.8034\n",
      "Epoch 200, Loss: 0.5059, Val Loss: 0.4579, Acc: 0.7947, Val Acc: 0.8120\n",
      "Epoch 210, Loss: 0.5165, Val Loss: 0.4452, Acc: 0.7912, Val Acc: 0.8234\n",
      "Epoch 220, Loss: 0.4922, Val Loss: 0.4413, Acc: 0.8026, Val Acc: 0.8234\n",
      "Epoch 230, Loss: 0.5017, Val Loss: 0.4391, Acc: 0.8018, Val Acc: 0.8177\n",
      "Epoch 240, Loss: 0.4863, Val Loss: 0.4334, Acc: 0.8118, Val Acc: 0.8234\n",
      "Epoch 250, Loss: 0.4583, Val Loss: 0.4341, Acc: 0.8153, Val Acc: 0.8262\n",
      "Epoch 260, Loss: 0.4393, Val Loss: 0.4251, Acc: 0.8267, Val Acc: 0.8348\n",
      "Epoch 270, Loss: 0.4272, Val Loss: 0.4226, Acc: 0.8352, Val Acc: 0.8376\n",
      "Epoch 280, Loss: 0.4589, Val Loss: 0.4191, Acc: 0.8295, Val Acc: 0.8433\n",
      "Epoch 290, Loss: 0.4397, Val Loss: 0.4145, Acc: 0.8182, Val Acc: 0.8405\n",
      "Best Validation Loss: 0.4127\n",
      "Fold Test Accuracy: 0.8090, F1-score: 0.8024, Homogeneity: 0.8113, NMI: 0.8160, ARI: 0.7030\n",
      "  Average Metrics for Threshold 0.06: {'test_accuracy': 0.8201591511936339, 'f1_score': 0.8161126123801405, 'homogeneity': 0.8096019053065527, 'nmi': 0.8145921231620598, 'ari': 0.7062671719817598}\n",
      "\n",
      "Training with threshold: 0.07\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6905, Val Loss: 2.5597, Acc: 0.0846, Val Acc: 0.0938\n",
      "Epoch 10, Loss: 2.4500, Val Loss: 2.4134, Acc: 0.1677, Val Acc: 0.2358\n",
      "Epoch 20, Loss: 2.1610, Val Loss: 2.1159, Acc: 0.2409, Val Acc: 0.2756\n",
      "Epoch 30, Loss: 1.9608, Val Loss: 1.8440, Acc: 0.2829, Val Acc: 0.3011\n",
      "Epoch 40, Loss: 1.7239, Val Loss: 1.6062, Acc: 0.3440, Val Acc: 0.3892\n",
      "Epoch 50, Loss: 1.5095, Val Loss: 1.4049, Acc: 0.4215, Val Acc: 0.5114\n",
      "Epoch 60, Loss: 1.3824, Val Loss: 1.2315, Acc: 0.4527, Val Acc: 0.5881\n",
      "Epoch 70, Loss: 1.2005, Val Loss: 1.0844, Acc: 0.5131, Val Acc: 0.6080\n",
      "Epoch 80, Loss: 1.0443, Val Loss: 0.9064, Acc: 0.5792, Val Acc: 0.6676\n",
      "Epoch 90, Loss: 0.9298, Val Loss: 0.7850, Acc: 0.6183, Val Acc: 0.7102\n",
      "Epoch 100, Loss: 0.8401, Val Loss: 0.7152, Acc: 0.6624, Val Acc: 0.7443\n",
      "Epoch 110, Loss: 0.7586, Val Loss: 0.7277, Acc: 0.7043, Val Acc: 0.6875\n",
      "Epoch 120, Loss: 0.6936, Val Loss: 0.6745, Acc: 0.7207, Val Acc: 0.7244\n",
      "Epoch 130, Loss: 0.6801, Val Loss: 0.6149, Acc: 0.7242, Val Acc: 0.7670\n",
      "Epoch 140, Loss: 0.6252, Val Loss: 0.5961, Acc: 0.7555, Val Acc: 0.7727\n",
      "Epoch 150, Loss: 0.6095, Val Loss: 0.5825, Acc: 0.7548, Val Acc: 0.7614\n",
      "Epoch 160, Loss: 0.5726, Val Loss: 0.5703, Acc: 0.7740, Val Acc: 0.7812\n",
      "Epoch 170, Loss: 0.5484, Val Loss: 0.5712, Acc: 0.7818, Val Acc: 0.7699\n",
      "Epoch 180, Loss: 0.5577, Val Loss: 0.5509, Acc: 0.7775, Val Acc: 0.7756\n",
      "Epoch 190, Loss: 0.5113, Val Loss: 0.5464, Acc: 0.7925, Val Acc: 0.7784\n",
      "Epoch 200, Loss: 0.5127, Val Loss: 0.5445, Acc: 0.7910, Val Acc: 0.7841\n",
      "Epoch 210, Loss: 0.5253, Val Loss: 0.5379, Acc: 0.7903, Val Acc: 0.7898\n",
      "Epoch 220, Loss: 0.5083, Val Loss: 0.5312, Acc: 0.7967, Val Acc: 0.7898\n",
      "Epoch 230, Loss: 0.4944, Val Loss: 0.5251, Acc: 0.8081, Val Acc: 0.7898\n",
      "Epoch 240, Loss: 0.4988, Val Loss: 0.5243, Acc: 0.8081, Val Acc: 0.7841\n",
      "Epoch 250, Loss: 0.4650, Val Loss: 0.5183, Acc: 0.8195, Val Acc: 0.7869\n",
      "Epoch 260, Loss: 0.4341, Val Loss: 0.5176, Acc: 0.8387, Val Acc: 0.8011\n",
      "Epoch 270, Loss: 0.4345, Val Loss: 0.5240, Acc: 0.8351, Val Acc: 0.7898\n",
      "Epoch 280, Loss: 0.4507, Val Loss: 0.5184, Acc: 0.8287, Val Acc: 0.7955\n",
      "Epoch 290, Loss: 0.4448, Val Loss: 0.5133, Acc: 0.8387, Val Acc: 0.8040\n",
      "Best Validation Loss: 0.5109\n",
      "Fold Test Accuracy: 0.8183, F1-score: 0.8107, Homogeneity: 0.7996, NMI: 0.8066, ARI: 0.6972\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7038, Val Loss: 2.5603, Acc: 0.0810, Val Acc: 0.0966\n",
      "Epoch 10, Loss: 2.4680, Val Loss: 2.4354, Acc: 0.1599, Val Acc: 0.2386\n",
      "Epoch 20, Loss: 2.1826, Val Loss: 2.1145, Acc: 0.2345, Val Acc: 0.2983\n",
      "Epoch 30, Loss: 1.9454, Val Loss: 1.8230, Acc: 0.2893, Val Acc: 0.3722\n",
      "Epoch 40, Loss: 1.7353, Val Loss: 1.5582, Acc: 0.3561, Val Acc: 0.4574\n",
      "Epoch 50, Loss: 1.5351, Val Loss: 1.3474, Acc: 0.4108, Val Acc: 0.5511\n",
      "Epoch 60, Loss: 1.3384, Val Loss: 1.1724, Acc: 0.4698, Val Acc: 0.6023\n",
      "Epoch 70, Loss: 1.1900, Val Loss: 1.0382, Acc: 0.5402, Val Acc: 0.6364\n",
      "Epoch 80, Loss: 1.0846, Val Loss: 0.9113, Acc: 0.5579, Val Acc: 0.6818\n",
      "Epoch 90, Loss: 0.9228, Val Loss: 0.8017, Acc: 0.6283, Val Acc: 0.7188\n",
      "Epoch 100, Loss: 0.8387, Val Loss: 0.7340, Acc: 0.6596, Val Acc: 0.7585\n",
      "Epoch 110, Loss: 0.7739, Val Loss: 0.7039, Acc: 0.6887, Val Acc: 0.7301\n",
      "Epoch 120, Loss: 0.6940, Val Loss: 0.6244, Acc: 0.7164, Val Acc: 0.7756\n",
      "Epoch 130, Loss: 0.6617, Val Loss: 0.5725, Acc: 0.7271, Val Acc: 0.7898\n",
      "Epoch 140, Loss: 0.6234, Val Loss: 0.5507, Acc: 0.7505, Val Acc: 0.7841\n",
      "Epoch 150, Loss: 0.5727, Val Loss: 0.5045, Acc: 0.7726, Val Acc: 0.7926\n",
      "Epoch 160, Loss: 0.4941, Val Loss: 0.4752, Acc: 0.7925, Val Acc: 0.8011\n",
      "Epoch 170, Loss: 0.4673, Val Loss: 0.4723, Acc: 0.8173, Val Acc: 0.8068\n",
      "Epoch 180, Loss: 0.4394, Val Loss: 0.4657, Acc: 0.8230, Val Acc: 0.8125\n",
      "Epoch 190, Loss: 0.4311, Val Loss: 0.4509, Acc: 0.8259, Val Acc: 0.8210\n",
      "Epoch 200, Loss: 0.4242, Val Loss: 0.4113, Acc: 0.8443, Val Acc: 0.8381\n",
      "Epoch 210, Loss: 0.3880, Val Loss: 0.4175, Acc: 0.8564, Val Acc: 0.8352\n",
      "Epoch 220, Loss: 0.3805, Val Loss: 0.4280, Acc: 0.8515, Val Acc: 0.8295\n",
      "Epoch 230, Loss: 0.3940, Val Loss: 0.4226, Acc: 0.8436, Val Acc: 0.8324\n",
      "Epoch 240, Loss: 0.3823, Val Loss: 0.4205, Acc: 0.8493, Val Acc: 0.8352\n",
      "Epoch 250, Loss: 0.3557, Val Loss: 0.4184, Acc: 0.8678, Val Acc: 0.8352\n",
      "Epoch 260, Loss: 0.3555, Val Loss: 0.4189, Acc: 0.8678, Val Acc: 0.8352\n",
      "Epoch 270, Loss: 0.3715, Val Loss: 0.4204, Acc: 0.8614, Val Acc: 0.8352\n",
      "Epoch 280, Loss: 0.3563, Val Loss: 0.4194, Acc: 0.8607, Val Acc: 0.8352\n",
      "Epoch 290, Loss: 0.3540, Val Loss: 0.4215, Acc: 0.8699, Val Acc: 0.8352\n",
      "Best Validation Loss: 0.4103\n",
      "Fold Test Accuracy: 0.8249, F1-score: 0.8222, Homogeneity: 0.8102, NMI: 0.8141, ARI: 0.7090\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6995, Val Loss: 2.5620, Acc: 0.0725, Val Acc: 0.0881\n",
      "Epoch 10, Loss: 2.4742, Val Loss: 2.4372, Acc: 0.1606, Val Acc: 0.2784\n",
      "Epoch 20, Loss: 2.2065, Val Loss: 2.1265, Acc: 0.2395, Val Acc: 0.3040\n",
      "Epoch 30, Loss: 1.9354, Val Loss: 1.8223, Acc: 0.3092, Val Acc: 0.3438\n",
      "Epoch 40, Loss: 1.7414, Val Loss: 1.5728, Acc: 0.3404, Val Acc: 0.4460\n",
      "Epoch 50, Loss: 1.5542, Val Loss: 1.3561, Acc: 0.3994, Val Acc: 0.5256\n",
      "Epoch 60, Loss: 1.3725, Val Loss: 1.1765, Acc: 0.4556, Val Acc: 0.6080\n",
      "Epoch 70, Loss: 1.2053, Val Loss: 1.0218, Acc: 0.5188, Val Acc: 0.6420\n",
      "Epoch 80, Loss: 1.0616, Val Loss: 0.8580, Acc: 0.5842, Val Acc: 0.7159\n",
      "Epoch 90, Loss: 0.9499, Val Loss: 0.7452, Acc: 0.6311, Val Acc: 0.7415\n",
      "Epoch 100, Loss: 0.8354, Val Loss: 0.6729, Acc: 0.6603, Val Acc: 0.7500\n",
      "Epoch 110, Loss: 0.7527, Val Loss: 0.6247, Acc: 0.7029, Val Acc: 0.7670\n",
      "Epoch 120, Loss: 0.7008, Val Loss: 0.5865, Acc: 0.7058, Val Acc: 0.7727\n",
      "Epoch 130, Loss: 0.6273, Val Loss: 0.5476, Acc: 0.7477, Val Acc: 0.7699\n",
      "Epoch 140, Loss: 0.5874, Val Loss: 0.5299, Acc: 0.7584, Val Acc: 0.7642\n",
      "Epoch 150, Loss: 0.5298, Val Loss: 0.5184, Acc: 0.7967, Val Acc: 0.7784\n",
      "Epoch 160, Loss: 0.5072, Val Loss: 0.5310, Acc: 0.8038, Val Acc: 0.7898\n",
      "Epoch 170, Loss: 0.4447, Val Loss: 0.4547, Acc: 0.8202, Val Acc: 0.8125\n",
      "Epoch 180, Loss: 0.4622, Val Loss: 0.4383, Acc: 0.8117, Val Acc: 0.8324\n",
      "Epoch 190, Loss: 0.4155, Val Loss: 0.4313, Acc: 0.8429, Val Acc: 0.8295\n",
      "Epoch 200, Loss: 0.4186, Val Loss: 0.4254, Acc: 0.8458, Val Acc: 0.8324\n",
      "Epoch 210, Loss: 0.4002, Val Loss: 0.4238, Acc: 0.8458, Val Acc: 0.8267\n",
      "Epoch 220, Loss: 0.3864, Val Loss: 0.4395, Acc: 0.8507, Val Acc: 0.8182\n",
      "Epoch 230, Loss: 0.3669, Val Loss: 0.4600, Acc: 0.8614, Val Acc: 0.8097\n",
      "Epoch 240, Loss: 0.3713, Val Loss: 0.4328, Acc: 0.8621, Val Acc: 0.8295\n",
      "Epoch 250, Loss: 0.3574, Val Loss: 0.4210, Acc: 0.8600, Val Acc: 0.8267\n",
      "Epoch 260, Loss: 0.3715, Val Loss: 0.4162, Acc: 0.8721, Val Acc: 0.8239\n",
      "Epoch 270, Loss: 0.3721, Val Loss: 0.4146, Acc: 0.8635, Val Acc: 0.8239\n",
      "Epoch 280, Loss: 0.3667, Val Loss: 0.4091, Acc: 0.8579, Val Acc: 0.8239\n",
      "Epoch 290, Loss: 0.3437, Val Loss: 0.4117, Acc: 0.8863, Val Acc: 0.8267\n",
      "Best Validation Loss: 0.4091\n",
      "Fold Test Accuracy: 0.8223, F1-score: 0.8184, Homogeneity: 0.8102, NMI: 0.8140, ARI: 0.7087\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7097, Val Loss: 2.5670, Acc: 0.0903, Val Acc: 0.0625\n",
      "Epoch 10, Loss: 2.4860, Val Loss: 2.4381, Acc: 0.1464, Val Acc: 0.2330\n",
      "Epoch 20, Loss: 2.1995, Val Loss: 2.1364, Acc: 0.2289, Val Acc: 0.3381\n",
      "Epoch 30, Loss: 1.9618, Val Loss: 1.8305, Acc: 0.2964, Val Acc: 0.3608\n",
      "Epoch 40, Loss: 1.7287, Val Loss: 1.5613, Acc: 0.3518, Val Acc: 0.4460\n",
      "Epoch 50, Loss: 1.5372, Val Loss: 1.3490, Acc: 0.3937, Val Acc: 0.5085\n",
      "Epoch 60, Loss: 1.3512, Val Loss: 1.1674, Acc: 0.4499, Val Acc: 0.5994\n",
      "Epoch 70, Loss: 1.1727, Val Loss: 1.0053, Acc: 0.5352, Val Acc: 0.6250\n",
      "Epoch 80, Loss: 1.0223, Val Loss: 0.8566, Acc: 0.5920, Val Acc: 0.6591\n",
      "Epoch 90, Loss: 0.8732, Val Loss: 0.7889, Acc: 0.6596, Val Acc: 0.6619\n",
      "Epoch 100, Loss: 0.8138, Val Loss: 0.7103, Acc: 0.6780, Val Acc: 0.7017\n",
      "Epoch 110, Loss: 0.7276, Val Loss: 0.6513, Acc: 0.6951, Val Acc: 0.7500\n",
      "Epoch 120, Loss: 0.6782, Val Loss: 0.6408, Acc: 0.7512, Val Acc: 0.7131\n",
      "Epoch 130, Loss: 0.5959, Val Loss: 0.5788, Acc: 0.7470, Val Acc: 0.7642\n",
      "Epoch 140, Loss: 0.5784, Val Loss: 0.5844, Acc: 0.7704, Val Acc: 0.7528\n",
      "Epoch 150, Loss: 0.5412, Val Loss: 0.6024, Acc: 0.7846, Val Acc: 0.7472\n",
      "Epoch 160, Loss: 0.5116, Val Loss: 0.5653, Acc: 0.7967, Val Acc: 0.7614\n",
      "Epoch 170, Loss: 0.4873, Val Loss: 0.5238, Acc: 0.8145, Val Acc: 0.7898\n",
      "Epoch 180, Loss: 0.4541, Val Loss: 0.5073, Acc: 0.8273, Val Acc: 0.7983\n",
      "Epoch 190, Loss: 0.4540, Val Loss: 0.4940, Acc: 0.8237, Val Acc: 0.7983\n",
      "Epoch 200, Loss: 0.4764, Val Loss: 0.5100, Acc: 0.8230, Val Acc: 0.7955\n",
      "Epoch 210, Loss: 0.3940, Val Loss: 0.4854, Acc: 0.8507, Val Acc: 0.8068\n",
      "Epoch 220, Loss: 0.4095, Val Loss: 0.4874, Acc: 0.8394, Val Acc: 0.8097\n",
      "Epoch 230, Loss: 0.4380, Val Loss: 0.4821, Acc: 0.8273, Val Acc: 0.8182\n",
      "Epoch 240, Loss: 0.3953, Val Loss: 0.4788, Acc: 0.8500, Val Acc: 0.8210\n",
      "Epoch 250, Loss: 0.4136, Val Loss: 0.4732, Acc: 0.8429, Val Acc: 0.8239\n",
      "Epoch 260, Loss: 0.3780, Val Loss: 0.4697, Acc: 0.8593, Val Acc: 0.8153\n",
      "Epoch 270, Loss: 0.3871, Val Loss: 0.4770, Acc: 0.8479, Val Acc: 0.8182\n",
      "Epoch 280, Loss: 0.3824, Val Loss: 0.4765, Acc: 0.8515, Val Acc: 0.8210\n",
      "Epoch 290, Loss: 0.4023, Val Loss: 0.4757, Acc: 0.8536, Val Acc: 0.8210\n",
      "Best Validation Loss: 0.4679\n",
      "Fold Test Accuracy: 0.8103, F1-score: 0.8070, Homogeneity: 0.7924, NMI: 0.7981, ARI: 0.6824\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7000, Val Loss: 2.5633, Acc: 0.0795, Val Acc: 0.0997\n",
      "Epoch 10, Loss: 2.4938, Val Loss: 2.4400, Acc: 0.1357, Val Acc: 0.2821\n",
      "Epoch 20, Loss: 2.1941, Val Loss: 2.1163, Acc: 0.2500, Val Acc: 0.3191\n",
      "Epoch 30, Loss: 1.9574, Val Loss: 1.8074, Acc: 0.3033, Val Acc: 0.3875\n",
      "Epoch 40, Loss: 1.7168, Val Loss: 1.5328, Acc: 0.3501, Val Acc: 0.4786\n",
      "Epoch 50, Loss: 1.4883, Val Loss: 1.3094, Acc: 0.4134, Val Acc: 0.5641\n",
      "Epoch 60, Loss: 1.3330, Val Loss: 1.1320, Acc: 0.4716, Val Acc: 0.6211\n",
      "Epoch 70, Loss: 1.2028, Val Loss: 0.9843, Acc: 0.5149, Val Acc: 0.6524\n",
      "Epoch 80, Loss: 1.0509, Val Loss: 0.8514, Acc: 0.5824, Val Acc: 0.6838\n",
      "Epoch 90, Loss: 0.9527, Val Loss: 0.7545, Acc: 0.6207, Val Acc: 0.7350\n",
      "Epoch 100, Loss: 0.8235, Val Loss: 0.7021, Acc: 0.6499, Val Acc: 0.7493\n",
      "Epoch 110, Loss: 0.7669, Val Loss: 0.6528, Acc: 0.6811, Val Acc: 0.7721\n",
      "Epoch 120, Loss: 0.7065, Val Loss: 0.6132, Acc: 0.7173, Val Acc: 0.7607\n",
      "Epoch 130, Loss: 0.6440, Val Loss: 0.5303, Acc: 0.7280, Val Acc: 0.8091\n",
      "Epoch 140, Loss: 0.5860, Val Loss: 0.5368, Acc: 0.7628, Val Acc: 0.7607\n",
      "Epoch 150, Loss: 0.5514, Val Loss: 0.4899, Acc: 0.7841, Val Acc: 0.7863\n",
      "Epoch 160, Loss: 0.5332, Val Loss: 0.4618, Acc: 0.7947, Val Acc: 0.8006\n",
      "Epoch 170, Loss: 0.5063, Val Loss: 0.4365, Acc: 0.7997, Val Acc: 0.8348\n",
      "Epoch 180, Loss: 0.4860, Val Loss: 0.4127, Acc: 0.8061, Val Acc: 0.8348\n",
      "Epoch 190, Loss: 0.4779, Val Loss: 0.4108, Acc: 0.8161, Val Acc: 0.8433\n",
      "Epoch 200, Loss: 0.4680, Val Loss: 0.4307, Acc: 0.8317, Val Acc: 0.8376\n",
      "Epoch 210, Loss: 0.4496, Val Loss: 0.3964, Acc: 0.8310, Val Acc: 0.8405\n",
      "Epoch 220, Loss: 0.4524, Val Loss: 0.3861, Acc: 0.8253, Val Acc: 0.8519\n",
      "Epoch 230, Loss: 0.4237, Val Loss: 0.3840, Acc: 0.8324, Val Acc: 0.8575\n",
      "Epoch 240, Loss: 0.4368, Val Loss: 0.3800, Acc: 0.8267, Val Acc: 0.8632\n",
      "Epoch 250, Loss: 0.4175, Val Loss: 0.3791, Acc: 0.8366, Val Acc: 0.8547\n",
      "Epoch 260, Loss: 0.4364, Val Loss: 0.3761, Acc: 0.8224, Val Acc: 0.8547\n",
      "Epoch 270, Loss: 0.4101, Val Loss: 0.3775, Acc: 0.8430, Val Acc: 0.8547\n",
      "Epoch 280, Loss: 0.4214, Val Loss: 0.3743, Acc: 0.8310, Val Acc: 0.8575\n",
      "Epoch 290, Loss: 0.4091, Val Loss: 0.3730, Acc: 0.8523, Val Acc: 0.8575\n",
      "Best Validation Loss: 0.3709\n",
      "Fold Test Accuracy: 0.8289, F1-score: 0.8226, Homogeneity: 0.8213, NMI: 0.8274, ARI: 0.7236\n",
      "  Average Metrics for Threshold 0.07: {'test_accuracy': 0.8209549071618036, 'f1_score': 0.8161785833741744, 'homogeneity': 0.8067399040598993, 'nmi': 0.8120419746911871, 'ari': 0.7041854207446072}\n",
      "\n",
      "Training with threshold: 0.08000000000000002\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7083, Val Loss: 2.5606, Acc: 0.0796, Val Acc: 0.0795\n",
      "Epoch 10, Loss: 2.4668, Val Loss: 2.4355, Acc: 0.1635, Val Acc: 0.2244\n",
      "Epoch 20, Loss: 2.1704, Val Loss: 2.1115, Acc: 0.2509, Val Acc: 0.3040\n",
      "Epoch 30, Loss: 1.9255, Val Loss: 1.8313, Acc: 0.2985, Val Acc: 0.3920\n",
      "Epoch 40, Loss: 1.7013, Val Loss: 1.5865, Acc: 0.3447, Val Acc: 0.4261\n",
      "Epoch 50, Loss: 1.4741, Val Loss: 1.3460, Acc: 0.4364, Val Acc: 0.5142\n",
      "Epoch 60, Loss: 1.3183, Val Loss: 1.1342, Acc: 0.4797, Val Acc: 0.6108\n",
      "Epoch 70, Loss: 1.1533, Val Loss: 0.9716, Acc: 0.5480, Val Acc: 0.6619\n",
      "Epoch 80, Loss: 1.0036, Val Loss: 0.8411, Acc: 0.5913, Val Acc: 0.6903\n",
      "Epoch 90, Loss: 0.8726, Val Loss: 0.7462, Acc: 0.6503, Val Acc: 0.7244\n",
      "Epoch 100, Loss: 0.7737, Val Loss: 0.6777, Acc: 0.7001, Val Acc: 0.7443\n",
      "Epoch 110, Loss: 0.7395, Val Loss: 0.6432, Acc: 0.7050, Val Acc: 0.7330\n",
      "Epoch 120, Loss: 0.6960, Val Loss: 0.9029, Acc: 0.7171, Val Acc: 0.6193\n",
      "Epoch 130, Loss: 0.6189, Val Loss: 0.6604, Acc: 0.7413, Val Acc: 0.7102\n",
      "Epoch 140, Loss: 0.6078, Val Loss: 0.5789, Acc: 0.7498, Val Acc: 0.7358\n",
      "Epoch 150, Loss: 0.5783, Val Loss: 0.5457, Acc: 0.7619, Val Acc: 0.7756\n",
      "Epoch 160, Loss: 0.5470, Val Loss: 0.5430, Acc: 0.7868, Val Acc: 0.7670\n",
      "Epoch 170, Loss: 0.5240, Val Loss: 0.5353, Acc: 0.7932, Val Acc: 0.7784\n",
      "Epoch 180, Loss: 0.5171, Val Loss: 0.5214, Acc: 0.7967, Val Acc: 0.7812\n",
      "Epoch 190, Loss: 0.4996, Val Loss: 0.5205, Acc: 0.8045, Val Acc: 0.7812\n",
      "Epoch 200, Loss: 0.5091, Val Loss: 0.5180, Acc: 0.7854, Val Acc: 0.7784\n",
      "Epoch 210, Loss: 0.5032, Val Loss: 0.5100, Acc: 0.8053, Val Acc: 0.7898\n",
      "Epoch 220, Loss: 0.4754, Val Loss: 0.5057, Acc: 0.8188, Val Acc: 0.7926\n",
      "Epoch 230, Loss: 0.4806, Val Loss: 0.5025, Acc: 0.8095, Val Acc: 0.8011\n",
      "Epoch 240, Loss: 0.4831, Val Loss: 0.4997, Acc: 0.8173, Val Acc: 0.7926\n",
      "Epoch 250, Loss: 0.4809, Val Loss: 0.5027, Acc: 0.8024, Val Acc: 0.7898\n",
      "Epoch 260, Loss: 0.4857, Val Loss: 0.5007, Acc: 0.7974, Val Acc: 0.7955\n",
      "Epoch 270, Loss: 0.4641, Val Loss: 0.5006, Acc: 0.8173, Val Acc: 0.7955\n",
      "Epoch 280, Loss: 0.4745, Val Loss: 0.5015, Acc: 0.8181, Val Acc: 0.7926\n",
      "Epoch 290, Loss: 0.4833, Val Loss: 0.5008, Acc: 0.8223, Val Acc: 0.7926\n",
      "Best Validation Loss: 0.4993\n",
      "Fold Test Accuracy: 0.7944, F1-score: 0.7828, Homogeneity: 0.7871, NMI: 0.7966, ARI: 0.6754\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7215, Val Loss: 2.5668, Acc: 0.0725, Val Acc: 0.0568\n",
      "Epoch 10, Loss: 2.4878, Val Loss: 2.4540, Acc: 0.1557, Val Acc: 0.2784\n",
      "Epoch 20, Loss: 2.1965, Val Loss: 2.1534, Acc: 0.2431, Val Acc: 0.3153\n",
      "Epoch 30, Loss: 1.9794, Val Loss: 1.8369, Acc: 0.2878, Val Acc: 0.3693\n",
      "Epoch 40, Loss: 1.7606, Val Loss: 1.5863, Acc: 0.3177, Val Acc: 0.3807\n",
      "Epoch 50, Loss: 1.5515, Val Loss: 1.3830, Acc: 0.4165, Val Acc: 0.5142\n",
      "Epoch 60, Loss: 1.3612, Val Loss: 1.1970, Acc: 0.4790, Val Acc: 0.5938\n",
      "Epoch 70, Loss: 1.2262, Val Loss: 1.0358, Acc: 0.5068, Val Acc: 0.6733\n",
      "Epoch 80, Loss: 1.0724, Val Loss: 0.9081, Acc: 0.5686, Val Acc: 0.6875\n",
      "Epoch 90, Loss: 0.9383, Val Loss: 0.7985, Acc: 0.6141, Val Acc: 0.7386\n",
      "Epoch 100, Loss: 0.8506, Val Loss: 0.7339, Acc: 0.6567, Val Acc: 0.7386\n",
      "Epoch 110, Loss: 0.7896, Val Loss: 0.6561, Acc: 0.6866, Val Acc: 0.7812\n",
      "Epoch 120, Loss: 0.7047, Val Loss: 0.6206, Acc: 0.7178, Val Acc: 0.7670\n",
      "Epoch 130, Loss: 0.6483, Val Loss: 0.5752, Acc: 0.7413, Val Acc: 0.7756\n",
      "Epoch 140, Loss: 0.5990, Val Loss: 0.6156, Acc: 0.7640, Val Acc: 0.7585\n",
      "Epoch 150, Loss: 0.5915, Val Loss: 0.5075, Acc: 0.7541, Val Acc: 0.8011\n",
      "Epoch 160, Loss: 0.5591, Val Loss: 0.4880, Acc: 0.7697, Val Acc: 0.8068\n",
      "Epoch 170, Loss: 0.5133, Val Loss: 0.4779, Acc: 0.7882, Val Acc: 0.8068\n",
      "Epoch 180, Loss: 0.4871, Val Loss: 0.4683, Acc: 0.8102, Val Acc: 0.8097\n",
      "Epoch 190, Loss: 0.4804, Val Loss: 0.4572, Acc: 0.8145, Val Acc: 0.8239\n",
      "Epoch 200, Loss: 0.4543, Val Loss: 0.4377, Acc: 0.8287, Val Acc: 0.8267\n",
      "Epoch 210, Loss: 0.4333, Val Loss: 0.4248, Acc: 0.8387, Val Acc: 0.8409\n",
      "Epoch 220, Loss: 0.4161, Val Loss: 0.4091, Acc: 0.8443, Val Acc: 0.8523\n",
      "Epoch 230, Loss: 0.4057, Val Loss: 0.3953, Acc: 0.8436, Val Acc: 0.8551\n",
      "Epoch 240, Loss: 0.3954, Val Loss: 0.3935, Acc: 0.8436, Val Acc: 0.8580\n",
      "Epoch 250, Loss: 0.3833, Val Loss: 0.3763, Acc: 0.8571, Val Acc: 0.8551\n",
      "Epoch 260, Loss: 0.3534, Val Loss: 0.3677, Acc: 0.8678, Val Acc: 0.8665\n",
      "Epoch 270, Loss: 0.3498, Val Loss: 0.3759, Acc: 0.8671, Val Acc: 0.8523\n",
      "Epoch 280, Loss: 0.3428, Val Loss: 0.3798, Acc: 0.8756, Val Acc: 0.8523\n",
      "Epoch 290, Loss: 0.3357, Val Loss: 0.3830, Acc: 0.8657, Val Acc: 0.8523\n",
      "Best Validation Loss: 0.3638\n",
      "Fold Test Accuracy: 0.8276, F1-score: 0.8270, Homogeneity: 0.8043, NMI: 0.8080, ARI: 0.7026\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6956, Val Loss: 2.5621, Acc: 0.0654, Val Acc: 0.1023\n",
      "Epoch 10, Loss: 2.4688, Val Loss: 2.4196, Acc: 0.1457, Val Acc: 0.2812\n",
      "Epoch 20, Loss: 2.1944, Val Loss: 2.1018, Acc: 0.2246, Val Acc: 0.3011\n",
      "Epoch 30, Loss: 1.9359, Val Loss: 1.8702, Acc: 0.2978, Val Acc: 0.2926\n",
      "Epoch 40, Loss: 1.7876, Val Loss: 1.6313, Acc: 0.3269, Val Acc: 0.3949\n",
      "Epoch 50, Loss: 1.5616, Val Loss: 1.3762, Acc: 0.4072, Val Acc: 0.5170\n",
      "Epoch 60, Loss: 1.3333, Val Loss: 1.1604, Acc: 0.4819, Val Acc: 0.6080\n",
      "Epoch 70, Loss: 1.2121, Val Loss: 0.9916, Acc: 0.5217, Val Acc: 0.6506\n",
      "Epoch 80, Loss: 1.0386, Val Loss: 0.8621, Acc: 0.5785, Val Acc: 0.6648\n",
      "Epoch 90, Loss: 0.9118, Val Loss: 0.7693, Acc: 0.6311, Val Acc: 0.7102\n",
      "Epoch 100, Loss: 0.8233, Val Loss: 0.7142, Acc: 0.6667, Val Acc: 0.7216\n",
      "Epoch 110, Loss: 0.7578, Val Loss: 0.6679, Acc: 0.7015, Val Acc: 0.7244\n",
      "Epoch 120, Loss: 0.6611, Val Loss: 0.6320, Acc: 0.7413, Val Acc: 0.7472\n",
      "Epoch 130, Loss: 0.6143, Val Loss: 0.6506, Acc: 0.7555, Val Acc: 0.7131\n",
      "Epoch 140, Loss: 0.5971, Val Loss: 0.5659, Acc: 0.7633, Val Acc: 0.7614\n",
      "Epoch 150, Loss: 0.5484, Val Loss: 0.5416, Acc: 0.7825, Val Acc: 0.7670\n",
      "Epoch 160, Loss: 0.5393, Val Loss: 0.5066, Acc: 0.7832, Val Acc: 0.8011\n",
      "Epoch 170, Loss: 0.5057, Val Loss: 0.5029, Acc: 0.8010, Val Acc: 0.7841\n",
      "Epoch 180, Loss: 0.4922, Val Loss: 0.4905, Acc: 0.8124, Val Acc: 0.7869\n",
      "Epoch 190, Loss: 0.4884, Val Loss: 0.4720, Acc: 0.8088, Val Acc: 0.8097\n",
      "Epoch 200, Loss: 0.4373, Val Loss: 0.4695, Acc: 0.8273, Val Acc: 0.7955\n",
      "Epoch 210, Loss: 0.4422, Val Loss: 0.4625, Acc: 0.8266, Val Acc: 0.8068\n",
      "Epoch 220, Loss: 0.4282, Val Loss: 0.4406, Acc: 0.8429, Val Acc: 0.8125\n",
      "Epoch 230, Loss: 0.4307, Val Loss: 0.4457, Acc: 0.8380, Val Acc: 0.8097\n",
      "Epoch 240, Loss: 0.4181, Val Loss: 0.4436, Acc: 0.8465, Val Acc: 0.8040\n",
      "Epoch 250, Loss: 0.4069, Val Loss: 0.4526, Acc: 0.8571, Val Acc: 0.8040\n",
      "Epoch 260, Loss: 0.4499, Val Loss: 0.4567, Acc: 0.8209, Val Acc: 0.7983\n",
      "Epoch 270, Loss: 0.4103, Val Loss: 0.4549, Acc: 0.8507, Val Acc: 0.8011\n",
      "Epoch 280, Loss: 0.4183, Val Loss: 0.4513, Acc: 0.8330, Val Acc: 0.7983\n",
      "Epoch 290, Loss: 0.4153, Val Loss: 0.4503, Acc: 0.8436, Val Acc: 0.7983\n",
      "Best Validation Loss: 0.4393\n",
      "Fold Test Accuracy: 0.8156, F1-score: 0.8096, Homogeneity: 0.8101, NMI: 0.8149, ARI: 0.7048\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7120, Val Loss: 2.5580, Acc: 0.0760, Val Acc: 0.0795\n",
      "Epoch 10, Loss: 2.4269, Val Loss: 2.3863, Acc: 0.1756, Val Acc: 0.2727\n",
      "Epoch 20, Loss: 2.2003, Val Loss: 2.1145, Acc: 0.2253, Val Acc: 0.3239\n",
      "Epoch 30, Loss: 1.9682, Val Loss: 1.8397, Acc: 0.2857, Val Acc: 0.4062\n",
      "Epoch 40, Loss: 1.7441, Val Loss: 1.5855, Acc: 0.3475, Val Acc: 0.3892\n",
      "Epoch 50, Loss: 1.5283, Val Loss: 1.3467, Acc: 0.4094, Val Acc: 0.5426\n",
      "Epoch 60, Loss: 1.3312, Val Loss: 1.1500, Acc: 0.4741, Val Acc: 0.6222\n",
      "Epoch 70, Loss: 1.1413, Val Loss: 0.9951, Acc: 0.5572, Val Acc: 0.6449\n",
      "Epoch 80, Loss: 1.0554, Val Loss: 0.8766, Acc: 0.5778, Val Acc: 0.6648\n",
      "Epoch 90, Loss: 0.9110, Val Loss: 0.7780, Acc: 0.6347, Val Acc: 0.6960\n",
      "Epoch 100, Loss: 0.8222, Val Loss: 0.7156, Acc: 0.6681, Val Acc: 0.6960\n",
      "Epoch 110, Loss: 0.7757, Val Loss: 0.6863, Acc: 0.6823, Val Acc: 0.7216\n",
      "Epoch 120, Loss: 0.6894, Val Loss: 0.6311, Acc: 0.7186, Val Acc: 0.7131\n",
      "Epoch 130, Loss: 0.6370, Val Loss: 0.6264, Acc: 0.7363, Val Acc: 0.7131\n",
      "Epoch 140, Loss: 0.5797, Val Loss: 0.6738, Acc: 0.7626, Val Acc: 0.7102\n",
      "Epoch 150, Loss: 0.5606, Val Loss: 0.6197, Acc: 0.7839, Val Acc: 0.7102\n",
      "Epoch 160, Loss: 0.5450, Val Loss: 0.5143, Acc: 0.7903, Val Acc: 0.7898\n",
      "Epoch 170, Loss: 0.5359, Val Loss: 0.5160, Acc: 0.7896, Val Acc: 0.7869\n",
      "Epoch 180, Loss: 0.5105, Val Loss: 0.5126, Acc: 0.7925, Val Acc: 0.7926\n",
      "Epoch 190, Loss: 0.4925, Val Loss: 0.5102, Acc: 0.8102, Val Acc: 0.7898\n",
      "Epoch 200, Loss: 0.5060, Val Loss: 0.5090, Acc: 0.8031, Val Acc: 0.7926\n",
      "Epoch 210, Loss: 0.4680, Val Loss: 0.5047, Acc: 0.8195, Val Acc: 0.7955\n",
      "Epoch 220, Loss: 0.4827, Val Loss: 0.5043, Acc: 0.8316, Val Acc: 0.7926\n",
      "Epoch 230, Loss: 0.4815, Val Loss: 0.4970, Acc: 0.8109, Val Acc: 0.7955\n",
      "Epoch 240, Loss: 0.4745, Val Loss: 0.4902, Acc: 0.8031, Val Acc: 0.7955\n",
      "Epoch 250, Loss: 0.4998, Val Loss: 0.4881, Acc: 0.7953, Val Acc: 0.7955\n",
      "Epoch 260, Loss: 0.4885, Val Loss: 0.4875, Acc: 0.8124, Val Acc: 0.7955\n",
      "Epoch 270, Loss: 0.4816, Val Loss: 0.4929, Acc: 0.8095, Val Acc: 0.7926\n",
      "Epoch 280, Loss: 0.5022, Val Loss: 0.4930, Acc: 0.7982, Val Acc: 0.7955\n",
      "Epoch 290, Loss: 0.4859, Val Loss: 0.4904, Acc: 0.8074, Val Acc: 0.7955\n",
      "Best Validation Loss: 0.4870\n",
      "Fold Test Accuracy: 0.7878, F1-score: 0.7743, Homogeneity: 0.7745, NMI: 0.7871, ARI: 0.6575\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7056, Val Loss: 2.5593, Acc: 0.0724, Val Acc: 0.0598\n",
      "Epoch 10, Loss: 2.4458, Val Loss: 2.4187, Acc: 0.1626, Val Acc: 0.2422\n",
      "Epoch 20, Loss: 2.1617, Val Loss: 2.0972, Acc: 0.2401, Val Acc: 0.3191\n",
      "Epoch 30, Loss: 1.9215, Val Loss: 1.7946, Acc: 0.2969, Val Acc: 0.3590\n",
      "Epoch 40, Loss: 1.7013, Val Loss: 1.5213, Acc: 0.3501, Val Acc: 0.4074\n",
      "Epoch 50, Loss: 1.4984, Val Loss: 1.3096, Acc: 0.4134, Val Acc: 0.5356\n",
      "Epoch 60, Loss: 1.3380, Val Loss: 1.1347, Acc: 0.4801, Val Acc: 0.6068\n",
      "Epoch 70, Loss: 1.1786, Val Loss: 0.9725, Acc: 0.5256, Val Acc: 0.6752\n",
      "Epoch 80, Loss: 1.0436, Val Loss: 0.8313, Acc: 0.5803, Val Acc: 0.7236\n",
      "Epoch 90, Loss: 0.9271, Val Loss: 0.7274, Acc: 0.6271, Val Acc: 0.7379\n",
      "Epoch 100, Loss: 0.8236, Val Loss: 0.6560, Acc: 0.6662, Val Acc: 0.7578\n",
      "Epoch 110, Loss: 0.7497, Val Loss: 0.6106, Acc: 0.7053, Val Acc: 0.7664\n",
      "Epoch 120, Loss: 0.7049, Val Loss: 0.5417, Acc: 0.7166, Val Acc: 0.7920\n",
      "Epoch 130, Loss: 0.6367, Val Loss: 0.5013, Acc: 0.7330, Val Acc: 0.7977\n",
      "Epoch 140, Loss: 0.5971, Val Loss: 0.4604, Acc: 0.7656, Val Acc: 0.8091\n",
      "Epoch 150, Loss: 0.5392, Val Loss: 0.4453, Acc: 0.7876, Val Acc: 0.8348\n",
      "Epoch 160, Loss: 0.5125, Val Loss: 0.4390, Acc: 0.7940, Val Acc: 0.8205\n",
      "Epoch 170, Loss: 0.4405, Val Loss: 0.4118, Acc: 0.8338, Val Acc: 0.8405\n",
      "Epoch 180, Loss: 0.4395, Val Loss: 0.4057, Acc: 0.8168, Val Acc: 0.8262\n",
      "Epoch 190, Loss: 0.3711, Val Loss: 0.3642, Acc: 0.8658, Val Acc: 0.8575\n",
      "Epoch 200, Loss: 0.3454, Val Loss: 0.3658, Acc: 0.8679, Val Acc: 0.8632\n",
      "Epoch 210, Loss: 0.3209, Val Loss: 0.3597, Acc: 0.8750, Val Acc: 0.8632\n",
      "Epoch 220, Loss: 0.3199, Val Loss: 0.3369, Acc: 0.8857, Val Acc: 0.8775\n",
      "Epoch 230, Loss: 0.2940, Val Loss: 0.3437, Acc: 0.8906, Val Acc: 0.8689\n",
      "Epoch 240, Loss: 0.3131, Val Loss: 0.3329, Acc: 0.8949, Val Acc: 0.8746\n",
      "Epoch 250, Loss: 0.3158, Val Loss: 0.3241, Acc: 0.8835, Val Acc: 0.8775\n",
      "Epoch 260, Loss: 0.2806, Val Loss: 0.3302, Acc: 0.9020, Val Acc: 0.8803\n",
      "Epoch 270, Loss: 0.2755, Val Loss: 0.3326, Acc: 0.8977, Val Acc: 0.8746\n",
      "Epoch 280, Loss: 0.2648, Val Loss: 0.3250, Acc: 0.9027, Val Acc: 0.8775\n",
      "Epoch 290, Loss: 0.2793, Val Loss: 0.3241, Acc: 0.8899, Val Acc: 0.8775\n",
      "Best Validation Loss: 0.3227\n",
      "Fold Test Accuracy: 0.8727, F1-score: 0.8729, Homogeneity: 0.8490, NMI: 0.8503, ARI: 0.7733\n",
      "  Average Metrics for Threshold 0.08: {'test_accuracy': 0.8196286472148542, 'f1_score': 0.8133218716156844, 'homogeneity': 0.8050171970294876, 'nmi': 0.8113834367711013, 'ari': 0.702723960809965}\n",
      "\n",
      "Training with threshold: 0.09000000000000001\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7126, Val Loss: 2.5616, Acc: 0.0796, Val Acc: 0.0881\n",
      "Epoch 10, Loss: 2.4664, Val Loss: 2.4538, Acc: 0.1457, Val Acc: 0.2500\n",
      "Epoch 20, Loss: 2.2100, Val Loss: 2.1586, Acc: 0.2161, Val Acc: 0.2983\n",
      "Epoch 30, Loss: 1.9331, Val Loss: 1.8544, Acc: 0.2850, Val Acc: 0.3608\n",
      "Epoch 40, Loss: 1.6900, Val Loss: 1.5798, Acc: 0.3475, Val Acc: 0.4489\n",
      "Epoch 50, Loss: 1.4871, Val Loss: 1.3422, Acc: 0.4236, Val Acc: 0.5312\n",
      "Epoch 60, Loss: 1.3106, Val Loss: 1.1593, Acc: 0.4869, Val Acc: 0.6108\n",
      "Epoch 70, Loss: 1.1610, Val Loss: 0.9966, Acc: 0.5366, Val Acc: 0.6449\n",
      "Epoch 80, Loss: 1.0212, Val Loss: 0.8491, Acc: 0.5842, Val Acc: 0.7045\n",
      "Epoch 90, Loss: 0.9022, Val Loss: 0.7494, Acc: 0.6347, Val Acc: 0.7131\n",
      "Epoch 100, Loss: 0.7933, Val Loss: 0.7346, Acc: 0.6859, Val Acc: 0.6989\n",
      "Epoch 110, Loss: 0.7454, Val Loss: 0.7136, Acc: 0.7001, Val Acc: 0.6903\n",
      "Epoch 120, Loss: 0.6965, Val Loss: 0.6429, Acc: 0.7122, Val Acc: 0.7386\n",
      "Epoch 130, Loss: 0.6587, Val Loss: 0.6192, Acc: 0.7392, Val Acc: 0.7443\n",
      "Epoch 140, Loss: 0.6190, Val Loss: 0.5907, Acc: 0.7711, Val Acc: 0.7699\n",
      "Epoch 150, Loss: 0.6342, Val Loss: 0.5709, Acc: 0.7456, Val Acc: 0.7898\n",
      "Epoch 160, Loss: 0.5804, Val Loss: 0.5556, Acc: 0.7676, Val Acc: 0.7784\n",
      "Epoch 170, Loss: 0.5668, Val Loss: 0.5632, Acc: 0.7811, Val Acc: 0.7557\n",
      "Epoch 180, Loss: 0.5521, Val Loss: 0.5353, Acc: 0.7775, Val Acc: 0.7812\n",
      "Epoch 190, Loss: 0.5388, Val Loss: 0.5209, Acc: 0.7953, Val Acc: 0.8011\n",
      "Epoch 200, Loss: 0.5386, Val Loss: 0.5158, Acc: 0.7804, Val Acc: 0.7983\n",
      "Epoch 210, Loss: 0.5130, Val Loss: 0.5033, Acc: 0.8010, Val Acc: 0.7898\n",
      "Epoch 220, Loss: 0.5252, Val Loss: 0.4958, Acc: 0.8095, Val Acc: 0.8040\n",
      "Epoch 230, Loss: 0.4961, Val Loss: 0.4926, Acc: 0.8102, Val Acc: 0.7955\n",
      "Epoch 240, Loss: 0.4753, Val Loss: 0.4895, Acc: 0.8266, Val Acc: 0.8011\n",
      "Epoch 250, Loss: 0.4857, Val Loss: 0.4827, Acc: 0.8088, Val Acc: 0.8040\n",
      "Epoch 260, Loss: 0.4751, Val Loss: 0.4832, Acc: 0.8138, Val Acc: 0.8040\n",
      "Epoch 270, Loss: 0.4394, Val Loss: 0.4845, Acc: 0.8280, Val Acc: 0.8097\n",
      "Epoch 280, Loss: 0.4666, Val Loss: 0.4849, Acc: 0.8181, Val Acc: 0.8097\n",
      "Epoch 290, Loss: 0.4673, Val Loss: 0.4835, Acc: 0.8252, Val Acc: 0.8097\n",
      "Best Validation Loss: 0.4824\n",
      "Fold Test Accuracy: 0.8156, F1-score: 0.8006, Homogeneity: 0.8057, NMI: 0.8155, ARI: 0.7072\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6953, Val Loss: 2.5597, Acc: 0.0824, Val Acc: 0.0966\n",
      "Epoch 10, Loss: 2.4700, Val Loss: 2.4253, Acc: 0.1528, Val Acc: 0.2415\n",
      "Epoch 20, Loss: 2.2029, Val Loss: 2.1428, Acc: 0.2196, Val Acc: 0.2841\n",
      "Epoch 30, Loss: 1.9655, Val Loss: 1.8692, Acc: 0.2893, Val Acc: 0.3722\n",
      "Epoch 40, Loss: 1.7657, Val Loss: 1.5870, Acc: 0.3355, Val Acc: 0.4858\n",
      "Epoch 50, Loss: 1.5403, Val Loss: 1.3359, Acc: 0.4051, Val Acc: 0.5682\n",
      "Epoch 60, Loss: 1.3353, Val Loss: 1.1332, Acc: 0.4705, Val Acc: 0.6733\n",
      "Epoch 70, Loss: 1.1827, Val Loss: 0.9707, Acc: 0.5359, Val Acc: 0.7159\n",
      "Epoch 80, Loss: 1.0498, Val Loss: 0.8399, Acc: 0.5885, Val Acc: 0.7500\n",
      "Epoch 90, Loss: 0.9597, Val Loss: 0.7448, Acc: 0.6141, Val Acc: 0.7642\n",
      "Epoch 100, Loss: 0.8520, Val Loss: 0.6915, Acc: 0.6525, Val Acc: 0.7642\n",
      "Epoch 110, Loss: 0.7490, Val Loss: 0.6721, Acc: 0.6986, Val Acc: 0.7415\n",
      "Epoch 120, Loss: 0.6755, Val Loss: 0.5633, Acc: 0.7306, Val Acc: 0.8011\n",
      "Epoch 130, Loss: 0.6293, Val Loss: 0.5719, Acc: 0.7413, Val Acc: 0.7926\n",
      "Epoch 140, Loss: 0.6047, Val Loss: 0.5351, Acc: 0.7534, Val Acc: 0.7926\n",
      "Epoch 150, Loss: 0.5881, Val Loss: 0.4668, Acc: 0.7662, Val Acc: 0.8239\n",
      "Epoch 160, Loss: 0.5577, Val Loss: 0.4458, Acc: 0.7797, Val Acc: 0.8295\n",
      "Epoch 170, Loss: 0.5046, Val Loss: 0.4472, Acc: 0.8045, Val Acc: 0.8267\n",
      "Epoch 180, Loss: 0.5017, Val Loss: 0.4377, Acc: 0.8060, Val Acc: 0.8267\n",
      "Epoch 190, Loss: 0.4889, Val Loss: 0.4271, Acc: 0.8095, Val Acc: 0.8295\n",
      "Epoch 200, Loss: 0.4715, Val Loss: 0.4258, Acc: 0.8273, Val Acc: 0.8324\n",
      "Epoch 210, Loss: 0.4590, Val Loss: 0.4324, Acc: 0.8294, Val Acc: 0.8352\n",
      "Epoch 220, Loss: 0.4469, Val Loss: 0.4270, Acc: 0.8273, Val Acc: 0.8381\n",
      "Epoch 230, Loss: 0.4371, Val Loss: 0.4257, Acc: 0.8351, Val Acc: 0.8295\n",
      "Epoch 240, Loss: 0.4652, Val Loss: 0.4235, Acc: 0.8237, Val Acc: 0.8295\n",
      "Epoch 250, Loss: 0.4312, Val Loss: 0.4218, Acc: 0.8351, Val Acc: 0.8324\n",
      "Epoch 260, Loss: 0.4697, Val Loss: 0.4230, Acc: 0.8273, Val Acc: 0.8324\n",
      "Epoch 270, Loss: 0.4692, Val Loss: 0.4191, Acc: 0.8195, Val Acc: 0.8352\n",
      "Epoch 280, Loss: 0.4767, Val Loss: 0.4177, Acc: 0.8209, Val Acc: 0.8352\n",
      "Epoch 290, Loss: 0.4614, Val Loss: 0.4166, Acc: 0.8124, Val Acc: 0.8381\n",
      "Best Validation Loss: 0.4158\n",
      "Fold Test Accuracy: 0.8130, F1-score: 0.8092, Homogeneity: 0.7944, NMI: 0.7996, ARI: 0.6890\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7236, Val Loss: 2.5633, Acc: 0.0746, Val Acc: 0.0824\n",
      "Epoch 10, Loss: 2.4691, Val Loss: 2.4254, Acc: 0.1457, Val Acc: 0.2614\n",
      "Epoch 20, Loss: 2.1893, Val Loss: 2.1101, Acc: 0.2296, Val Acc: 0.2926\n",
      "Epoch 30, Loss: 1.9687, Val Loss: 1.8369, Acc: 0.2637, Val Acc: 0.3580\n",
      "Epoch 40, Loss: 1.7396, Val Loss: 1.5920, Acc: 0.3284, Val Acc: 0.4290\n",
      "Epoch 50, Loss: 1.5710, Val Loss: 1.3713, Acc: 0.4009, Val Acc: 0.5483\n",
      "Epoch 60, Loss: 1.3876, Val Loss: 1.1868, Acc: 0.4385, Val Acc: 0.6449\n",
      "Epoch 70, Loss: 1.2382, Val Loss: 1.0328, Acc: 0.4982, Val Acc: 0.6562\n",
      "Epoch 80, Loss: 1.1085, Val Loss: 0.8766, Acc: 0.5636, Val Acc: 0.6790\n",
      "Epoch 90, Loss: 0.9408, Val Loss: 0.7754, Acc: 0.6198, Val Acc: 0.7017\n",
      "Epoch 100, Loss: 0.8570, Val Loss: 0.7071, Acc: 0.6596, Val Acc: 0.7244\n",
      "Epoch 110, Loss: 0.7746, Val Loss: 0.6400, Acc: 0.6773, Val Acc: 0.7642\n",
      "Epoch 120, Loss: 0.7145, Val Loss: 0.7099, Acc: 0.7029, Val Acc: 0.6875\n",
      "Epoch 130, Loss: 0.6648, Val Loss: 0.5904, Acc: 0.7377, Val Acc: 0.7670\n",
      "Epoch 140, Loss: 0.6555, Val Loss: 0.5485, Acc: 0.7306, Val Acc: 0.7841\n",
      "Epoch 150, Loss: 0.6177, Val Loss: 0.5335, Acc: 0.7498, Val Acc: 0.7955\n",
      "Epoch 160, Loss: 0.6036, Val Loss: 0.5115, Acc: 0.7584, Val Acc: 0.7926\n",
      "Epoch 170, Loss: 0.5494, Val Loss: 0.4947, Acc: 0.7775, Val Acc: 0.8097\n",
      "Epoch 180, Loss: 0.5322, Val Loss: 0.5006, Acc: 0.7925, Val Acc: 0.7869\n",
      "Epoch 190, Loss: 0.5360, Val Loss: 0.4849, Acc: 0.7861, Val Acc: 0.7983\n",
      "Epoch 200, Loss: 0.5099, Val Loss: 0.4687, Acc: 0.7896, Val Acc: 0.8097\n",
      "Epoch 210, Loss: 0.5056, Val Loss: 0.4653, Acc: 0.7918, Val Acc: 0.8097\n",
      "Epoch 220, Loss: 0.5219, Val Loss: 0.4572, Acc: 0.7889, Val Acc: 0.8153\n",
      "Epoch 230, Loss: 0.4702, Val Loss: 0.4543, Acc: 0.8159, Val Acc: 0.8040\n",
      "Epoch 240, Loss: 0.4742, Val Loss: 0.4423, Acc: 0.8138, Val Acc: 0.8324\n",
      "Epoch 250, Loss: 0.4582, Val Loss: 0.4385, Acc: 0.8131, Val Acc: 0.8182\n",
      "Epoch 260, Loss: 0.4515, Val Loss: 0.4337, Acc: 0.8216, Val Acc: 0.8182\n",
      "Epoch 270, Loss: 0.4608, Val Loss: 0.4357, Acc: 0.8244, Val Acc: 0.8182\n",
      "Epoch 280, Loss: 0.4463, Val Loss: 0.4291, Acc: 0.8280, Val Acc: 0.8210\n",
      "Epoch 290, Loss: 0.4343, Val Loss: 0.4217, Acc: 0.8308, Val Acc: 0.8352\n",
      "Best Validation Loss: 0.4165\n",
      "Fold Test Accuracy: 0.8302, F1-score: 0.8277, Homogeneity: 0.8166, NMI: 0.8202, ARI: 0.7187\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7155, Val Loss: 2.5611, Acc: 0.0739, Val Acc: 0.0966\n",
      "Epoch 10, Loss: 2.4495, Val Loss: 2.4193, Acc: 0.1599, Val Acc: 0.2869\n",
      "Epoch 20, Loss: 2.1890, Val Loss: 2.0992, Acc: 0.2416, Val Acc: 0.3040\n",
      "Epoch 30, Loss: 1.9414, Val Loss: 1.8415, Acc: 0.2964, Val Acc: 0.3551\n",
      "Epoch 40, Loss: 1.7102, Val Loss: 1.6022, Acc: 0.3582, Val Acc: 0.3807\n",
      "Epoch 50, Loss: 1.5348, Val Loss: 1.3717, Acc: 0.4151, Val Acc: 0.5000\n",
      "Epoch 60, Loss: 1.3425, Val Loss: 1.1830, Acc: 0.4712, Val Acc: 0.5938\n",
      "Epoch 70, Loss: 1.1689, Val Loss: 1.0377, Acc: 0.5366, Val Acc: 0.6278\n",
      "Epoch 80, Loss: 1.0441, Val Loss: 0.9097, Acc: 0.5593, Val Acc: 0.6619\n",
      "Epoch 90, Loss: 0.9671, Val Loss: 0.8142, Acc: 0.6148, Val Acc: 0.6562\n",
      "Epoch 100, Loss: 0.8535, Val Loss: 0.7664, Acc: 0.6375, Val Acc: 0.6932\n",
      "Epoch 110, Loss: 0.7709, Val Loss: 0.7565, Acc: 0.6873, Val Acc: 0.6847\n",
      "Epoch 120, Loss: 0.7193, Val Loss: 0.6818, Acc: 0.7086, Val Acc: 0.6847\n",
      "Epoch 130, Loss: 0.7056, Val Loss: 0.6393, Acc: 0.7065, Val Acc: 0.7159\n",
      "Epoch 140, Loss: 0.6655, Val Loss: 0.6255, Acc: 0.7377, Val Acc: 0.7244\n",
      "Epoch 150, Loss: 0.6461, Val Loss: 0.6183, Acc: 0.7306, Val Acc: 0.7188\n",
      "Epoch 160, Loss: 0.6266, Val Loss: 0.5851, Acc: 0.7534, Val Acc: 0.7415\n",
      "Epoch 170, Loss: 0.5883, Val Loss: 0.5626, Acc: 0.7719, Val Acc: 0.7585\n",
      "Epoch 180, Loss: 0.5721, Val Loss: 0.5531, Acc: 0.7591, Val Acc: 0.7670\n",
      "Epoch 190, Loss: 0.5244, Val Loss: 0.5342, Acc: 0.7882, Val Acc: 0.7756\n",
      "Epoch 200, Loss: 0.5320, Val Loss: 0.5057, Acc: 0.7946, Val Acc: 0.7869\n",
      "Epoch 210, Loss: 0.4701, Val Loss: 0.4957, Acc: 0.8109, Val Acc: 0.7926\n",
      "Epoch 220, Loss: 0.4818, Val Loss: 0.4849, Acc: 0.8216, Val Acc: 0.7955\n",
      "Epoch 230, Loss: 0.4766, Val Loss: 0.4828, Acc: 0.8053, Val Acc: 0.7926\n",
      "Epoch 240, Loss: 0.4442, Val Loss: 0.4801, Acc: 0.8280, Val Acc: 0.8011\n",
      "Epoch 250, Loss: 0.4045, Val Loss: 0.4950, Acc: 0.8436, Val Acc: 0.7926\n",
      "Epoch 260, Loss: 0.3935, Val Loss: 0.4724, Acc: 0.8550, Val Acc: 0.8153\n",
      "Epoch 270, Loss: 0.3821, Val Loss: 0.4659, Acc: 0.8643, Val Acc: 0.8097\n",
      "Epoch 280, Loss: 0.4065, Val Loss: 0.4664, Acc: 0.8408, Val Acc: 0.8153\n",
      "Epoch 290, Loss: 0.3947, Val Loss: 0.4646, Acc: 0.8550, Val Acc: 0.8239\n",
      "Best Validation Loss: 0.4626\n",
      "Fold Test Accuracy: 0.8170, F1-score: 0.8139, Homogeneity: 0.7934, NMI: 0.8011, ARI: 0.6848\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6968, Val Loss: 2.5601, Acc: 0.0732, Val Acc: 0.1140\n",
      "Epoch 10, Loss: 2.4419, Val Loss: 2.4138, Acc: 0.1783, Val Acc: 0.2251\n",
      "Epoch 20, Loss: 2.1759, Val Loss: 2.1197, Acc: 0.2365, Val Acc: 0.2792\n",
      "Epoch 30, Loss: 1.9296, Val Loss: 1.8232, Acc: 0.2955, Val Acc: 0.3533\n",
      "Epoch 40, Loss: 1.7175, Val Loss: 1.5172, Acc: 0.3274, Val Acc: 0.4758\n",
      "Epoch 50, Loss: 1.5092, Val Loss: 1.2875, Acc: 0.4041, Val Acc: 0.5470\n",
      "Epoch 60, Loss: 1.3188, Val Loss: 1.1177, Acc: 0.4773, Val Acc: 0.6239\n",
      "Epoch 70, Loss: 1.1500, Val Loss: 0.9596, Acc: 0.5440, Val Acc: 0.6724\n",
      "Epoch 80, Loss: 1.0287, Val Loss: 0.8283, Acc: 0.5866, Val Acc: 0.7123\n",
      "Epoch 90, Loss: 0.9211, Val Loss: 0.7347, Acc: 0.6136, Val Acc: 0.7350\n",
      "Epoch 100, Loss: 0.8205, Val Loss: 0.6947, Acc: 0.6726, Val Acc: 0.7265\n",
      "Epoch 110, Loss: 0.7489, Val Loss: 0.6299, Acc: 0.6903, Val Acc: 0.7493\n",
      "Epoch 120, Loss: 0.6734, Val Loss: 0.5542, Acc: 0.7166, Val Acc: 0.7920\n",
      "Epoch 130, Loss: 0.6058, Val Loss: 0.5478, Acc: 0.7592, Val Acc: 0.7550\n",
      "Epoch 140, Loss: 0.6046, Val Loss: 0.5830, Acc: 0.7486, Val Acc: 0.7607\n",
      "Epoch 150, Loss: 0.5611, Val Loss: 0.4865, Acc: 0.7656, Val Acc: 0.7778\n",
      "Epoch 160, Loss: 0.5151, Val Loss: 0.4497, Acc: 0.7919, Val Acc: 0.8120\n",
      "Epoch 170, Loss: 0.5066, Val Loss: 0.4741, Acc: 0.8033, Val Acc: 0.8205\n",
      "Epoch 180, Loss: 0.4779, Val Loss: 0.4626, Acc: 0.8089, Val Acc: 0.8205\n",
      "Epoch 190, Loss: 0.4909, Val Loss: 0.4145, Acc: 0.8061, Val Acc: 0.8405\n",
      "Epoch 200, Loss: 0.4874, Val Loss: 0.4038, Acc: 0.8047, Val Acc: 0.8405\n",
      "Epoch 210, Loss: 0.4799, Val Loss: 0.3987, Acc: 0.7969, Val Acc: 0.8433\n",
      "Epoch 220, Loss: 0.4494, Val Loss: 0.3959, Acc: 0.8345, Val Acc: 0.8462\n",
      "Epoch 230, Loss: 0.4405, Val Loss: 0.3901, Acc: 0.8324, Val Acc: 0.8433\n",
      "Epoch 240, Loss: 0.4266, Val Loss: 0.3854, Acc: 0.8445, Val Acc: 0.8433\n",
      "Epoch 250, Loss: 0.4253, Val Loss: 0.3820, Acc: 0.8430, Val Acc: 0.8575\n",
      "Epoch 260, Loss: 0.4089, Val Loss: 0.3870, Acc: 0.8551, Val Acc: 0.8547\n",
      "Epoch 270, Loss: 0.3979, Val Loss: 0.3776, Acc: 0.8445, Val Acc: 0.8490\n",
      "Epoch 280, Loss: 0.3888, Val Loss: 0.3782, Acc: 0.8459, Val Acc: 0.8433\n",
      "Epoch 290, Loss: 0.3996, Val Loss: 0.3757, Acc: 0.8551, Val Acc: 0.8547\n",
      "Best Validation Loss: 0.3733\n",
      "Fold Test Accuracy: 0.8369, F1-score: 0.8339, Homogeneity: 0.8265, NMI: 0.8300, ARI: 0.7313\n",
      "  Average Metrics for Threshold 0.09: {'test_accuracy': 0.8225464190981432, 'f1_score': 0.8170486219640237, 'homogeneity': 0.8073525852989297, 'nmi': 0.8132736260411777, 'ari': 0.7061775795412505}\n",
      "\n",
      "Training with threshold: 0.1\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7233, Val Loss: 2.5589, Acc: 0.0725, Val Acc: 0.0795\n",
      "Epoch 10, Loss: 2.4500, Val Loss: 2.4102, Acc: 0.1564, Val Acc: 0.2756\n",
      "Epoch 20, Loss: 2.1679, Val Loss: 2.1177, Acc: 0.2431, Val Acc: 0.3466\n",
      "Epoch 30, Loss: 1.9388, Val Loss: 1.8124, Acc: 0.3063, Val Acc: 0.4233\n",
      "Epoch 40, Loss: 1.6942, Val Loss: 1.5230, Acc: 0.3554, Val Acc: 0.4830\n",
      "Epoch 50, Loss: 1.4777, Val Loss: 1.2923, Acc: 0.4271, Val Acc: 0.5455\n",
      "Epoch 60, Loss: 1.2900, Val Loss: 1.1175, Acc: 0.4712, Val Acc: 0.6165\n",
      "Epoch 70, Loss: 1.1570, Val Loss: 0.9941, Acc: 0.5451, Val Acc: 0.6420\n",
      "Epoch 80, Loss: 1.0173, Val Loss: 0.8977, Acc: 0.5970, Val Acc: 0.6790\n",
      "Epoch 90, Loss: 0.9082, Val Loss: 0.7734, Acc: 0.6283, Val Acc: 0.7102\n",
      "Epoch 100, Loss: 0.8100, Val Loss: 0.7178, Acc: 0.6844, Val Acc: 0.7358\n",
      "Epoch 110, Loss: 0.7331, Val Loss: 0.6939, Acc: 0.7029, Val Acc: 0.7244\n",
      "Epoch 120, Loss: 0.6508, Val Loss: 0.7324, Acc: 0.7392, Val Acc: 0.6790\n",
      "Epoch 130, Loss: 0.6387, Val Loss: 0.6474, Acc: 0.7505, Val Acc: 0.7386\n",
      "Epoch 140, Loss: 0.5979, Val Loss: 0.6228, Acc: 0.7598, Val Acc: 0.7415\n",
      "Epoch 150, Loss: 0.5873, Val Loss: 0.5735, Acc: 0.7598, Val Acc: 0.7585\n",
      "Epoch 160, Loss: 0.5411, Val Loss: 0.5562, Acc: 0.7918, Val Acc: 0.7642\n",
      "Epoch 170, Loss: 0.5492, Val Loss: 0.5497, Acc: 0.8010, Val Acc: 0.7670\n",
      "Epoch 180, Loss: 0.5073, Val Loss: 0.5382, Acc: 0.7967, Val Acc: 0.7756\n",
      "Epoch 190, Loss: 0.4994, Val Loss: 0.5478, Acc: 0.7996, Val Acc: 0.7614\n",
      "Epoch 200, Loss: 0.4774, Val Loss: 0.5163, Acc: 0.8266, Val Acc: 0.7812\n",
      "Epoch 210, Loss: 0.4436, Val Loss: 0.5146, Acc: 0.8330, Val Acc: 0.7841\n",
      "Epoch 220, Loss: 0.4367, Val Loss: 0.5123, Acc: 0.8330, Val Acc: 0.7756\n",
      "Epoch 230, Loss: 0.4574, Val Loss: 0.5098, Acc: 0.8266, Val Acc: 0.7784\n",
      "Epoch 240, Loss: 0.4529, Val Loss: 0.5056, Acc: 0.8216, Val Acc: 0.7926\n",
      "Epoch 250, Loss: 0.4455, Val Loss: 0.5000, Acc: 0.8209, Val Acc: 0.7869\n",
      "Epoch 260, Loss: 0.4455, Val Loss: 0.5020, Acc: 0.8294, Val Acc: 0.7869\n",
      "Epoch 270, Loss: 0.4475, Val Loss: 0.5032, Acc: 0.8195, Val Acc: 0.7869\n",
      "Epoch 280, Loss: 0.4269, Val Loss: 0.5025, Acc: 0.8301, Val Acc: 0.7869\n",
      "Epoch 290, Loss: 0.4253, Val Loss: 0.5034, Acc: 0.8351, Val Acc: 0.7869\n",
      "Best Validation Loss: 0.4989\n",
      "Fold Test Accuracy: 0.8090, F1-score: 0.7975, Homogeneity: 0.7927, NMI: 0.8026, ARI: 0.6889\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7180, Val Loss: 2.5639, Acc: 0.0661, Val Acc: 0.0909\n",
      "Epoch 10, Loss: 2.4823, Val Loss: 2.4615, Acc: 0.1457, Val Acc: 0.2898\n",
      "Epoch 20, Loss: 2.2148, Val Loss: 2.1635, Acc: 0.2345, Val Acc: 0.2784\n",
      "Epoch 30, Loss: 1.9961, Val Loss: 1.8973, Acc: 0.2765, Val Acc: 0.2983\n",
      "Epoch 40, Loss: 1.7331, Val Loss: 1.6142, Acc: 0.3483, Val Acc: 0.4261\n",
      "Epoch 50, Loss: 1.5506, Val Loss: 1.3651, Acc: 0.4044, Val Acc: 0.5540\n",
      "Epoch 60, Loss: 1.3373, Val Loss: 1.1500, Acc: 0.4733, Val Acc: 0.6051\n",
      "Epoch 70, Loss: 1.1632, Val Loss: 0.9815, Acc: 0.5494, Val Acc: 0.6733\n",
      "Epoch 80, Loss: 1.0129, Val Loss: 0.8650, Acc: 0.5906, Val Acc: 0.7216\n",
      "Epoch 90, Loss: 0.9258, Val Loss: 0.7589, Acc: 0.6190, Val Acc: 0.7557\n",
      "Epoch 100, Loss: 0.8240, Val Loss: 0.7578, Acc: 0.6588, Val Acc: 0.7159\n",
      "Epoch 110, Loss: 0.7497, Val Loss: 0.7339, Acc: 0.7029, Val Acc: 0.7244\n",
      "Epoch 120, Loss: 0.7061, Val Loss: 0.6303, Acc: 0.7001, Val Acc: 0.7756\n",
      "Epoch 130, Loss: 0.6721, Val Loss: 0.5839, Acc: 0.7306, Val Acc: 0.8011\n",
      "Epoch 140, Loss: 0.6343, Val Loss: 0.5649, Acc: 0.7306, Val Acc: 0.8011\n",
      "Epoch 150, Loss: 0.6477, Val Loss: 0.5453, Acc: 0.7470, Val Acc: 0.8125\n",
      "Epoch 160, Loss: 0.5897, Val Loss: 0.5293, Acc: 0.7534, Val Acc: 0.8097\n",
      "Epoch 170, Loss: 0.6038, Val Loss: 0.5096, Acc: 0.7541, Val Acc: 0.8210\n",
      "Epoch 180, Loss: 0.5388, Val Loss: 0.4883, Acc: 0.7882, Val Acc: 0.8182\n",
      "Epoch 190, Loss: 0.5040, Val Loss: 0.4850, Acc: 0.8131, Val Acc: 0.8182\n",
      "Epoch 200, Loss: 0.5160, Val Loss: 0.4785, Acc: 0.7925, Val Acc: 0.8210\n",
      "Epoch 210, Loss: 0.4788, Val Loss: 0.4514, Acc: 0.8308, Val Acc: 0.8381\n",
      "Epoch 220, Loss: 0.4508, Val Loss: 0.4379, Acc: 0.8237, Val Acc: 0.8381\n",
      "Epoch 230, Loss: 0.4542, Val Loss: 0.4253, Acc: 0.8252, Val Acc: 0.8438\n",
      "Epoch 240, Loss: 0.4489, Val Loss: 0.4180, Acc: 0.8330, Val Acc: 0.8438\n",
      "Epoch 250, Loss: 0.3695, Val Loss: 0.4032, Acc: 0.8571, Val Acc: 0.8580\n",
      "Epoch 260, Loss: 0.3747, Val Loss: 0.3988, Acc: 0.8593, Val Acc: 0.8551\n",
      "Epoch 270, Loss: 0.3503, Val Loss: 0.3917, Acc: 0.8877, Val Acc: 0.8466\n",
      "Epoch 280, Loss: 0.3508, Val Loss: 0.3791, Acc: 0.8749, Val Acc: 0.8523\n",
      "Epoch 290, Loss: 0.3482, Val Loss: 0.3851, Acc: 0.8785, Val Acc: 0.8580\n",
      "Best Validation Loss: 0.3756\n",
      "Fold Test Accuracy: 0.8382, F1-score: 0.8370, Homogeneity: 0.8160, NMI: 0.8181, ARI: 0.7224\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6613, Val Loss: 2.5625, Acc: 0.0817, Val Acc: 0.1108\n",
      "Epoch 10, Loss: 2.4332, Val Loss: 2.4041, Acc: 0.1663, Val Acc: 0.2188\n",
      "Epoch 20, Loss: 2.1573, Val Loss: 2.0912, Acc: 0.2523, Val Acc: 0.2273\n",
      "Epoch 30, Loss: 1.9323, Val Loss: 1.8194, Acc: 0.3120, Val Acc: 0.3608\n",
      "Epoch 40, Loss: 1.6940, Val Loss: 1.5670, Acc: 0.3532, Val Acc: 0.4119\n",
      "Epoch 50, Loss: 1.5004, Val Loss: 1.3339, Acc: 0.4151, Val Acc: 0.5170\n",
      "Epoch 60, Loss: 1.3464, Val Loss: 1.1461, Acc: 0.4705, Val Acc: 0.5767\n",
      "Epoch 70, Loss: 1.1731, Val Loss: 1.0006, Acc: 0.5238, Val Acc: 0.6278\n",
      "Epoch 80, Loss: 1.0444, Val Loss: 0.8646, Acc: 0.5771, Val Acc: 0.6903\n",
      "Epoch 90, Loss: 0.9317, Val Loss: 0.7593, Acc: 0.6262, Val Acc: 0.7017\n",
      "Epoch 100, Loss: 0.8318, Val Loss: 0.7196, Acc: 0.6688, Val Acc: 0.6960\n",
      "Epoch 110, Loss: 0.7485, Val Loss: 0.6626, Acc: 0.6866, Val Acc: 0.7386\n",
      "Epoch 120, Loss: 0.6853, Val Loss: 0.6351, Acc: 0.7249, Val Acc: 0.7216\n",
      "Epoch 130, Loss: 0.6223, Val Loss: 0.6087, Acc: 0.7619, Val Acc: 0.7188\n",
      "Epoch 140, Loss: 0.5716, Val Loss: 0.5469, Acc: 0.7740, Val Acc: 0.7528\n",
      "Epoch 150, Loss: 0.5327, Val Loss: 0.5057, Acc: 0.7989, Val Acc: 0.7841\n",
      "Epoch 160, Loss: 0.5052, Val Loss: 0.5381, Acc: 0.7939, Val Acc: 0.7727\n",
      "Epoch 170, Loss: 0.4857, Val Loss: 0.4723, Acc: 0.8152, Val Acc: 0.8011\n",
      "Epoch 180, Loss: 0.4330, Val Loss: 0.4474, Acc: 0.8316, Val Acc: 0.8097\n",
      "Epoch 190, Loss: 0.4418, Val Loss: 0.4359, Acc: 0.8351, Val Acc: 0.8125\n",
      "Epoch 200, Loss: 0.3889, Val Loss: 0.4202, Acc: 0.8621, Val Acc: 0.8210\n",
      "Epoch 210, Loss: 0.3968, Val Loss: 0.4184, Acc: 0.8515, Val Acc: 0.8239\n",
      "Epoch 220, Loss: 0.3897, Val Loss: 0.4009, Acc: 0.8650, Val Acc: 0.8409\n",
      "Epoch 230, Loss: 0.3350, Val Loss: 0.3933, Acc: 0.8913, Val Acc: 0.8409\n",
      "Epoch 240, Loss: 0.3354, Val Loss: 0.3859, Acc: 0.8778, Val Acc: 0.8438\n",
      "Epoch 250, Loss: 0.3358, Val Loss: 0.3954, Acc: 0.8749, Val Acc: 0.8409\n",
      "Epoch 260, Loss: 0.3302, Val Loss: 0.3782, Acc: 0.8785, Val Acc: 0.8466\n",
      "Epoch 270, Loss: 0.3286, Val Loss: 0.3756, Acc: 0.8756, Val Acc: 0.8438\n",
      "Epoch 280, Loss: 0.3213, Val Loss: 0.3685, Acc: 0.8863, Val Acc: 0.8438\n",
      "Epoch 290, Loss: 0.2868, Val Loss: 0.3635, Acc: 0.8977, Val Acc: 0.8523\n",
      "Best Validation Loss: 0.3635\n",
      "Fold Test Accuracy: 0.8528, F1-score: 0.8540, Homogeneity: 0.8293, NMI: 0.8306, ARI: 0.7405\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6987, Val Loss: 2.5596, Acc: 0.0753, Val Acc: 0.0994\n",
      "Epoch 10, Loss: 2.4515, Val Loss: 2.4143, Acc: 0.1684, Val Acc: 0.2528\n",
      "Epoch 20, Loss: 2.1569, Val Loss: 2.0960, Acc: 0.2331, Val Acc: 0.3580\n",
      "Epoch 30, Loss: 1.9146, Val Loss: 1.7910, Acc: 0.3149, Val Acc: 0.4261\n",
      "Epoch 40, Loss: 1.6934, Val Loss: 1.5289, Acc: 0.3639, Val Acc: 0.4574\n",
      "Epoch 50, Loss: 1.4460, Val Loss: 1.3038, Acc: 0.4321, Val Acc: 0.5312\n",
      "Epoch 60, Loss: 1.3012, Val Loss: 1.1507, Acc: 0.4840, Val Acc: 0.5540\n",
      "Epoch 70, Loss: 1.1703, Val Loss: 1.0131, Acc: 0.5366, Val Acc: 0.6051\n",
      "Epoch 80, Loss: 1.0347, Val Loss: 0.8758, Acc: 0.5800, Val Acc: 0.6648\n",
      "Epoch 90, Loss: 0.9377, Val Loss: 0.7732, Acc: 0.6269, Val Acc: 0.6989\n",
      "Epoch 100, Loss: 0.8351, Val Loss: 0.7105, Acc: 0.6816, Val Acc: 0.6818\n",
      "Epoch 110, Loss: 0.7068, Val Loss: 0.6718, Acc: 0.7235, Val Acc: 0.7102\n",
      "Epoch 120, Loss: 0.6662, Val Loss: 0.6088, Acc: 0.7299, Val Acc: 0.7415\n",
      "Epoch 130, Loss: 0.6239, Val Loss: 0.6397, Acc: 0.7527, Val Acc: 0.7301\n",
      "Epoch 140, Loss: 0.5864, Val Loss: 0.5803, Acc: 0.7640, Val Acc: 0.7557\n",
      "Epoch 150, Loss: 0.5486, Val Loss: 0.5284, Acc: 0.7854, Val Acc: 0.7841\n",
      "Epoch 160, Loss: 0.5330, Val Loss: 0.5147, Acc: 0.7967, Val Acc: 0.7812\n",
      "Epoch 170, Loss: 0.5008, Val Loss: 0.5103, Acc: 0.8053, Val Acc: 0.7869\n",
      "Epoch 180, Loss: 0.4731, Val Loss: 0.4823, Acc: 0.8280, Val Acc: 0.8040\n",
      "Epoch 190, Loss: 0.4568, Val Loss: 0.5007, Acc: 0.8209, Val Acc: 0.7869\n",
      "Epoch 200, Loss: 0.4166, Val Loss: 0.4896, Acc: 0.8472, Val Acc: 0.7898\n",
      "Epoch 210, Loss: 0.4081, Val Loss: 0.4806, Acc: 0.8458, Val Acc: 0.7955\n",
      "Epoch 220, Loss: 0.4455, Val Loss: 0.4808, Acc: 0.8316, Val Acc: 0.8011\n",
      "Epoch 230, Loss: 0.4397, Val Loss: 0.4772, Acc: 0.8223, Val Acc: 0.8011\n",
      "Epoch 240, Loss: 0.4161, Val Loss: 0.4743, Acc: 0.8308, Val Acc: 0.8011\n",
      "Epoch 250, Loss: 0.4333, Val Loss: 0.4677, Acc: 0.8344, Val Acc: 0.8068\n",
      "Epoch 260, Loss: 0.4284, Val Loss: 0.4737, Acc: 0.8358, Val Acc: 0.8011\n",
      "Epoch 270, Loss: 0.4088, Val Loss: 0.4749, Acc: 0.8550, Val Acc: 0.8011\n",
      "Epoch 280, Loss: 0.4064, Val Loss: 0.4719, Acc: 0.8564, Val Acc: 0.8068\n",
      "Epoch 290, Loss: 0.4164, Val Loss: 0.4729, Acc: 0.8415, Val Acc: 0.7983\n",
      "Best Validation Loss: 0.4677\n",
      "Fold Test Accuracy: 0.8156, F1-score: 0.8136, Homogeneity: 0.7939, NMI: 0.7987, ARI: 0.6874\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7001, Val Loss: 2.5584, Acc: 0.0767, Val Acc: 0.0912\n",
      "Epoch 10, Loss: 2.4897, Val Loss: 2.4289, Acc: 0.1449, Val Acc: 0.2393\n",
      "Epoch 20, Loss: 2.2112, Val Loss: 2.1286, Acc: 0.2180, Val Acc: 0.3333\n",
      "Epoch 30, Loss: 1.9612, Val Loss: 1.8206, Acc: 0.2990, Val Acc: 0.3875\n",
      "Epoch 40, Loss: 1.7169, Val Loss: 1.5239, Acc: 0.3622, Val Acc: 0.4758\n",
      "Epoch 50, Loss: 1.4941, Val Loss: 1.2903, Acc: 0.4325, Val Acc: 0.5470\n",
      "Epoch 60, Loss: 1.3290, Val Loss: 1.1234, Acc: 0.4666, Val Acc: 0.6211\n",
      "Epoch 70, Loss: 1.1554, Val Loss: 0.9805, Acc: 0.5384, Val Acc: 0.6553\n",
      "Epoch 80, Loss: 1.0598, Val Loss: 0.8237, Acc: 0.5710, Val Acc: 0.7009\n",
      "Epoch 90, Loss: 0.9110, Val Loss: 0.7313, Acc: 0.6307, Val Acc: 0.7350\n",
      "Epoch 100, Loss: 0.8262, Val Loss: 0.6838, Acc: 0.6690, Val Acc: 0.7379\n",
      "Epoch 110, Loss: 0.7357, Val Loss: 0.6006, Acc: 0.6974, Val Acc: 0.7778\n",
      "Epoch 120, Loss: 0.6910, Val Loss: 0.5933, Acc: 0.7344, Val Acc: 0.7749\n",
      "Epoch 130, Loss: 0.6164, Val Loss: 0.5765, Acc: 0.7557, Val Acc: 0.7578\n",
      "Epoch 140, Loss: 0.5815, Val Loss: 0.4774, Acc: 0.7678, Val Acc: 0.8063\n",
      "Epoch 150, Loss: 0.5458, Val Loss: 0.4521, Acc: 0.7749, Val Acc: 0.8034\n",
      "Epoch 160, Loss: 0.5052, Val Loss: 0.4784, Acc: 0.8047, Val Acc: 0.8034\n",
      "Epoch 170, Loss: 0.4634, Val Loss: 0.4306, Acc: 0.8075, Val Acc: 0.8291\n",
      "Epoch 180, Loss: 0.4563, Val Loss: 0.4048, Acc: 0.8260, Val Acc: 0.8433\n",
      "Epoch 190, Loss: 0.4432, Val Loss: 0.4062, Acc: 0.8509, Val Acc: 0.8405\n",
      "Epoch 200, Loss: 0.4394, Val Loss: 0.4006, Acc: 0.8295, Val Acc: 0.8462\n",
      "Epoch 210, Loss: 0.4236, Val Loss: 0.3950, Acc: 0.8381, Val Acc: 0.8490\n",
      "Epoch 220, Loss: 0.4153, Val Loss: 0.3869, Acc: 0.8459, Val Acc: 0.8433\n",
      "Epoch 230, Loss: 0.4087, Val Loss: 0.3817, Acc: 0.8459, Val Acc: 0.8575\n",
      "Epoch 240, Loss: 0.4094, Val Loss: 0.3779, Acc: 0.8338, Val Acc: 0.8519\n",
      "Epoch 250, Loss: 0.3771, Val Loss: 0.3783, Acc: 0.8544, Val Acc: 0.8575\n",
      "Epoch 260, Loss: 0.3378, Val Loss: 0.3749, Acc: 0.8729, Val Acc: 0.8604\n",
      "Epoch 270, Loss: 0.3767, Val Loss: 0.3720, Acc: 0.8693, Val Acc: 0.8575\n",
      "Epoch 280, Loss: 0.3530, Val Loss: 0.3702, Acc: 0.8629, Val Acc: 0.8604\n",
      "Epoch 290, Loss: 0.3459, Val Loss: 0.3678, Acc: 0.8643, Val Acc: 0.8604\n",
      "Best Validation Loss: 0.3671\n",
      "Fold Test Accuracy: 0.8369, F1-score: 0.8334, Homogeneity: 0.8227, NMI: 0.8263, ARI: 0.7279\n",
      "  Average Metrics for Threshold 0.10: {'test_accuracy': 0.8305039787798408, 'f1_score': 0.8270866334632359, 'homogeneity': 0.8109318240224498, 'nmi': 0.8152595877389042, 'ari': 0.7134363392038657}\n",
      "\n",
      "Training with threshold: 0.11000000000000001\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6643, Val Loss: 2.5640, Acc: 0.0853, Val Acc: 0.0625\n",
      "Epoch 10, Loss: 2.4562, Val Loss: 2.4364, Acc: 0.1585, Val Acc: 0.2301\n",
      "Epoch 20, Loss: 2.1973, Val Loss: 2.1554, Acc: 0.2431, Val Acc: 0.2926\n",
      "Epoch 30, Loss: 1.9708, Val Loss: 1.8908, Acc: 0.2992, Val Acc: 0.3608\n",
      "Epoch 40, Loss: 1.7447, Val Loss: 1.6269, Acc: 0.3497, Val Acc: 0.4290\n",
      "Epoch 50, Loss: 1.5367, Val Loss: 1.3673, Acc: 0.3923, Val Acc: 0.5256\n",
      "Epoch 60, Loss: 1.3345, Val Loss: 1.1526, Acc: 0.4776, Val Acc: 0.6136\n",
      "Epoch 70, Loss: 1.1828, Val Loss: 1.0058, Acc: 0.5316, Val Acc: 0.6534\n",
      "Epoch 80, Loss: 1.0626, Val Loss: 0.8786, Acc: 0.5757, Val Acc: 0.6847\n",
      "Epoch 90, Loss: 0.9442, Val Loss: 0.7593, Acc: 0.6112, Val Acc: 0.7358\n",
      "Epoch 100, Loss: 0.8416, Val Loss: 0.6983, Acc: 0.6624, Val Acc: 0.7301\n",
      "Epoch 110, Loss: 0.7650, Val Loss: 0.6635, Acc: 0.7043, Val Acc: 0.7216\n",
      "Epoch 120, Loss: 0.6618, Val Loss: 0.6528, Acc: 0.7299, Val Acc: 0.7045\n",
      "Epoch 130, Loss: 0.5985, Val Loss: 0.6476, Acc: 0.7640, Val Acc: 0.7102\n",
      "Epoch 140, Loss: 0.5879, Val Loss: 0.6539, Acc: 0.7619, Val Acc: 0.7017\n",
      "Epoch 150, Loss: 0.5506, Val Loss: 0.7048, Acc: 0.7889, Val Acc: 0.7017\n",
      "Epoch 160, Loss: 0.5077, Val Loss: 0.5388, Acc: 0.8060, Val Acc: 0.7557\n",
      "Epoch 170, Loss: 0.4810, Val Loss: 0.4833, Acc: 0.8145, Val Acc: 0.7869\n",
      "Epoch 180, Loss: 0.4535, Val Loss: 0.4784, Acc: 0.8266, Val Acc: 0.7898\n",
      "Epoch 190, Loss: 0.4844, Val Loss: 0.4694, Acc: 0.8188, Val Acc: 0.7926\n",
      "Epoch 200, Loss: 0.4285, Val Loss: 0.4738, Acc: 0.8316, Val Acc: 0.7841\n",
      "Epoch 210, Loss: 0.3927, Val Loss: 0.4624, Acc: 0.8422, Val Acc: 0.7898\n",
      "Epoch 220, Loss: 0.4030, Val Loss: 0.4388, Acc: 0.8429, Val Acc: 0.8153\n",
      "Epoch 230, Loss: 0.3861, Val Loss: 0.4506, Acc: 0.8593, Val Acc: 0.7983\n",
      "Epoch 240, Loss: 0.3839, Val Loss: 0.4501, Acc: 0.8628, Val Acc: 0.7955\n",
      "Epoch 250, Loss: 0.3700, Val Loss: 0.4402, Acc: 0.8600, Val Acc: 0.8068\n",
      "Epoch 260, Loss: 0.3770, Val Loss: 0.4384, Acc: 0.8614, Val Acc: 0.8097\n",
      "Epoch 270, Loss: 0.3370, Val Loss: 0.4393, Acc: 0.8856, Val Acc: 0.8097\n",
      "Epoch 280, Loss: 0.3679, Val Loss: 0.4422, Acc: 0.8650, Val Acc: 0.8125\n",
      "Epoch 290, Loss: 0.3835, Val Loss: 0.4411, Acc: 0.8529, Val Acc: 0.8125\n",
      "Best Validation Loss: 0.4368\n",
      "Fold Test Accuracy: 0.8316, F1-score: 0.8280, Homogeneity: 0.8055, NMI: 0.8104, ARI: 0.7118\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7201, Val Loss: 2.5621, Acc: 0.0803, Val Acc: 0.1136\n",
      "Epoch 10, Loss: 2.4507, Val Loss: 2.4159, Acc: 0.1628, Val Acc: 0.2244\n",
      "Epoch 20, Loss: 2.1942, Val Loss: 2.1406, Acc: 0.2303, Val Acc: 0.3352\n",
      "Epoch 30, Loss: 1.9577, Val Loss: 1.8425, Acc: 0.3049, Val Acc: 0.4062\n",
      "Epoch 40, Loss: 1.7220, Val Loss: 1.5512, Acc: 0.3397, Val Acc: 0.5142\n",
      "Epoch 50, Loss: 1.5207, Val Loss: 1.3042, Acc: 0.4144, Val Acc: 0.6023\n",
      "Epoch 60, Loss: 1.3036, Val Loss: 1.1153, Acc: 0.4776, Val Acc: 0.6591\n",
      "Epoch 70, Loss: 1.1614, Val Loss: 0.9881, Acc: 0.5437, Val Acc: 0.6733\n",
      "Epoch 80, Loss: 1.0263, Val Loss: 0.8707, Acc: 0.5672, Val Acc: 0.7188\n",
      "Epoch 90, Loss: 0.9280, Val Loss: 0.7808, Acc: 0.6148, Val Acc: 0.7386\n",
      "Epoch 100, Loss: 0.8215, Val Loss: 0.7246, Acc: 0.6738, Val Acc: 0.7443\n",
      "Epoch 110, Loss: 0.7402, Val Loss: 0.7210, Acc: 0.6951, Val Acc: 0.7358\n",
      "Epoch 120, Loss: 0.6689, Val Loss: 0.6511, Acc: 0.7356, Val Acc: 0.7443\n",
      "Epoch 130, Loss: 0.6384, Val Loss: 0.5794, Acc: 0.7448, Val Acc: 0.7812\n",
      "Epoch 140, Loss: 0.5969, Val Loss: 0.5452, Acc: 0.7584, Val Acc: 0.8040\n",
      "Epoch 150, Loss: 0.5193, Val Loss: 0.5074, Acc: 0.7960, Val Acc: 0.8182\n",
      "Epoch 160, Loss: 0.4986, Val Loss: 0.5002, Acc: 0.8031, Val Acc: 0.8097\n",
      "Epoch 170, Loss: 0.4692, Val Loss: 0.4810, Acc: 0.8209, Val Acc: 0.8210\n",
      "Epoch 180, Loss: 0.4442, Val Loss: 0.4422, Acc: 0.8387, Val Acc: 0.8324\n",
      "Epoch 190, Loss: 0.4044, Val Loss: 0.4322, Acc: 0.8557, Val Acc: 0.8381\n",
      "Epoch 200, Loss: 0.4206, Val Loss: 0.4418, Acc: 0.8422, Val Acc: 0.8324\n",
      "Epoch 210, Loss: 0.3845, Val Loss: 0.4080, Acc: 0.8579, Val Acc: 0.8352\n",
      "Epoch 220, Loss: 0.4117, Val Loss: 0.4091, Acc: 0.8415, Val Acc: 0.8381\n",
      "Epoch 230, Loss: 0.3534, Val Loss: 0.4092, Acc: 0.8643, Val Acc: 0.8466\n",
      "Epoch 240, Loss: 0.3275, Val Loss: 0.4137, Acc: 0.8813, Val Acc: 0.8352\n",
      "Epoch 250, Loss: 0.3471, Val Loss: 0.4194, Acc: 0.8742, Val Acc: 0.8239\n",
      "Epoch 260, Loss: 0.3396, Val Loss: 0.4216, Acc: 0.8742, Val Acc: 0.8295\n",
      "Epoch 270, Loss: 0.3494, Val Loss: 0.4226, Acc: 0.8799, Val Acc: 0.8295\n",
      "Epoch 280, Loss: 0.3940, Val Loss: 0.4244, Acc: 0.8593, Val Acc: 0.8295\n",
      "Epoch 290, Loss: 0.3395, Val Loss: 0.4243, Acc: 0.8721, Val Acc: 0.8295\n",
      "Best Validation Loss: 0.4042\n",
      "Fold Test Accuracy: 0.8369, F1-score: 0.8368, Homogeneity: 0.8164, NMI: 0.8188, ARI: 0.7213\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7028, Val Loss: 2.5590, Acc: 0.0697, Val Acc: 0.1193\n",
      "Epoch 10, Loss: 2.4294, Val Loss: 2.4074, Acc: 0.1734, Val Acc: 0.2699\n",
      "Epoch 20, Loss: 2.1722, Val Loss: 2.1109, Acc: 0.2402, Val Acc: 0.2955\n",
      "Epoch 30, Loss: 1.9535, Val Loss: 1.8260, Acc: 0.3085, Val Acc: 0.3153\n",
      "Epoch 40, Loss: 1.7500, Val Loss: 1.5837, Acc: 0.3419, Val Acc: 0.4602\n",
      "Epoch 50, Loss: 1.5382, Val Loss: 1.3596, Acc: 0.4165, Val Acc: 0.5455\n",
      "Epoch 60, Loss: 1.3556, Val Loss: 1.1361, Acc: 0.4783, Val Acc: 0.6506\n",
      "Epoch 70, Loss: 1.1659, Val Loss: 0.9589, Acc: 0.5416, Val Acc: 0.6761\n",
      "Epoch 80, Loss: 1.0241, Val Loss: 0.8275, Acc: 0.5984, Val Acc: 0.7017\n",
      "Epoch 90, Loss: 0.9075, Val Loss: 0.7528, Acc: 0.6411, Val Acc: 0.6989\n",
      "Epoch 100, Loss: 0.8278, Val Loss: 0.6899, Acc: 0.6631, Val Acc: 0.7244\n",
      "Epoch 110, Loss: 0.7368, Val Loss: 0.6374, Acc: 0.7150, Val Acc: 0.7585\n",
      "Epoch 120, Loss: 0.6540, Val Loss: 0.5993, Acc: 0.7335, Val Acc: 0.7557\n",
      "Epoch 130, Loss: 0.6244, Val Loss: 0.5395, Acc: 0.7406, Val Acc: 0.7812\n",
      "Epoch 140, Loss: 0.5741, Val Loss: 0.5478, Acc: 0.7804, Val Acc: 0.7841\n",
      "Epoch 150, Loss: 0.5301, Val Loss: 0.5001, Acc: 0.8074, Val Acc: 0.7898\n",
      "Epoch 160, Loss: 0.5062, Val Loss: 0.4749, Acc: 0.8095, Val Acc: 0.8267\n",
      "Epoch 170, Loss: 0.4735, Val Loss: 0.4552, Acc: 0.8138, Val Acc: 0.8381\n",
      "Epoch 180, Loss: 0.4559, Val Loss: 0.4454, Acc: 0.8202, Val Acc: 0.8438\n",
      "Epoch 190, Loss: 0.4669, Val Loss: 0.4442, Acc: 0.8124, Val Acc: 0.8324\n",
      "Epoch 200, Loss: 0.3958, Val Loss: 0.4226, Acc: 0.8500, Val Acc: 0.8551\n",
      "Epoch 210, Loss: 0.3972, Val Loss: 0.4139, Acc: 0.8308, Val Acc: 0.8494\n",
      "Epoch 220, Loss: 0.3979, Val Loss: 0.4269, Acc: 0.8579, Val Acc: 0.8352\n",
      "Epoch 230, Loss: 0.4093, Val Loss: 0.3989, Acc: 0.8465, Val Acc: 0.8608\n",
      "Epoch 240, Loss: 0.3524, Val Loss: 0.4068, Acc: 0.8714, Val Acc: 0.8551\n",
      "Epoch 250, Loss: 0.3784, Val Loss: 0.4124, Acc: 0.8593, Val Acc: 0.8409\n",
      "Epoch 260, Loss: 0.3590, Val Loss: 0.4157, Acc: 0.8671, Val Acc: 0.8352\n",
      "Epoch 270, Loss: 0.3481, Val Loss: 0.4120, Acc: 0.8770, Val Acc: 0.8381\n",
      "Epoch 280, Loss: 0.3643, Val Loss: 0.4172, Acc: 0.8628, Val Acc: 0.8352\n",
      "Epoch 290, Loss: 0.3323, Val Loss: 0.4190, Acc: 0.8799, Val Acc: 0.8352\n",
      "Best Validation Loss: 0.3975\n",
      "Fold Test Accuracy: 0.8355, F1-score: 0.8339, Homogeneity: 0.8136, NMI: 0.8172, ARI: 0.7190\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7125, Val Loss: 2.5635, Acc: 0.0803, Val Acc: 0.0739\n",
      "Epoch 10, Loss: 2.4800, Val Loss: 2.4391, Acc: 0.1457, Val Acc: 0.2699\n",
      "Epoch 20, Loss: 2.1925, Val Loss: 2.1377, Acc: 0.2217, Val Acc: 0.2898\n",
      "Epoch 30, Loss: 1.9791, Val Loss: 1.8641, Acc: 0.2878, Val Acc: 0.3608\n",
      "Epoch 40, Loss: 1.7792, Val Loss: 1.6244, Acc: 0.3149, Val Acc: 0.4006\n",
      "Epoch 50, Loss: 1.5765, Val Loss: 1.4047, Acc: 0.3888, Val Acc: 0.4943\n",
      "Epoch 60, Loss: 1.3821, Val Loss: 1.2155, Acc: 0.4520, Val Acc: 0.5625\n",
      "Epoch 70, Loss: 1.2383, Val Loss: 1.0737, Acc: 0.4925, Val Acc: 0.5938\n",
      "Epoch 80, Loss: 1.0881, Val Loss: 0.9260, Acc: 0.5665, Val Acc: 0.6278\n",
      "Epoch 90, Loss: 0.9581, Val Loss: 0.8347, Acc: 0.6212, Val Acc: 0.6705\n",
      "Epoch 100, Loss: 0.8696, Val Loss: 0.7684, Acc: 0.6610, Val Acc: 0.6733\n",
      "Epoch 110, Loss: 0.7610, Val Loss: 0.7144, Acc: 0.6866, Val Acc: 0.7045\n",
      "Epoch 120, Loss: 0.6962, Val Loss: 0.6930, Acc: 0.7235, Val Acc: 0.7074\n",
      "Epoch 130, Loss: 0.6564, Val Loss: 0.6815, Acc: 0.7385, Val Acc: 0.7017\n",
      "Epoch 140, Loss: 0.5743, Val Loss: 0.6003, Acc: 0.7690, Val Acc: 0.7415\n",
      "Epoch 150, Loss: 0.5538, Val Loss: 0.5239, Acc: 0.7747, Val Acc: 0.7926\n",
      "Epoch 160, Loss: 0.4974, Val Loss: 0.5278, Acc: 0.8060, Val Acc: 0.7983\n",
      "Epoch 170, Loss: 0.4862, Val Loss: 0.6117, Acc: 0.8124, Val Acc: 0.7670\n",
      "Epoch 180, Loss: 0.4289, Val Loss: 0.5066, Acc: 0.8408, Val Acc: 0.7926\n",
      "Epoch 190, Loss: 0.4638, Val Loss: 0.4858, Acc: 0.8152, Val Acc: 0.8068\n",
      "Epoch 200, Loss: 0.4492, Val Loss: 0.4884, Acc: 0.8209, Val Acc: 0.8068\n",
      "Epoch 210, Loss: 0.4365, Val Loss: 0.4887, Acc: 0.8294, Val Acc: 0.8068\n",
      "Epoch 220, Loss: 0.4182, Val Loss: 0.4880, Acc: 0.8316, Val Acc: 0.8097\n",
      "Epoch 230, Loss: 0.4189, Val Loss: 0.4869, Acc: 0.8330, Val Acc: 0.8068\n",
      "Epoch 240, Loss: 0.4048, Val Loss: 0.4868, Acc: 0.8465, Val Acc: 0.8068\n",
      "Epoch 250, Loss: 0.4162, Val Loss: 0.4867, Acc: 0.8344, Val Acc: 0.8068\n",
      "Epoch 260, Loss: 0.4002, Val Loss: 0.4892, Acc: 0.8465, Val Acc: 0.8068\n",
      "Epoch 270, Loss: 0.4196, Val Loss: 0.4915, Acc: 0.8408, Val Acc: 0.8040\n",
      "Epoch 280, Loss: 0.4215, Val Loss: 0.4899, Acc: 0.8358, Val Acc: 0.8068\n",
      "Epoch 290, Loss: 0.4151, Val Loss: 0.4908, Acc: 0.8429, Val Acc: 0.8011\n",
      "Best Validation Loss: 0.4847\n",
      "Fold Test Accuracy: 0.8064, F1-score: 0.8005, Homogeneity: 0.7855, NMI: 0.7929, ARI: 0.6748\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7179, Val Loss: 2.5652, Acc: 0.0760, Val Acc: 0.0712\n",
      "Epoch 10, Loss: 2.4724, Val Loss: 2.4467, Acc: 0.1577, Val Acc: 0.2251\n",
      "Epoch 20, Loss: 2.1993, Val Loss: 2.1419, Acc: 0.2301, Val Acc: 0.2621\n",
      "Epoch 30, Loss: 1.9727, Val Loss: 1.8365, Acc: 0.2678, Val Acc: 0.3333\n",
      "Epoch 40, Loss: 1.7227, Val Loss: 1.5761, Acc: 0.3438, Val Acc: 0.3504\n",
      "Epoch 50, Loss: 1.5664, Val Loss: 1.3811, Acc: 0.3778, Val Acc: 0.4929\n",
      "Epoch 60, Loss: 1.3766, Val Loss: 1.1915, Acc: 0.4425, Val Acc: 0.6211\n",
      "Epoch 70, Loss: 1.2228, Val Loss: 1.0331, Acc: 0.5057, Val Acc: 0.6581\n",
      "Epoch 80, Loss: 1.1089, Val Loss: 0.8867, Acc: 0.5497, Val Acc: 0.6809\n",
      "Epoch 90, Loss: 0.9832, Val Loss: 0.7736, Acc: 0.6122, Val Acc: 0.7151\n",
      "Epoch 100, Loss: 0.8556, Val Loss: 0.7173, Acc: 0.6442, Val Acc: 0.7236\n",
      "Epoch 110, Loss: 0.7740, Val Loss: 0.7191, Acc: 0.6811, Val Acc: 0.6980\n",
      "Epoch 120, Loss: 0.7037, Val Loss: 0.6817, Acc: 0.7294, Val Acc: 0.7179\n",
      "Epoch 130, Loss: 0.6985, Val Loss: 0.5877, Acc: 0.7152, Val Acc: 0.7863\n",
      "Epoch 140, Loss: 0.6761, Val Loss: 0.5430, Acc: 0.7209, Val Acc: 0.7835\n",
      "Epoch 150, Loss: 0.6156, Val Loss: 0.5273, Acc: 0.7337, Val Acc: 0.8006\n",
      "Epoch 160, Loss: 0.5909, Val Loss: 0.5066, Acc: 0.7592, Val Acc: 0.8148\n",
      "Epoch 170, Loss: 0.5816, Val Loss: 0.4968, Acc: 0.7692, Val Acc: 0.8006\n",
      "Epoch 180, Loss: 0.5744, Val Loss: 0.4796, Acc: 0.7713, Val Acc: 0.8177\n",
      "Epoch 190, Loss: 0.5364, Val Loss: 0.4653, Acc: 0.7820, Val Acc: 0.8177\n",
      "Epoch 200, Loss: 0.5255, Val Loss: 0.4590, Acc: 0.7933, Val Acc: 0.8177\n",
      "Epoch 210, Loss: 0.4881, Val Loss: 0.4567, Acc: 0.8111, Val Acc: 0.8205\n",
      "Epoch 220, Loss: 0.4641, Val Loss: 0.4419, Acc: 0.8111, Val Acc: 0.8262\n",
      "Epoch 230, Loss: 0.4521, Val Loss: 0.4321, Acc: 0.8224, Val Acc: 0.8291\n",
      "Epoch 240, Loss: 0.4651, Val Loss: 0.4190, Acc: 0.8033, Val Acc: 0.8205\n",
      "Epoch 250, Loss: 0.4232, Val Loss: 0.4048, Acc: 0.8253, Val Acc: 0.8319\n",
      "Epoch 260, Loss: 0.3958, Val Loss: 0.3983, Acc: 0.8487, Val Acc: 0.8376\n",
      "Epoch 270, Loss: 0.3979, Val Loss: 0.3964, Acc: 0.8565, Val Acc: 0.8490\n",
      "Epoch 280, Loss: 0.3932, Val Loss: 0.4163, Acc: 0.8594, Val Acc: 0.8519\n",
      "Epoch 290, Loss: 0.3871, Val Loss: 0.4053, Acc: 0.8544, Val Acc: 0.8575\n",
      "Best Validation Loss: 0.3842\n",
      "Fold Test Accuracy: 0.8647, F1-score: 0.8607, Homogeneity: 0.8457, NMI: 0.8496, ARI: 0.7678\n",
      "  Average Metrics for Threshold 0.11: {'test_accuracy': 0.8350132625994695, 'f1_score': 0.8319859130331559, 'homogeneity': 0.813306898961288, 'nmi': 0.8177960166511999, 'ari': 0.7189394486859011}\n",
      "\n",
      "Training with threshold: 0.12000000000000001\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6803, Val Loss: 2.5608, Acc: 0.0824, Val Acc: 0.0739\n",
      "Epoch 10, Loss: 2.4905, Val Loss: 2.4435, Acc: 0.1507, Val Acc: 0.2358\n",
      "Epoch 20, Loss: 2.1983, Val Loss: 2.1601, Acc: 0.2225, Val Acc: 0.3153\n",
      "Epoch 30, Loss: 1.9460, Val Loss: 1.8691, Acc: 0.2985, Val Acc: 0.3665\n",
      "Epoch 40, Loss: 1.7074, Val Loss: 1.6063, Acc: 0.3582, Val Acc: 0.4119\n",
      "Epoch 50, Loss: 1.5125, Val Loss: 1.3545, Acc: 0.4122, Val Acc: 0.4886\n",
      "Epoch 60, Loss: 1.3204, Val Loss: 1.1696, Acc: 0.4783, Val Acc: 0.5938\n",
      "Epoch 70, Loss: 1.1818, Val Loss: 1.0305, Acc: 0.5366, Val Acc: 0.6307\n",
      "Epoch 80, Loss: 1.0539, Val Loss: 0.8944, Acc: 0.5828, Val Acc: 0.6761\n",
      "Epoch 90, Loss: 0.9202, Val Loss: 0.7836, Acc: 0.6148, Val Acc: 0.7017\n",
      "Epoch 100, Loss: 0.8324, Val Loss: 0.7199, Acc: 0.6688, Val Acc: 0.7216\n",
      "Epoch 110, Loss: 0.7572, Val Loss: 0.7101, Acc: 0.6923, Val Acc: 0.6875\n",
      "Epoch 120, Loss: 0.6993, Val Loss: 0.6898, Acc: 0.7186, Val Acc: 0.6960\n",
      "Epoch 130, Loss: 0.6678, Val Loss: 0.6537, Acc: 0.7385, Val Acc: 0.7188\n",
      "Epoch 140, Loss: 0.6324, Val Loss: 0.6231, Acc: 0.7356, Val Acc: 0.7330\n",
      "Epoch 150, Loss: 0.6000, Val Loss: 0.6155, Acc: 0.7470, Val Acc: 0.7358\n",
      "Epoch 160, Loss: 0.5689, Val Loss: 0.5909, Acc: 0.7811, Val Acc: 0.7443\n",
      "Epoch 170, Loss: 0.5459, Val Loss: 0.5677, Acc: 0.7754, Val Acc: 0.7585\n",
      "Epoch 180, Loss: 0.5416, Val Loss: 0.5209, Acc: 0.7960, Val Acc: 0.7784\n",
      "Epoch 190, Loss: 0.5145, Val Loss: 0.5121, Acc: 0.8017, Val Acc: 0.7841\n",
      "Epoch 200, Loss: 0.4815, Val Loss: 0.4987, Acc: 0.8102, Val Acc: 0.7898\n",
      "Epoch 210, Loss: 0.4736, Val Loss: 0.5140, Acc: 0.8266, Val Acc: 0.7756\n",
      "Epoch 220, Loss: 0.4383, Val Loss: 0.5073, Acc: 0.8372, Val Acc: 0.7812\n",
      "Epoch 230, Loss: 0.4362, Val Loss: 0.4968, Acc: 0.8408, Val Acc: 0.7926\n",
      "Epoch 240, Loss: 0.4402, Val Loss: 0.4972, Acc: 0.8372, Val Acc: 0.7898\n",
      "Epoch 250, Loss: 0.4371, Val Loss: 0.4956, Acc: 0.8408, Val Acc: 0.7926\n",
      "Epoch 260, Loss: 0.4437, Val Loss: 0.4927, Acc: 0.8323, Val Acc: 0.7955\n",
      "Epoch 270, Loss: 0.4272, Val Loss: 0.4948, Acc: 0.8372, Val Acc: 0.7898\n",
      "Epoch 280, Loss: 0.4136, Val Loss: 0.4929, Acc: 0.8422, Val Acc: 0.7898\n",
      "Epoch 290, Loss: 0.4294, Val Loss: 0.4950, Acc: 0.8451, Val Acc: 0.7955\n",
      "Best Validation Loss: 0.4925\n",
      "Fold Test Accuracy: 0.8223, F1-score: 0.8171, Homogeneity: 0.8029, NMI: 0.8104, ARI: 0.7023\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7122, Val Loss: 2.5623, Acc: 0.0768, Val Acc: 0.0767\n",
      "Epoch 10, Loss: 2.4785, Val Loss: 2.4500, Acc: 0.1464, Val Acc: 0.2784\n",
      "Epoch 20, Loss: 2.2208, Val Loss: 2.1511, Acc: 0.2360, Val Acc: 0.2841\n",
      "Epoch 30, Loss: 1.9700, Val Loss: 1.8568, Acc: 0.3049, Val Acc: 0.3551\n",
      "Epoch 40, Loss: 1.7463, Val Loss: 1.5757, Acc: 0.3312, Val Acc: 0.4688\n",
      "Epoch 50, Loss: 1.5321, Val Loss: 1.3368, Acc: 0.4165, Val Acc: 0.5483\n",
      "Epoch 60, Loss: 1.3206, Val Loss: 1.1572, Acc: 0.4648, Val Acc: 0.6108\n",
      "Epoch 70, Loss: 1.1785, Val Loss: 0.9948, Acc: 0.5295, Val Acc: 0.6648\n",
      "Epoch 80, Loss: 1.0323, Val Loss: 0.8641, Acc: 0.5864, Val Acc: 0.7131\n",
      "Epoch 90, Loss: 0.9201, Val Loss: 0.7718, Acc: 0.6183, Val Acc: 0.7472\n",
      "Epoch 100, Loss: 0.8286, Val Loss: 0.7165, Acc: 0.6553, Val Acc: 0.7472\n",
      "Epoch 110, Loss: 0.7695, Val Loss: 0.6585, Acc: 0.6937, Val Acc: 0.7528\n",
      "Epoch 120, Loss: 0.6781, Val Loss: 0.6082, Acc: 0.7321, Val Acc: 0.7670\n",
      "Epoch 130, Loss: 0.6277, Val Loss: 0.5505, Acc: 0.7399, Val Acc: 0.7841\n",
      "Epoch 140, Loss: 0.6073, Val Loss: 0.5115, Acc: 0.7584, Val Acc: 0.8068\n",
      "Epoch 150, Loss: 0.5490, Val Loss: 0.4882, Acc: 0.7939, Val Acc: 0.8068\n",
      "Epoch 160, Loss: 0.4962, Val Loss: 0.4348, Acc: 0.8102, Val Acc: 0.8267\n",
      "Epoch 170, Loss: 0.4948, Val Loss: 0.4084, Acc: 0.8024, Val Acc: 0.8352\n",
      "Epoch 180, Loss: 0.4279, Val Loss: 0.3916, Acc: 0.8308, Val Acc: 0.8494\n",
      "Epoch 190, Loss: 0.3804, Val Loss: 0.3898, Acc: 0.8586, Val Acc: 0.8551\n",
      "Epoch 200, Loss: 0.3617, Val Loss: 0.3763, Acc: 0.8635, Val Acc: 0.8523\n",
      "Epoch 210, Loss: 0.3544, Val Loss: 0.5398, Acc: 0.8806, Val Acc: 0.8068\n",
      "Epoch 220, Loss: 0.3239, Val Loss: 0.4318, Acc: 0.8678, Val Acc: 0.8551\n",
      "Epoch 230, Loss: 0.3246, Val Loss: 0.3311, Acc: 0.8806, Val Acc: 0.8778\n",
      "Epoch 240, Loss: 0.2608, Val Loss: 0.3352, Acc: 0.9033, Val Acc: 0.8750\n",
      "Epoch 250, Loss: 0.2696, Val Loss: 0.3491, Acc: 0.9090, Val Acc: 0.8722\n",
      "Epoch 260, Loss: 0.2826, Val Loss: 0.3539, Acc: 0.8998, Val Acc: 0.8693\n",
      "Epoch 270, Loss: 0.2964, Val Loss: 0.3485, Acc: 0.8898, Val Acc: 0.8722\n",
      "Epoch 280, Loss: 0.2741, Val Loss: 0.3493, Acc: 0.8977, Val Acc: 0.8750\n",
      "Epoch 290, Loss: 0.2926, Val Loss: 0.3457, Acc: 0.8927, Val Acc: 0.8778\n",
      "Best Validation Loss: 0.3281\n",
      "Fold Test Accuracy: 0.8422, F1-score: 0.8422, Homogeneity: 0.8214, NMI: 0.8228, ARI: 0.7297\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7117, Val Loss: 2.5654, Acc: 0.0697, Val Acc: 0.0852\n",
      "Epoch 10, Loss: 2.4627, Val Loss: 2.4260, Acc: 0.1606, Val Acc: 0.2699\n",
      "Epoch 20, Loss: 2.1835, Val Loss: 2.1203, Acc: 0.2438, Val Acc: 0.2983\n",
      "Epoch 30, Loss: 1.9452, Val Loss: 1.7970, Acc: 0.2942, Val Acc: 0.3665\n",
      "Epoch 40, Loss: 1.6938, Val Loss: 1.5189, Acc: 0.3412, Val Acc: 0.4801\n",
      "Epoch 50, Loss: 1.4994, Val Loss: 1.3025, Acc: 0.4037, Val Acc: 0.5653\n",
      "Epoch 60, Loss: 1.3191, Val Loss: 1.1226, Acc: 0.4769, Val Acc: 0.6449\n",
      "Epoch 70, Loss: 1.1561, Val Loss: 0.9691, Acc: 0.5522, Val Acc: 0.6733\n",
      "Epoch 80, Loss: 1.0152, Val Loss: 0.8374, Acc: 0.6006, Val Acc: 0.6875\n",
      "Epoch 90, Loss: 0.8880, Val Loss: 0.7428, Acc: 0.6368, Val Acc: 0.7188\n",
      "Epoch 100, Loss: 0.8450, Val Loss: 0.6781, Acc: 0.6496, Val Acc: 0.7415\n",
      "Epoch 110, Loss: 0.7471, Val Loss: 0.7087, Acc: 0.6958, Val Acc: 0.7102\n",
      "Epoch 120, Loss: 0.6832, Val Loss: 0.6003, Acc: 0.7136, Val Acc: 0.7557\n",
      "Epoch 130, Loss: 0.6591, Val Loss: 0.5870, Acc: 0.7335, Val Acc: 0.7557\n",
      "Epoch 140, Loss: 0.6381, Val Loss: 0.5649, Acc: 0.7576, Val Acc: 0.7670\n",
      "Epoch 150, Loss: 0.6215, Val Loss: 0.5466, Acc: 0.7484, Val Acc: 0.7756\n",
      "Epoch 160, Loss: 0.5782, Val Loss: 0.5384, Acc: 0.7690, Val Acc: 0.7670\n",
      "Epoch 170, Loss: 0.5598, Val Loss: 0.5201, Acc: 0.7804, Val Acc: 0.7727\n",
      "Epoch 180, Loss: 0.5295, Val Loss: 0.5054, Acc: 0.8024, Val Acc: 0.7756\n",
      "Epoch 190, Loss: 0.5175, Val Loss: 0.4987, Acc: 0.7953, Val Acc: 0.7841\n",
      "Epoch 200, Loss: 0.4791, Val Loss: 0.4876, Acc: 0.8181, Val Acc: 0.7869\n",
      "Epoch 210, Loss: 0.4879, Val Loss: 0.4811, Acc: 0.8095, Val Acc: 0.7841\n",
      "Epoch 220, Loss: 0.4609, Val Loss: 0.4683, Acc: 0.8202, Val Acc: 0.7869\n",
      "Epoch 230, Loss: 0.4422, Val Loss: 0.4672, Acc: 0.8415, Val Acc: 0.7955\n",
      "Epoch 240, Loss: 0.4118, Val Loss: 0.4824, Acc: 0.8465, Val Acc: 0.7869\n",
      "Epoch 250, Loss: 0.4199, Val Loss: 0.4647, Acc: 0.8344, Val Acc: 0.7955\n",
      "Epoch 260, Loss: 0.3958, Val Loss: 0.4521, Acc: 0.8451, Val Acc: 0.7926\n",
      "Epoch 270, Loss: 0.3835, Val Loss: 0.4408, Acc: 0.8550, Val Acc: 0.8011\n",
      "Epoch 280, Loss: 0.3668, Val Loss: 0.4420, Acc: 0.8657, Val Acc: 0.8040\n",
      "Epoch 290, Loss: 0.3681, Val Loss: 0.4427, Acc: 0.8607, Val Acc: 0.8125\n",
      "Best Validation Loss: 0.4394\n",
      "Fold Test Accuracy: 0.8249, F1-score: 0.8220, Homogeneity: 0.8169, NMI: 0.8210, ARI: 0.7152\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6928, Val Loss: 2.5592, Acc: 0.0817, Val Acc: 0.1136\n",
      "Epoch 10, Loss: 2.4548, Val Loss: 2.4270, Acc: 0.1628, Val Acc: 0.2983\n",
      "Epoch 20, Loss: 2.2017, Val Loss: 2.1153, Acc: 0.2246, Val Acc: 0.3409\n",
      "Epoch 30, Loss: 1.9578, Val Loss: 1.8276, Acc: 0.3021, Val Acc: 0.3920\n",
      "Epoch 40, Loss: 1.7304, Val Loss: 1.5730, Acc: 0.3497, Val Acc: 0.4318\n",
      "Epoch 50, Loss: 1.5462, Val Loss: 1.3467, Acc: 0.4037, Val Acc: 0.5369\n",
      "Epoch 60, Loss: 1.3427, Val Loss: 1.1659, Acc: 0.4741, Val Acc: 0.5994\n",
      "Epoch 70, Loss: 1.2071, Val Loss: 1.0256, Acc: 0.5046, Val Acc: 0.6477\n",
      "Epoch 80, Loss: 1.0680, Val Loss: 0.8947, Acc: 0.5721, Val Acc: 0.6818\n",
      "Epoch 90, Loss: 0.9603, Val Loss: 0.8499, Acc: 0.6205, Val Acc: 0.6534\n",
      "Epoch 100, Loss: 0.8161, Val Loss: 0.7094, Acc: 0.6660, Val Acc: 0.7131\n",
      "Epoch 110, Loss: 0.7601, Val Loss: 0.6945, Acc: 0.6937, Val Acc: 0.7017\n",
      "Epoch 120, Loss: 0.6775, Val Loss: 0.6843, Acc: 0.7249, Val Acc: 0.6989\n",
      "Epoch 130, Loss: 0.6500, Val Loss: 0.6684, Acc: 0.7313, Val Acc: 0.7045\n",
      "Epoch 140, Loss: 0.5811, Val Loss: 0.6069, Acc: 0.7683, Val Acc: 0.7301\n",
      "Epoch 150, Loss: 0.5345, Val Loss: 0.6159, Acc: 0.7825, Val Acc: 0.7415\n",
      "Epoch 160, Loss: 0.5336, Val Loss: 0.5501, Acc: 0.7925, Val Acc: 0.7670\n",
      "Epoch 170, Loss: 0.5012, Val Loss: 0.5244, Acc: 0.8074, Val Acc: 0.7869\n",
      "Epoch 180, Loss: 0.4477, Val Loss: 0.5291, Acc: 0.8380, Val Acc: 0.7869\n",
      "Epoch 190, Loss: 0.4557, Val Loss: 0.5066, Acc: 0.8252, Val Acc: 0.8011\n",
      "Epoch 200, Loss: 0.4503, Val Loss: 0.4918, Acc: 0.8266, Val Acc: 0.8097\n",
      "Epoch 210, Loss: 0.4629, Val Loss: 0.4881, Acc: 0.8181, Val Acc: 0.8097\n",
      "Epoch 220, Loss: 0.4590, Val Loss: 0.4767, Acc: 0.8173, Val Acc: 0.8182\n",
      "Epoch 230, Loss: 0.4359, Val Loss: 0.4772, Acc: 0.8273, Val Acc: 0.8153\n",
      "Epoch 240, Loss: 0.4247, Val Loss: 0.4885, Acc: 0.8365, Val Acc: 0.8097\n",
      "Epoch 250, Loss: 0.4163, Val Loss: 0.4857, Acc: 0.8380, Val Acc: 0.8125\n",
      "Epoch 260, Loss: 0.4307, Val Loss: 0.4787, Acc: 0.8365, Val Acc: 0.8153\n",
      "Epoch 270, Loss: 0.4199, Val Loss: 0.4805, Acc: 0.8365, Val Acc: 0.8125\n",
      "Epoch 280, Loss: 0.4131, Val Loss: 0.4801, Acc: 0.8500, Val Acc: 0.8125\n",
      "Epoch 290, Loss: 0.4179, Val Loss: 0.4823, Acc: 0.8486, Val Acc: 0.8153\n",
      "Best Validation Loss: 0.4740\n",
      "Fold Test Accuracy: 0.8143, F1-score: 0.8079, Homogeneity: 0.7941, NMI: 0.8012, ARI: 0.6912\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7131, Val Loss: 2.5625, Acc: 0.0724, Val Acc: 0.0855\n",
      "Epoch 10, Loss: 2.4351, Val Loss: 2.4165, Acc: 0.1612, Val Acc: 0.2251\n",
      "Epoch 20, Loss: 2.1660, Val Loss: 2.1155, Acc: 0.2443, Val Acc: 0.3276\n",
      "Epoch 30, Loss: 1.9484, Val Loss: 1.8028, Acc: 0.2855, Val Acc: 0.3846\n",
      "Epoch 40, Loss: 1.7114, Val Loss: 1.5446, Acc: 0.3615, Val Acc: 0.4872\n",
      "Epoch 50, Loss: 1.5031, Val Loss: 1.3330, Acc: 0.4134, Val Acc: 0.5356\n",
      "Epoch 60, Loss: 1.3531, Val Loss: 1.1537, Acc: 0.4695, Val Acc: 0.6239\n",
      "Epoch 70, Loss: 1.1707, Val Loss: 0.9799, Acc: 0.5362, Val Acc: 0.6838\n",
      "Epoch 80, Loss: 1.0628, Val Loss: 0.8370, Acc: 0.5795, Val Acc: 0.7009\n",
      "Epoch 90, Loss: 0.9016, Val Loss: 0.7209, Acc: 0.6300, Val Acc: 0.7578\n",
      "Epoch 100, Loss: 0.8560, Val Loss: 0.7069, Acc: 0.6548, Val Acc: 0.7151\n",
      "Epoch 110, Loss: 0.7737, Val Loss: 0.6316, Acc: 0.6960, Val Acc: 0.7379\n",
      "Epoch 120, Loss: 0.6671, Val Loss: 0.5506, Acc: 0.7351, Val Acc: 0.8006\n",
      "Epoch 130, Loss: 0.6154, Val Loss: 0.5523, Acc: 0.7585, Val Acc: 0.7778\n",
      "Epoch 140, Loss: 0.5730, Val Loss: 0.5136, Acc: 0.7706, Val Acc: 0.8034\n",
      "Epoch 150, Loss: 0.5548, Val Loss: 0.4694, Acc: 0.7820, Val Acc: 0.8262\n",
      "Epoch 160, Loss: 0.5447, Val Loss: 0.4678, Acc: 0.7713, Val Acc: 0.8262\n",
      "Epoch 170, Loss: 0.4990, Val Loss: 0.4475, Acc: 0.7947, Val Acc: 0.8405\n",
      "Epoch 180, Loss: 0.4907, Val Loss: 0.4344, Acc: 0.8075, Val Acc: 0.8405\n",
      "Epoch 190, Loss: 0.4780, Val Loss: 0.4210, Acc: 0.8246, Val Acc: 0.8433\n",
      "Epoch 200, Loss: 0.4447, Val Loss: 0.3982, Acc: 0.8345, Val Acc: 0.8519\n",
      "Epoch 210, Loss: 0.4328, Val Loss: 0.4186, Acc: 0.8423, Val Acc: 0.8319\n",
      "Epoch 220, Loss: 0.4174, Val Loss: 0.3840, Acc: 0.8359, Val Acc: 0.8632\n",
      "Epoch 230, Loss: 0.4341, Val Loss: 0.3853, Acc: 0.8359, Val Acc: 0.8547\n",
      "Epoch 240, Loss: 0.3906, Val Loss: 0.3820, Acc: 0.8537, Val Acc: 0.8661\n",
      "Epoch 250, Loss: 0.4001, Val Loss: 0.3811, Acc: 0.8402, Val Acc: 0.8632\n",
      "Epoch 260, Loss: 0.3873, Val Loss: 0.3805, Acc: 0.8565, Val Acc: 0.8632\n",
      "Epoch 270, Loss: 0.3772, Val Loss: 0.3807, Acc: 0.8580, Val Acc: 0.8632\n",
      "Epoch 280, Loss: 0.3885, Val Loss: 0.3802, Acc: 0.8459, Val Acc: 0.8575\n",
      "Epoch 290, Loss: 0.3897, Val Loss: 0.3783, Acc: 0.8551, Val Acc: 0.8632\n",
      "Best Validation Loss: 0.3776\n",
      "Fold Test Accuracy: 0.8475, F1-score: 0.8449, Homogeneity: 0.8346, NMI: 0.8376, ARI: 0.7447\n",
      "  Average Metrics for Threshold 0.12: {'test_accuracy': 0.8302387267904511, 'f1_score': 0.8268156844863487, 'homogeneity': 0.8139876810125685, 'nmi': 0.8186019029155048, 'ari': 0.7166031583204625}\n",
      "\n",
      "Training with threshold: 0.13\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6925, Val Loss: 2.5640, Acc: 0.0704, Val Acc: 0.0597\n",
      "Epoch 10, Loss: 2.4355, Val Loss: 2.4054, Acc: 0.1663, Val Acc: 0.2358\n",
      "Epoch 20, Loss: 2.1781, Val Loss: 2.1308, Acc: 0.2303, Val Acc: 0.2614\n",
      "Epoch 30, Loss: 1.9483, Val Loss: 1.8684, Acc: 0.2871, Val Acc: 0.3182\n",
      "Epoch 40, Loss: 1.7182, Val Loss: 1.6046, Acc: 0.3298, Val Acc: 0.3949\n",
      "Epoch 50, Loss: 1.5188, Val Loss: 1.3676, Acc: 0.4165, Val Acc: 0.5312\n",
      "Epoch 60, Loss: 1.3380, Val Loss: 1.1750, Acc: 0.4598, Val Acc: 0.6307\n",
      "Epoch 70, Loss: 1.1624, Val Loss: 1.0195, Acc: 0.5501, Val Acc: 0.6676\n",
      "Epoch 80, Loss: 1.0283, Val Loss: 0.8708, Acc: 0.6006, Val Acc: 0.6903\n",
      "Epoch 90, Loss: 0.9073, Val Loss: 0.7625, Acc: 0.6510, Val Acc: 0.7216\n",
      "Epoch 100, Loss: 0.7958, Val Loss: 0.7582, Acc: 0.6695, Val Acc: 0.6875\n",
      "Epoch 110, Loss: 0.7563, Val Loss: 0.7024, Acc: 0.7058, Val Acc: 0.7216\n",
      "Epoch 120, Loss: 0.6888, Val Loss: 0.6358, Acc: 0.7249, Val Acc: 0.7585\n",
      "Epoch 130, Loss: 0.6755, Val Loss: 0.6096, Acc: 0.7285, Val Acc: 0.7614\n",
      "Epoch 140, Loss: 0.6613, Val Loss: 0.5982, Acc: 0.7406, Val Acc: 0.7557\n",
      "Epoch 150, Loss: 0.6269, Val Loss: 0.5799, Acc: 0.7434, Val Acc: 0.7699\n",
      "Epoch 160, Loss: 0.5961, Val Loss: 0.5524, Acc: 0.7811, Val Acc: 0.7841\n",
      "Epoch 170, Loss: 0.5670, Val Loss: 0.5464, Acc: 0.7882, Val Acc: 0.7699\n",
      "Epoch 180, Loss: 0.5408, Val Loss: 0.5380, Acc: 0.7868, Val Acc: 0.7898\n",
      "Epoch 190, Loss: 0.5377, Val Loss: 0.5212, Acc: 0.7818, Val Acc: 0.7955\n",
      "Epoch 200, Loss: 0.4980, Val Loss: 0.5055, Acc: 0.8081, Val Acc: 0.8011\n",
      "Epoch 210, Loss: 0.4699, Val Loss: 0.5010, Acc: 0.8273, Val Acc: 0.7926\n",
      "Epoch 220, Loss: 0.4486, Val Loss: 0.5026, Acc: 0.8287, Val Acc: 0.7898\n",
      "Epoch 230, Loss: 0.4450, Val Loss: 0.4826, Acc: 0.8323, Val Acc: 0.8097\n",
      "Epoch 240, Loss: 0.4422, Val Loss: 0.4703, Acc: 0.8344, Val Acc: 0.8040\n",
      "Epoch 250, Loss: 0.4415, Val Loss: 0.4725, Acc: 0.8316, Val Acc: 0.8040\n",
      "Epoch 260, Loss: 0.4149, Val Loss: 0.4708, Acc: 0.8365, Val Acc: 0.8040\n",
      "Epoch 270, Loss: 0.4009, Val Loss: 0.4727, Acc: 0.8408, Val Acc: 0.8068\n",
      "Epoch 280, Loss: 0.4164, Val Loss: 0.4730, Acc: 0.8387, Val Acc: 0.8068\n",
      "Epoch 290, Loss: 0.4268, Val Loss: 0.4736, Acc: 0.8323, Val Acc: 0.8068\n",
      "Best Validation Loss: 0.4703\n",
      "Fold Test Accuracy: 0.8342, F1-score: 0.8288, Homogeneity: 0.8128, NMI: 0.8190, ARI: 0.7194\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7007, Val Loss: 2.5625, Acc: 0.0810, Val Acc: 0.0938\n",
      "Epoch 10, Loss: 2.4791, Val Loss: 2.4536, Acc: 0.1571, Val Acc: 0.3295\n",
      "Epoch 20, Loss: 2.2013, Val Loss: 2.1166, Acc: 0.2310, Val Acc: 0.3210\n",
      "Epoch 30, Loss: 1.9536, Val Loss: 1.8208, Acc: 0.2921, Val Acc: 0.3352\n",
      "Epoch 40, Loss: 1.7284, Val Loss: 1.5551, Acc: 0.3419, Val Acc: 0.4631\n",
      "Epoch 50, Loss: 1.5292, Val Loss: 1.3215, Acc: 0.4065, Val Acc: 0.6023\n",
      "Epoch 60, Loss: 1.3304, Val Loss: 1.1229, Acc: 0.4705, Val Acc: 0.6562\n",
      "Epoch 70, Loss: 1.1759, Val Loss: 0.9690, Acc: 0.5345, Val Acc: 0.7159\n",
      "Epoch 80, Loss: 1.0367, Val Loss: 0.8434, Acc: 0.5928, Val Acc: 0.7216\n",
      "Epoch 90, Loss: 0.9230, Val Loss: 0.7825, Acc: 0.6304, Val Acc: 0.7244\n",
      "Epoch 100, Loss: 0.8340, Val Loss: 0.6965, Acc: 0.6681, Val Acc: 0.7614\n",
      "Epoch 110, Loss: 0.7304, Val Loss: 0.7254, Acc: 0.7058, Val Acc: 0.7472\n",
      "Epoch 120, Loss: 0.7246, Val Loss: 0.6179, Acc: 0.7107, Val Acc: 0.7983\n",
      "Epoch 130, Loss: 0.6817, Val Loss: 0.6060, Acc: 0.7122, Val Acc: 0.7670\n",
      "Epoch 140, Loss: 0.6028, Val Loss: 0.5808, Acc: 0.7676, Val Acc: 0.7670\n",
      "Epoch 150, Loss: 0.6360, Val Loss: 0.5557, Acc: 0.7406, Val Acc: 0.7926\n",
      "Epoch 160, Loss: 0.6312, Val Loss: 0.5519, Acc: 0.7392, Val Acc: 0.8040\n",
      "Epoch 170, Loss: 0.6042, Val Loss: 0.5484, Acc: 0.7598, Val Acc: 0.7983\n",
      "Epoch 180, Loss: 0.5877, Val Loss: 0.5360, Acc: 0.7704, Val Acc: 0.8097\n",
      "Epoch 190, Loss: 0.5891, Val Loss: 0.5334, Acc: 0.7619, Val Acc: 0.8011\n",
      "Epoch 200, Loss: 0.5724, Val Loss: 0.5328, Acc: 0.7783, Val Acc: 0.8040\n",
      "Epoch 210, Loss: 0.5542, Val Loss: 0.5245, Acc: 0.7889, Val Acc: 0.8068\n",
      "Epoch 220, Loss: 0.5522, Val Loss: 0.5138, Acc: 0.7875, Val Acc: 0.8011\n",
      "Epoch 230, Loss: 0.5483, Val Loss: 0.5055, Acc: 0.7790, Val Acc: 0.7983\n",
      "Epoch 240, Loss: 0.5295, Val Loss: 0.5026, Acc: 0.7861, Val Acc: 0.8011\n",
      "Epoch 250, Loss: 0.4972, Val Loss: 0.4917, Acc: 0.8053, Val Acc: 0.8125\n",
      "Epoch 260, Loss: 0.5062, Val Loss: 0.4831, Acc: 0.7875, Val Acc: 0.8153\n",
      "Epoch 270, Loss: 0.4925, Val Loss: 0.4798, Acc: 0.8081, Val Acc: 0.8153\n",
      "Epoch 280, Loss: 0.4595, Val Loss: 0.4751, Acc: 0.8166, Val Acc: 0.8182\n",
      "Epoch 290, Loss: 0.4746, Val Loss: 0.4675, Acc: 0.8124, Val Acc: 0.8239\n",
      "Best Validation Loss: 0.4618\n",
      "Fold Test Accuracy: 0.8156, F1-score: 0.8118, Homogeneity: 0.8098, NMI: 0.8144, ARI: 0.7029\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7152, Val Loss: 2.5607, Acc: 0.0661, Val Acc: 0.1278\n",
      "Epoch 10, Loss: 2.4558, Val Loss: 2.4333, Acc: 0.1571, Val Acc: 0.2898\n",
      "Epoch 20, Loss: 2.2008, Val Loss: 2.1353, Acc: 0.2310, Val Acc: 0.3239\n",
      "Epoch 30, Loss: 1.9486, Val Loss: 1.8311, Acc: 0.3149, Val Acc: 0.3892\n",
      "Epoch 40, Loss: 1.7385, Val Loss: 1.5754, Acc: 0.3390, Val Acc: 0.4545\n",
      "Epoch 50, Loss: 1.5274, Val Loss: 1.3487, Acc: 0.4094, Val Acc: 0.5369\n",
      "Epoch 60, Loss: 1.3592, Val Loss: 1.1504, Acc: 0.4627, Val Acc: 0.6165\n",
      "Epoch 70, Loss: 1.1778, Val Loss: 0.9822, Acc: 0.5458, Val Acc: 0.6449\n",
      "Epoch 80, Loss: 1.0413, Val Loss: 0.8489, Acc: 0.5913, Val Acc: 0.6818\n",
      "Epoch 90, Loss: 0.9140, Val Loss: 0.7629, Acc: 0.6205, Val Acc: 0.6903\n",
      "Epoch 100, Loss: 0.8213, Val Loss: 0.6993, Acc: 0.6795, Val Acc: 0.7045\n",
      "Epoch 110, Loss: 0.7509, Val Loss: 0.6582, Acc: 0.7086, Val Acc: 0.7188\n",
      "Epoch 120, Loss: 0.6831, Val Loss: 0.6092, Acc: 0.7200, Val Acc: 0.7528\n",
      "Epoch 130, Loss: 0.6345, Val Loss: 0.5828, Acc: 0.7576, Val Acc: 0.7358\n",
      "Epoch 140, Loss: 0.5876, Val Loss: 0.5142, Acc: 0.7839, Val Acc: 0.7812\n",
      "Epoch 150, Loss: 0.5365, Val Loss: 0.5014, Acc: 0.7960, Val Acc: 0.7812\n",
      "Epoch 160, Loss: 0.4939, Val Loss: 0.4881, Acc: 0.8117, Val Acc: 0.7983\n",
      "Epoch 170, Loss: 0.4411, Val Loss: 0.4619, Acc: 0.8316, Val Acc: 0.8068\n",
      "Epoch 180, Loss: 0.4505, Val Loss: 0.4710, Acc: 0.8223, Val Acc: 0.8097\n",
      "Epoch 190, Loss: 0.3951, Val Loss: 0.4511, Acc: 0.8507, Val Acc: 0.8153\n",
      "Epoch 200, Loss: 0.3908, Val Loss: 0.4129, Acc: 0.8579, Val Acc: 0.8239\n",
      "Epoch 210, Loss: 0.3727, Val Loss: 0.4118, Acc: 0.8600, Val Acc: 0.8239\n",
      "Epoch 220, Loss: 0.3474, Val Loss: 0.4088, Acc: 0.8657, Val Acc: 0.8324\n",
      "Epoch 230, Loss: 0.3624, Val Loss: 0.4112, Acc: 0.8614, Val Acc: 0.8239\n",
      "Epoch 240, Loss: 0.3548, Val Loss: 0.4074, Acc: 0.8742, Val Acc: 0.8267\n",
      "Epoch 250, Loss: 0.3322, Val Loss: 0.4100, Acc: 0.8742, Val Acc: 0.8182\n",
      "Epoch 260, Loss: 0.3308, Val Loss: 0.4131, Acc: 0.8849, Val Acc: 0.8153\n",
      "Epoch 270, Loss: 0.3282, Val Loss: 0.4132, Acc: 0.8799, Val Acc: 0.8182\n",
      "Epoch 280, Loss: 0.3228, Val Loss: 0.4098, Acc: 0.8863, Val Acc: 0.8182\n",
      "Epoch 290, Loss: 0.3170, Val Loss: 0.4109, Acc: 0.8763, Val Acc: 0.8182\n",
      "Best Validation Loss: 0.4067\n",
      "Fold Test Accuracy: 0.8342, F1-score: 0.8342, Homogeneity: 0.8172, NMI: 0.8189, ARI: 0.7213\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7100, Val Loss: 2.5606, Acc: 0.0675, Val Acc: 0.0739\n",
      "Epoch 10, Loss: 2.4571, Val Loss: 2.4303, Acc: 0.1684, Val Acc: 0.2642\n",
      "Epoch 20, Loss: 2.1734, Val Loss: 2.0970, Acc: 0.2402, Val Acc: 0.2812\n",
      "Epoch 30, Loss: 1.9160, Val Loss: 1.7622, Acc: 0.2786, Val Acc: 0.3324\n",
      "Epoch 40, Loss: 1.6624, Val Loss: 1.4934, Acc: 0.3703, Val Acc: 0.4403\n",
      "Epoch 50, Loss: 1.4804, Val Loss: 1.2687, Acc: 0.4030, Val Acc: 0.5938\n",
      "Epoch 60, Loss: 1.2817, Val Loss: 1.0962, Acc: 0.4897, Val Acc: 0.6392\n",
      "Epoch 70, Loss: 1.1349, Val Loss: 0.9455, Acc: 0.5423, Val Acc: 0.6960\n",
      "Epoch 80, Loss: 0.9876, Val Loss: 0.8184, Acc: 0.5999, Val Acc: 0.7074\n",
      "Epoch 90, Loss: 0.8936, Val Loss: 0.7430, Acc: 0.6382, Val Acc: 0.7159\n",
      "Epoch 100, Loss: 0.7775, Val Loss: 0.7409, Acc: 0.6915, Val Acc: 0.6932\n",
      "Epoch 110, Loss: 0.7147, Val Loss: 0.6146, Acc: 0.7050, Val Acc: 0.7642\n",
      "Epoch 120, Loss: 0.6730, Val Loss: 0.5762, Acc: 0.7313, Val Acc: 0.7727\n",
      "Epoch 130, Loss: 0.6048, Val Loss: 0.6016, Acc: 0.7640, Val Acc: 0.7500\n",
      "Epoch 140, Loss: 0.5555, Val Loss: 0.5365, Acc: 0.7783, Val Acc: 0.7869\n",
      "Epoch 150, Loss: 0.5528, Val Loss: 0.5043, Acc: 0.7818, Val Acc: 0.8011\n",
      "Epoch 160, Loss: 0.4985, Val Loss: 0.4948, Acc: 0.8024, Val Acc: 0.8068\n",
      "Epoch 170, Loss: 0.4938, Val Loss: 0.5017, Acc: 0.8138, Val Acc: 0.8068\n",
      "Epoch 180, Loss: 0.4679, Val Loss: 0.4920, Acc: 0.8252, Val Acc: 0.8011\n",
      "Epoch 190, Loss: 0.4755, Val Loss: 0.4645, Acc: 0.8223, Val Acc: 0.8267\n",
      "Epoch 200, Loss: 0.4570, Val Loss: 0.4577, Acc: 0.8223, Val Acc: 0.8239\n",
      "Epoch 210, Loss: 0.4528, Val Loss: 0.4652, Acc: 0.8244, Val Acc: 0.8239\n",
      "Epoch 220, Loss: 0.4519, Val Loss: 0.4605, Acc: 0.8358, Val Acc: 0.8239\n",
      "Epoch 230, Loss: 0.4670, Val Loss: 0.4636, Acc: 0.8230, Val Acc: 0.8210\n",
      "Epoch 240, Loss: 0.4504, Val Loss: 0.4660, Acc: 0.8216, Val Acc: 0.8210\n",
      "Epoch 250, Loss: 0.4662, Val Loss: 0.4659, Acc: 0.8287, Val Acc: 0.8068\n",
      "Epoch 260, Loss: 0.4478, Val Loss: 0.4672, Acc: 0.8237, Val Acc: 0.8040\n",
      "Epoch 270, Loss: 0.4463, Val Loss: 0.4665, Acc: 0.8344, Val Acc: 0.8040\n",
      "Epoch 280, Loss: 0.4265, Val Loss: 0.4666, Acc: 0.8415, Val Acc: 0.8040\n",
      "Epoch 290, Loss: 0.4418, Val Loss: 0.4713, Acc: 0.8273, Val Acc: 0.8011\n",
      "Best Validation Loss: 0.4536\n",
      "Fold Test Accuracy: 0.8143, F1-score: 0.8101, Homogeneity: 0.7924, NMI: 0.7984, ARI: 0.6875\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7083, Val Loss: 2.5633, Acc: 0.0781, Val Acc: 0.0798\n",
      "Epoch 10, Loss: 2.4282, Val Loss: 2.3915, Acc: 0.1648, Val Acc: 0.2507\n",
      "Epoch 20, Loss: 2.1775, Val Loss: 2.1111, Acc: 0.2344, Val Acc: 0.2991\n",
      "Epoch 30, Loss: 1.9812, Val Loss: 1.8266, Acc: 0.2670, Val Acc: 0.3276\n",
      "Epoch 40, Loss: 1.7520, Val Loss: 1.5762, Acc: 0.3303, Val Acc: 0.4473\n",
      "Epoch 50, Loss: 1.5820, Val Loss: 1.3695, Acc: 0.3849, Val Acc: 0.5470\n",
      "Epoch 60, Loss: 1.3677, Val Loss: 1.1842, Acc: 0.4538, Val Acc: 0.6154\n",
      "Epoch 70, Loss: 1.2019, Val Loss: 1.0193, Acc: 0.5163, Val Acc: 0.6638\n",
      "Epoch 80, Loss: 1.0667, Val Loss: 0.8771, Acc: 0.5852, Val Acc: 0.7236\n",
      "Epoch 90, Loss: 0.9805, Val Loss: 0.7805, Acc: 0.6037, Val Acc: 0.7179\n",
      "Epoch 100, Loss: 0.8840, Val Loss: 0.7134, Acc: 0.6435, Val Acc: 0.7436\n",
      "Epoch 110, Loss: 0.7968, Val Loss: 0.7161, Acc: 0.6761, Val Acc: 0.7066\n",
      "Epoch 120, Loss: 0.7165, Val Loss: 0.5883, Acc: 0.7173, Val Acc: 0.7835\n",
      "Epoch 130, Loss: 0.6826, Val Loss: 0.5458, Acc: 0.7152, Val Acc: 0.7920\n",
      "Epoch 140, Loss: 0.6292, Val Loss: 0.5259, Acc: 0.7450, Val Acc: 0.7920\n",
      "Epoch 150, Loss: 0.5580, Val Loss: 0.5287, Acc: 0.7713, Val Acc: 0.7977\n",
      "Epoch 160, Loss: 0.5766, Val Loss: 0.4897, Acc: 0.7635, Val Acc: 0.8063\n",
      "Epoch 170, Loss: 0.5432, Val Loss: 0.4900, Acc: 0.7876, Val Acc: 0.8148\n",
      "Epoch 180, Loss: 0.5089, Val Loss: 0.4607, Acc: 0.7905, Val Acc: 0.8291\n",
      "Epoch 190, Loss: 0.4916, Val Loss: 0.4468, Acc: 0.8082, Val Acc: 0.8262\n",
      "Epoch 200, Loss: 0.4726, Val Loss: 0.4358, Acc: 0.8161, Val Acc: 0.8376\n",
      "Epoch 210, Loss: 0.4465, Val Loss: 0.4196, Acc: 0.8295, Val Acc: 0.8405\n",
      "Epoch 220, Loss: 0.4270, Val Loss: 0.4076, Acc: 0.8402, Val Acc: 0.8405\n",
      "Epoch 230, Loss: 0.4307, Val Loss: 0.4089, Acc: 0.8352, Val Acc: 0.8519\n",
      "Epoch 240, Loss: 0.4003, Val Loss: 0.4028, Acc: 0.8466, Val Acc: 0.8462\n",
      "Epoch 250, Loss: 0.3862, Val Loss: 0.3925, Acc: 0.8551, Val Acc: 0.8519\n",
      "Epoch 260, Loss: 0.3716, Val Loss: 0.3728, Acc: 0.8544, Val Acc: 0.8547\n",
      "Epoch 270, Loss: 0.3674, Val Loss: 0.3717, Acc: 0.8672, Val Acc: 0.8689\n",
      "Epoch 280, Loss: 0.3621, Val Loss: 0.3934, Acc: 0.8601, Val Acc: 0.8604\n",
      "Epoch 290, Loss: 0.3451, Val Loss: 0.3700, Acc: 0.8750, Val Acc: 0.8718\n",
      "Best Validation Loss: 0.3614\n",
      "Fold Test Accuracy: 0.8528, F1-score: 0.8515, Homogeneity: 0.8337, NMI: 0.8366, ARI: 0.7456\n",
      "  Average Metrics for Threshold 0.13: {'test_accuracy': 0.8302387267904509, 'f1_score': 0.8272590056510148, 'homogeneity': 0.8131702131759772, 'nmi': 0.8174629120792046, 'ari': 0.7153237972299523}\n",
      "\n",
      "Training with threshold: 0.14\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6887, Val Loss: 2.5613, Acc: 0.0718, Val Acc: 0.0824\n",
      "Epoch 10, Loss: 2.4478, Val Loss: 2.4194, Acc: 0.1663, Val Acc: 0.2273\n",
      "Epoch 20, Loss: 2.1652, Val Loss: 2.1195, Acc: 0.2544, Val Acc: 0.2955\n",
      "Epoch 30, Loss: 1.9073, Val Loss: 1.8458, Acc: 0.3056, Val Acc: 0.3864\n",
      "Epoch 40, Loss: 1.7118, Val Loss: 1.5890, Acc: 0.3305, Val Acc: 0.4489\n",
      "Epoch 50, Loss: 1.5153, Val Loss: 1.3675, Acc: 0.3802, Val Acc: 0.5085\n",
      "Epoch 60, Loss: 1.3140, Val Loss: 1.1795, Acc: 0.4940, Val Acc: 0.6165\n",
      "Epoch 70, Loss: 1.1638, Val Loss: 1.0261, Acc: 0.5444, Val Acc: 0.6705\n",
      "Epoch 80, Loss: 1.0161, Val Loss: 0.8647, Acc: 0.5991, Val Acc: 0.6989\n",
      "Epoch 90, Loss: 0.8886, Val Loss: 0.7729, Acc: 0.6411, Val Acc: 0.7017\n",
      "Epoch 100, Loss: 0.8097, Val Loss: 0.7671, Acc: 0.6816, Val Acc: 0.6790\n",
      "Epoch 110, Loss: 0.7693, Val Loss: 0.7273, Acc: 0.6930, Val Acc: 0.6761\n",
      "Epoch 120, Loss: 0.6837, Val Loss: 0.6896, Acc: 0.7207, Val Acc: 0.6875\n",
      "Epoch 130, Loss: 0.5932, Val Loss: 0.6914, Acc: 0.7612, Val Acc: 0.6818\n",
      "Epoch 140, Loss: 0.5611, Val Loss: 0.6113, Acc: 0.7818, Val Acc: 0.7244\n",
      "Epoch 150, Loss: 0.5429, Val Loss: 0.5669, Acc: 0.7946, Val Acc: 0.7500\n",
      "Epoch 160, Loss: 0.5563, Val Loss: 0.5402, Acc: 0.7726, Val Acc: 0.7614\n",
      "Epoch 170, Loss: 0.4983, Val Loss: 0.5332, Acc: 0.8017, Val Acc: 0.7614\n",
      "Epoch 180, Loss: 0.4752, Val Loss: 0.5342, Acc: 0.8181, Val Acc: 0.7557\n",
      "Epoch 190, Loss: 0.4539, Val Loss: 0.5313, Acc: 0.8337, Val Acc: 0.7528\n",
      "Epoch 200, Loss: 0.4521, Val Loss: 0.5274, Acc: 0.8252, Val Acc: 0.7670\n",
      "Epoch 210, Loss: 0.4623, Val Loss: 0.5205, Acc: 0.8117, Val Acc: 0.7642\n",
      "Epoch 220, Loss: 0.4625, Val Loss: 0.5156, Acc: 0.8131, Val Acc: 0.7756\n",
      "Epoch 230, Loss: 0.4492, Val Loss: 0.5134, Acc: 0.8280, Val Acc: 0.7670\n",
      "Epoch 240, Loss: 0.4062, Val Loss: 0.5166, Acc: 0.8493, Val Acc: 0.7614\n",
      "Epoch 250, Loss: 0.4452, Val Loss: 0.5126, Acc: 0.8351, Val Acc: 0.7614\n",
      "Epoch 260, Loss: 0.4105, Val Loss: 0.5077, Acc: 0.8543, Val Acc: 0.7670\n",
      "Epoch 270, Loss: 0.4303, Val Loss: 0.5050, Acc: 0.8330, Val Acc: 0.7727\n",
      "Epoch 280, Loss: 0.4348, Val Loss: 0.5056, Acc: 0.8372, Val Acc: 0.7727\n",
      "Epoch 290, Loss: 0.4407, Val Loss: 0.5053, Acc: 0.8209, Val Acc: 0.7727\n",
      "Best Validation Loss: 0.5040\n",
      "Fold Test Accuracy: 0.8156, F1-score: 0.8014, Homogeneity: 0.7985, NMI: 0.8096, ARI: 0.6971\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7070, Val Loss: 2.5637, Acc: 0.0732, Val Acc: 0.0739\n",
      "Epoch 10, Loss: 2.4744, Val Loss: 2.4460, Acc: 0.1514, Val Acc: 0.2273\n",
      "Epoch 20, Loss: 2.2153, Val Loss: 2.1422, Acc: 0.2225, Val Acc: 0.2188\n",
      "Epoch 30, Loss: 1.9687, Val Loss: 1.8631, Acc: 0.2772, Val Acc: 0.3750\n",
      "Epoch 40, Loss: 1.7530, Val Loss: 1.6006, Acc: 0.3475, Val Acc: 0.4631\n",
      "Epoch 50, Loss: 1.5769, Val Loss: 1.3867, Acc: 0.4009, Val Acc: 0.5795\n",
      "Epoch 60, Loss: 1.4055, Val Loss: 1.1977, Acc: 0.4300, Val Acc: 0.6392\n",
      "Epoch 70, Loss: 1.1872, Val Loss: 1.0280, Acc: 0.5345, Val Acc: 0.6790\n",
      "Epoch 80, Loss: 1.0705, Val Loss: 0.8736, Acc: 0.5807, Val Acc: 0.7244\n",
      "Epoch 90, Loss: 0.9640, Val Loss: 0.7645, Acc: 0.6098, Val Acc: 0.7557\n",
      "Epoch 100, Loss: 0.8674, Val Loss: 0.6898, Acc: 0.6382, Val Acc: 0.7699\n",
      "Epoch 110, Loss: 0.7611, Val Loss: 0.6557, Acc: 0.6880, Val Acc: 0.7699\n",
      "Epoch 120, Loss: 0.6935, Val Loss: 0.6007, Acc: 0.7186, Val Acc: 0.7898\n",
      "Epoch 130, Loss: 0.6350, Val Loss: 0.5462, Acc: 0.7406, Val Acc: 0.7983\n",
      "Epoch 140, Loss: 0.6174, Val Loss: 0.4684, Acc: 0.7534, Val Acc: 0.8295\n",
      "Epoch 150, Loss: 0.5372, Val Loss: 0.4471, Acc: 0.7925, Val Acc: 0.8295\n",
      "Epoch 160, Loss: 0.5051, Val Loss: 0.4556, Acc: 0.7996, Val Acc: 0.8295\n",
      "Epoch 170, Loss: 0.5001, Val Loss: 0.4126, Acc: 0.8124, Val Acc: 0.8551\n",
      "Epoch 180, Loss: 0.4346, Val Loss: 0.3813, Acc: 0.8195, Val Acc: 0.8722\n",
      "Epoch 190, Loss: 0.4371, Val Loss: 0.3647, Acc: 0.8287, Val Acc: 0.8778\n",
      "Epoch 200, Loss: 0.4024, Val Loss: 0.3806, Acc: 0.8408, Val Acc: 0.8778\n",
      "Epoch 210, Loss: 0.4023, Val Loss: 0.3520, Acc: 0.8429, Val Acc: 0.8807\n",
      "Epoch 220, Loss: 0.3919, Val Loss: 0.3498, Acc: 0.8621, Val Acc: 0.8864\n",
      "Epoch 230, Loss: 0.3801, Val Loss: 0.3518, Acc: 0.8714, Val Acc: 0.8750\n",
      "Epoch 240, Loss: 0.3862, Val Loss: 0.3539, Acc: 0.8593, Val Acc: 0.8722\n",
      "Epoch 250, Loss: 0.3829, Val Loss: 0.3589, Acc: 0.8650, Val Acc: 0.8665\n",
      "Epoch 260, Loss: 0.3644, Val Loss: 0.3632, Acc: 0.8600, Val Acc: 0.8636\n",
      "Epoch 270, Loss: 0.3905, Val Loss: 0.3659, Acc: 0.8579, Val Acc: 0.8636\n",
      "Epoch 280, Loss: 0.3534, Val Loss: 0.3656, Acc: 0.8635, Val Acc: 0.8636\n",
      "Epoch 290, Loss: 0.3748, Val Loss: 0.3628, Acc: 0.8586, Val Acc: 0.8636\n",
      "Best Validation Loss: 0.3463\n",
      "Fold Test Accuracy: 0.8342, F1-score: 0.8341, Homogeneity: 0.8158, NMI: 0.8180, ARI: 0.7191\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6859, Val Loss: 2.5607, Acc: 0.0796, Val Acc: 0.1136\n",
      "Epoch 10, Loss: 2.4535, Val Loss: 2.4346, Acc: 0.1670, Val Acc: 0.2642\n",
      "Epoch 20, Loss: 2.1889, Val Loss: 2.1280, Acc: 0.2480, Val Acc: 0.3580\n",
      "Epoch 30, Loss: 1.9470, Val Loss: 1.8132, Acc: 0.3063, Val Acc: 0.3693\n",
      "Epoch 40, Loss: 1.7003, Val Loss: 1.5491, Acc: 0.3426, Val Acc: 0.4006\n",
      "Epoch 50, Loss: 1.4940, Val Loss: 1.3127, Acc: 0.4279, Val Acc: 0.5142\n",
      "Epoch 60, Loss: 1.3308, Val Loss: 1.1160, Acc: 0.4833, Val Acc: 0.6165\n",
      "Epoch 70, Loss: 1.1636, Val Loss: 0.9632, Acc: 0.5380, Val Acc: 0.6506\n",
      "Epoch 80, Loss: 1.0331, Val Loss: 0.8341, Acc: 0.5956, Val Acc: 0.6676\n",
      "Epoch 90, Loss: 0.9190, Val Loss: 0.7524, Acc: 0.6247, Val Acc: 0.6676\n",
      "Epoch 100, Loss: 0.8328, Val Loss: 0.7211, Acc: 0.6567, Val Acc: 0.6903\n",
      "Epoch 110, Loss: 0.7569, Val Loss: 0.7089, Acc: 0.6887, Val Acc: 0.6818\n",
      "Epoch 120, Loss: 0.6551, Val Loss: 0.5895, Acc: 0.7434, Val Acc: 0.7699\n",
      "Epoch 130, Loss: 0.5949, Val Loss: 0.5527, Acc: 0.7676, Val Acc: 0.7614\n",
      "Epoch 140, Loss: 0.5722, Val Loss: 0.5646, Acc: 0.7690, Val Acc: 0.7500\n",
      "Epoch 150, Loss: 0.5168, Val Loss: 0.5512, Acc: 0.7932, Val Acc: 0.7642\n",
      "Epoch 160, Loss: 0.4884, Val Loss: 0.4949, Acc: 0.8081, Val Acc: 0.7812\n",
      "Epoch 170, Loss: 0.4334, Val Loss: 0.4586, Acc: 0.8401, Val Acc: 0.8040\n",
      "Epoch 180, Loss: 0.4025, Val Loss: 0.4551, Acc: 0.8451, Val Acc: 0.8068\n",
      "Epoch 190, Loss: 0.3701, Val Loss: 0.4477, Acc: 0.8550, Val Acc: 0.8125\n",
      "Epoch 200, Loss: 0.3849, Val Loss: 0.4451, Acc: 0.8529, Val Acc: 0.8153\n",
      "Epoch 210, Loss: 0.3617, Val Loss: 0.4245, Acc: 0.8699, Val Acc: 0.8267\n",
      "Epoch 220, Loss: 0.3634, Val Loss: 0.4249, Acc: 0.8586, Val Acc: 0.8267\n",
      "Epoch 230, Loss: 0.3439, Val Loss: 0.4293, Acc: 0.8785, Val Acc: 0.8324\n",
      "Epoch 240, Loss: 0.3306, Val Loss: 0.4315, Acc: 0.8778, Val Acc: 0.8324\n",
      "Epoch 250, Loss: 0.3662, Val Loss: 0.4330, Acc: 0.8600, Val Acc: 0.8324\n",
      "Epoch 260, Loss: 0.3499, Val Loss: 0.4391, Acc: 0.8664, Val Acc: 0.8239\n",
      "Epoch 270, Loss: 0.3235, Val Loss: 0.4360, Acc: 0.8792, Val Acc: 0.8295\n",
      "Epoch 280, Loss: 0.3452, Val Loss: 0.4349, Acc: 0.8763, Val Acc: 0.8324\n",
      "Epoch 290, Loss: 0.3446, Val Loss: 0.4334, Acc: 0.8714, Val Acc: 0.8324\n",
      "Best Validation Loss: 0.4225\n",
      "Fold Test Accuracy: 0.8316, F1-score: 0.8284, Homogeneity: 0.8073, NMI: 0.8115, ARI: 0.7121\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6962, Val Loss: 2.5581, Acc: 0.0753, Val Acc: 0.1108\n",
      "Epoch 10, Loss: 2.4840, Val Loss: 2.4211, Acc: 0.1649, Val Acc: 0.2670\n",
      "Epoch 20, Loss: 2.2033, Val Loss: 2.1099, Acc: 0.2253, Val Acc: 0.3153\n",
      "Epoch 30, Loss: 1.9327, Val Loss: 1.8370, Acc: 0.3035, Val Acc: 0.3949\n",
      "Epoch 40, Loss: 1.7380, Val Loss: 1.5658, Acc: 0.3205, Val Acc: 0.4602\n",
      "Epoch 50, Loss: 1.5144, Val Loss: 1.3280, Acc: 0.4058, Val Acc: 0.5653\n",
      "Epoch 60, Loss: 1.3339, Val Loss: 1.1536, Acc: 0.4776, Val Acc: 0.6193\n",
      "Epoch 70, Loss: 1.1955, Val Loss: 0.9908, Acc: 0.5146, Val Acc: 0.6790\n",
      "Epoch 80, Loss: 1.0282, Val Loss: 0.8492, Acc: 0.5878, Val Acc: 0.7159\n",
      "Epoch 90, Loss: 0.9143, Val Loss: 0.7333, Acc: 0.6397, Val Acc: 0.7472\n",
      "Epoch 100, Loss: 0.8075, Val Loss: 0.6767, Acc: 0.6809, Val Acc: 0.7443\n",
      "Epoch 110, Loss: 0.7512, Val Loss: 0.6382, Acc: 0.7050, Val Acc: 0.7358\n",
      "Epoch 120, Loss: 0.6934, Val Loss: 0.5797, Acc: 0.7164, Val Acc: 0.7841\n",
      "Epoch 130, Loss: 0.6289, Val Loss: 0.5436, Acc: 0.7463, Val Acc: 0.7926\n",
      "Epoch 140, Loss: 0.5690, Val Loss: 0.5099, Acc: 0.7711, Val Acc: 0.7869\n",
      "Epoch 150, Loss: 0.5153, Val Loss: 0.5809, Acc: 0.7967, Val Acc: 0.7528\n",
      "Epoch 160, Loss: 0.5057, Val Loss: 0.4825, Acc: 0.7974, Val Acc: 0.7955\n",
      "Epoch 170, Loss: 0.5126, Val Loss: 0.4492, Acc: 0.8031, Val Acc: 0.8153\n",
      "Epoch 180, Loss: 0.4387, Val Loss: 0.4464, Acc: 0.8436, Val Acc: 0.8182\n",
      "Epoch 190, Loss: 0.4301, Val Loss: 0.4427, Acc: 0.8394, Val Acc: 0.8267\n",
      "Epoch 200, Loss: 0.4433, Val Loss: 0.4313, Acc: 0.8216, Val Acc: 0.8210\n",
      "Epoch 210, Loss: 0.4099, Val Loss: 0.4247, Acc: 0.8543, Val Acc: 0.8239\n",
      "Epoch 220, Loss: 0.3854, Val Loss: 0.4281, Acc: 0.8522, Val Acc: 0.8267\n",
      "Epoch 230, Loss: 0.4004, Val Loss: 0.4159, Acc: 0.8429, Val Acc: 0.8352\n",
      "Epoch 240, Loss: 0.3804, Val Loss: 0.4191, Acc: 0.8593, Val Acc: 0.8324\n",
      "Epoch 250, Loss: 0.3621, Val Loss: 0.4171, Acc: 0.8685, Val Acc: 0.8352\n",
      "Epoch 260, Loss: 0.3670, Val Loss: 0.4189, Acc: 0.8607, Val Acc: 0.8324\n",
      "Epoch 270, Loss: 0.3655, Val Loss: 0.4202, Acc: 0.8600, Val Acc: 0.8324\n",
      "Epoch 280, Loss: 0.3499, Val Loss: 0.4185, Acc: 0.8699, Val Acc: 0.8352\n",
      "Epoch 290, Loss: 0.3664, Val Loss: 0.4167, Acc: 0.8813, Val Acc: 0.8324\n",
      "Best Validation Loss: 0.4150\n",
      "Fold Test Accuracy: 0.8263, F1-score: 0.8233, Homogeneity: 0.8114, NMI: 0.8164, ARI: 0.7104\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7127, Val Loss: 2.5580, Acc: 0.0753, Val Acc: 0.1026\n",
      "Epoch 10, Loss: 2.4771, Val Loss: 2.4466, Acc: 0.1605, Val Acc: 0.2650\n",
      "Epoch 20, Loss: 2.2042, Val Loss: 2.1299, Acc: 0.2322, Val Acc: 0.3647\n",
      "Epoch 30, Loss: 1.9462, Val Loss: 1.8406, Acc: 0.3004, Val Acc: 0.4046\n",
      "Epoch 40, Loss: 1.7583, Val Loss: 1.5752, Acc: 0.3395, Val Acc: 0.5071\n",
      "Epoch 50, Loss: 1.5046, Val Loss: 1.3391, Acc: 0.4190, Val Acc: 0.6011\n",
      "Epoch 60, Loss: 1.3398, Val Loss: 1.1472, Acc: 0.4815, Val Acc: 0.6325\n",
      "Epoch 70, Loss: 1.1842, Val Loss: 0.9912, Acc: 0.5312, Val Acc: 0.6724\n",
      "Epoch 80, Loss: 1.0501, Val Loss: 0.8350, Acc: 0.5639, Val Acc: 0.7293\n",
      "Epoch 90, Loss: 0.9169, Val Loss: 0.7452, Acc: 0.6491, Val Acc: 0.7322\n",
      "Epoch 100, Loss: 0.8443, Val Loss: 0.7662, Acc: 0.6591, Val Acc: 0.6923\n",
      "Epoch 110, Loss: 0.7285, Val Loss: 0.6698, Acc: 0.7088, Val Acc: 0.7293\n",
      "Epoch 120, Loss: 0.6640, Val Loss: 0.6041, Acc: 0.7337, Val Acc: 0.7721\n",
      "Epoch 130, Loss: 0.6402, Val Loss: 0.5380, Acc: 0.7479, Val Acc: 0.7920\n",
      "Epoch 140, Loss: 0.5785, Val Loss: 0.5046, Acc: 0.7599, Val Acc: 0.8006\n",
      "Epoch 150, Loss: 0.5296, Val Loss: 0.4884, Acc: 0.7947, Val Acc: 0.8063\n",
      "Epoch 160, Loss: 0.4984, Val Loss: 0.4809, Acc: 0.8004, Val Acc: 0.8034\n",
      "Epoch 170, Loss: 0.4855, Val Loss: 0.4610, Acc: 0.8146, Val Acc: 0.8291\n",
      "Epoch 180, Loss: 0.4658, Val Loss: 0.4454, Acc: 0.8097, Val Acc: 0.8490\n",
      "Epoch 190, Loss: 0.4214, Val Loss: 0.4225, Acc: 0.8303, Val Acc: 0.8490\n",
      "Epoch 200, Loss: 0.4147, Val Loss: 0.4069, Acc: 0.8395, Val Acc: 0.8632\n",
      "Epoch 210, Loss: 0.4356, Val Loss: 0.3978, Acc: 0.8359, Val Acc: 0.8462\n",
      "Epoch 220, Loss: 0.3795, Val Loss: 0.4018, Acc: 0.8551, Val Acc: 0.8575\n",
      "Epoch 230, Loss: 0.3797, Val Loss: 0.3932, Acc: 0.8587, Val Acc: 0.8547\n",
      "Epoch 240, Loss: 0.3821, Val Loss: 0.3889, Acc: 0.8551, Val Acc: 0.8575\n",
      "Epoch 250, Loss: 0.3708, Val Loss: 0.3853, Acc: 0.8665, Val Acc: 0.8632\n",
      "Epoch 260, Loss: 0.3723, Val Loss: 0.3824, Acc: 0.8572, Val Acc: 0.8632\n",
      "Epoch 270, Loss: 0.3488, Val Loss: 0.3803, Acc: 0.8793, Val Acc: 0.8746\n",
      "Epoch 280, Loss: 0.3162, Val Loss: 0.3784, Acc: 0.8970, Val Acc: 0.8689\n",
      "Epoch 290, Loss: 0.3434, Val Loss: 0.3793, Acc: 0.8672, Val Acc: 0.8689\n",
      "Best Validation Loss: 0.3775\n",
      "Fold Test Accuracy: 0.8249, F1-score: 0.8201, Homogeneity: 0.8063, NMI: 0.8111, ARI: 0.7083\n",
      "  Average Metrics for Threshold 0.14: {'test_accuracy': 0.826525198938992, 'f1_score': 0.8214264516619924, 'homogeneity': 0.8078577030529314, 'nmi': 0.8133153042788477, 'ari': 0.7093959840703302}\n",
      "\n",
      "Training with threshold: 0.15000000000000002\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6974, Val Loss: 2.5595, Acc: 0.0739, Val Acc: 0.0938\n",
      "Epoch 10, Loss: 2.4670, Val Loss: 2.4550, Acc: 0.1606, Val Acc: 0.3068\n",
      "Epoch 20, Loss: 2.2053, Val Loss: 2.1500, Acc: 0.2367, Val Acc: 0.3068\n",
      "Epoch 30, Loss: 1.9426, Val Loss: 1.8491, Acc: 0.2964, Val Acc: 0.3665\n",
      "Epoch 40, Loss: 1.6764, Val Loss: 1.5970, Acc: 0.3795, Val Acc: 0.4006\n",
      "Epoch 50, Loss: 1.4748, Val Loss: 1.3390, Acc: 0.4144, Val Acc: 0.5597\n",
      "Epoch 60, Loss: 1.3243, Val Loss: 1.1252, Acc: 0.4819, Val Acc: 0.6591\n",
      "Epoch 70, Loss: 1.1550, Val Loss: 0.9633, Acc: 0.5402, Val Acc: 0.6960\n",
      "Epoch 80, Loss: 1.0645, Val Loss: 0.8190, Acc: 0.5700, Val Acc: 0.7074\n",
      "Epoch 90, Loss: 0.8739, Val Loss: 0.7124, Acc: 0.6560, Val Acc: 0.7386\n",
      "Epoch 100, Loss: 0.8299, Val Loss: 0.6579, Acc: 0.6581, Val Acc: 0.7443\n",
      "Epoch 110, Loss: 0.6944, Val Loss: 0.6463, Acc: 0.7207, Val Acc: 0.7273\n",
      "Epoch 120, Loss: 0.6391, Val Loss: 0.6691, Acc: 0.7463, Val Acc: 0.7045\n",
      "Epoch 130, Loss: 0.6168, Val Loss: 0.5836, Acc: 0.7512, Val Acc: 0.7415\n",
      "Epoch 140, Loss: 0.5922, Val Loss: 0.5238, Acc: 0.7719, Val Acc: 0.7642\n",
      "Epoch 150, Loss: 0.5812, Val Loss: 0.5194, Acc: 0.7768, Val Acc: 0.7727\n",
      "Epoch 160, Loss: 0.5437, Val Loss: 0.4817, Acc: 0.7925, Val Acc: 0.7898\n",
      "Epoch 170, Loss: 0.5469, Val Loss: 0.4921, Acc: 0.7825, Val Acc: 0.7727\n",
      "Epoch 180, Loss: 0.4985, Val Loss: 0.4758, Acc: 0.8145, Val Acc: 0.7869\n",
      "Epoch 190, Loss: 0.4978, Val Loss: 0.4741, Acc: 0.8102, Val Acc: 0.7983\n",
      "Epoch 200, Loss: 0.4688, Val Loss: 0.4711, Acc: 0.8173, Val Acc: 0.7869\n",
      "Epoch 210, Loss: 0.4668, Val Loss: 0.4621, Acc: 0.8131, Val Acc: 0.8011\n",
      "Epoch 220, Loss: 0.4681, Val Loss: 0.4565, Acc: 0.8138, Val Acc: 0.8011\n",
      "Epoch 230, Loss: 0.4619, Val Loss: 0.4535, Acc: 0.8117, Val Acc: 0.8125\n",
      "Epoch 240, Loss: 0.4425, Val Loss: 0.4580, Acc: 0.8266, Val Acc: 0.8011\n",
      "Epoch 250, Loss: 0.4543, Val Loss: 0.4532, Acc: 0.8209, Val Acc: 0.8097\n",
      "Epoch 260, Loss: 0.4459, Val Loss: 0.4512, Acc: 0.8308, Val Acc: 0.8097\n",
      "Epoch 270, Loss: 0.4237, Val Loss: 0.4507, Acc: 0.8337, Val Acc: 0.8153\n",
      "Epoch 280, Loss: 0.4415, Val Loss: 0.4518, Acc: 0.8344, Val Acc: 0.8153\n",
      "Epoch 290, Loss: 0.4343, Val Loss: 0.4500, Acc: 0.8408, Val Acc: 0.8125\n",
      "Best Validation Loss: 0.4484\n",
      "Fold Test Accuracy: 0.8103, F1-score: 0.8032, Homogeneity: 0.7878, NMI: 0.7958, ARI: 0.6841\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7167, Val Loss: 2.5631, Acc: 0.0711, Val Acc: 0.0994\n",
      "Epoch 10, Loss: 2.4564, Val Loss: 2.4327, Acc: 0.1528, Val Acc: 0.2386\n",
      "Epoch 20, Loss: 2.1539, Val Loss: 2.1060, Acc: 0.2466, Val Acc: 0.3040\n",
      "Epoch 30, Loss: 1.9309, Val Loss: 1.7920, Acc: 0.2672, Val Acc: 0.3778\n",
      "Epoch 40, Loss: 1.6726, Val Loss: 1.5343, Acc: 0.3547, Val Acc: 0.5256\n",
      "Epoch 50, Loss: 1.4843, Val Loss: 1.2998, Acc: 0.4065, Val Acc: 0.6250\n",
      "Epoch 60, Loss: 1.3014, Val Loss: 1.1483, Acc: 0.4911, Val Acc: 0.6420\n",
      "Epoch 70, Loss: 1.1647, Val Loss: 0.9846, Acc: 0.5195, Val Acc: 0.6761\n",
      "Epoch 80, Loss: 1.0060, Val Loss: 0.8261, Acc: 0.5899, Val Acc: 0.7415\n",
      "Epoch 90, Loss: 0.8853, Val Loss: 0.7412, Acc: 0.6240, Val Acc: 0.7670\n",
      "Epoch 100, Loss: 0.8272, Val Loss: 0.7064, Acc: 0.6588, Val Acc: 0.7415\n",
      "Epoch 110, Loss: 0.7849, Val Loss: 0.6476, Acc: 0.6887, Val Acc: 0.7642\n",
      "Epoch 120, Loss: 0.6832, Val Loss: 0.5707, Acc: 0.7235, Val Acc: 0.7955\n",
      "Epoch 130, Loss: 0.6297, Val Loss: 0.5137, Acc: 0.7413, Val Acc: 0.8097\n",
      "Epoch 140, Loss: 0.6414, Val Loss: 0.5273, Acc: 0.7534, Val Acc: 0.7841\n",
      "Epoch 150, Loss: 0.5202, Val Loss: 0.4605, Acc: 0.7960, Val Acc: 0.8239\n",
      "Epoch 160, Loss: 0.4999, Val Loss: 0.4395, Acc: 0.7989, Val Acc: 0.8295\n",
      "Epoch 170, Loss: 0.4448, Val Loss: 0.4270, Acc: 0.8344, Val Acc: 0.8352\n",
      "Epoch 180, Loss: 0.4435, Val Loss: 0.4577, Acc: 0.8316, Val Acc: 0.8153\n",
      "Epoch 190, Loss: 0.4400, Val Loss: 0.4177, Acc: 0.8252, Val Acc: 0.8324\n",
      "Epoch 200, Loss: 0.4260, Val Loss: 0.3980, Acc: 0.8337, Val Acc: 0.8551\n",
      "Epoch 210, Loss: 0.4232, Val Loss: 0.4040, Acc: 0.8380, Val Acc: 0.8466\n",
      "Epoch 220, Loss: 0.3966, Val Loss: 0.4059, Acc: 0.8515, Val Acc: 0.8494\n",
      "Epoch 230, Loss: 0.3593, Val Loss: 0.4101, Acc: 0.8593, Val Acc: 0.8494\n",
      "Epoch 240, Loss: 0.3845, Val Loss: 0.4128, Acc: 0.8443, Val Acc: 0.8466\n",
      "Epoch 250, Loss: 0.3943, Val Loss: 0.4146, Acc: 0.8643, Val Acc: 0.8438\n",
      "Epoch 260, Loss: 0.3840, Val Loss: 0.4125, Acc: 0.8515, Val Acc: 0.8438\n",
      "Epoch 270, Loss: 0.3880, Val Loss: 0.4146, Acc: 0.8465, Val Acc: 0.8409\n",
      "Epoch 280, Loss: 0.3964, Val Loss: 0.4118, Acc: 0.8550, Val Acc: 0.8438\n",
      "Epoch 290, Loss: 0.3764, Val Loss: 0.4117, Acc: 0.8614, Val Acc: 0.8438\n",
      "Best Validation Loss: 0.3980\n",
      "Fold Test Accuracy: 0.8223, F1-score: 0.8204, Homogeneity: 0.8049, NMI: 0.8078, ARI: 0.7032\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7223, Val Loss: 2.5591, Acc: 0.0739, Val Acc: 0.1080\n",
      "Epoch 10, Loss: 2.4801, Val Loss: 2.4310, Acc: 0.1528, Val Acc: 0.2017\n",
      "Epoch 20, Loss: 2.2118, Val Loss: 2.1518, Acc: 0.2331, Val Acc: 0.2983\n",
      "Epoch 30, Loss: 1.9533, Val Loss: 1.8299, Acc: 0.3085, Val Acc: 0.3494\n",
      "Epoch 40, Loss: 1.6991, Val Loss: 1.5368, Acc: 0.3547, Val Acc: 0.4318\n",
      "Epoch 50, Loss: 1.4921, Val Loss: 1.3031, Acc: 0.4144, Val Acc: 0.5426\n",
      "Epoch 60, Loss: 1.3159, Val Loss: 1.1259, Acc: 0.4719, Val Acc: 0.6165\n",
      "Epoch 70, Loss: 1.1796, Val Loss: 0.9804, Acc: 0.5274, Val Acc: 0.6080\n",
      "Epoch 80, Loss: 1.0380, Val Loss: 0.8680, Acc: 0.5792, Val Acc: 0.6648\n",
      "Epoch 90, Loss: 0.9018, Val Loss: 0.8018, Acc: 0.6588, Val Acc: 0.7017\n",
      "Epoch 100, Loss: 0.8284, Val Loss: 0.7259, Acc: 0.6667, Val Acc: 0.7045\n",
      "Epoch 110, Loss: 0.7386, Val Loss: 0.7226, Acc: 0.7043, Val Acc: 0.6903\n",
      "Epoch 120, Loss: 0.6871, Val Loss: 0.6902, Acc: 0.7242, Val Acc: 0.7045\n",
      "Epoch 130, Loss: 0.6319, Val Loss: 0.5981, Acc: 0.7505, Val Acc: 0.7500\n",
      "Epoch 140, Loss: 0.6115, Val Loss: 0.5799, Acc: 0.7619, Val Acc: 0.7699\n",
      "Epoch 150, Loss: 0.5911, Val Loss: 0.5592, Acc: 0.7733, Val Acc: 0.7727\n",
      "Epoch 160, Loss: 0.5665, Val Loss: 0.5449, Acc: 0.7839, Val Acc: 0.7784\n",
      "Epoch 170, Loss: 0.5325, Val Loss: 0.5399, Acc: 0.7989, Val Acc: 0.7812\n",
      "Epoch 180, Loss: 0.5087, Val Loss: 0.5163, Acc: 0.8067, Val Acc: 0.7898\n",
      "Epoch 190, Loss: 0.4812, Val Loss: 0.5148, Acc: 0.8230, Val Acc: 0.7955\n",
      "Epoch 200, Loss: 0.4883, Val Loss: 0.5043, Acc: 0.8195, Val Acc: 0.7926\n",
      "Epoch 210, Loss: 0.4419, Val Loss: 0.5012, Acc: 0.8387, Val Acc: 0.7926\n",
      "Epoch 220, Loss: 0.4535, Val Loss: 0.4896, Acc: 0.8330, Val Acc: 0.7983\n",
      "Epoch 230, Loss: 0.4144, Val Loss: 0.4835, Acc: 0.8387, Val Acc: 0.8011\n",
      "Epoch 240, Loss: 0.3991, Val Loss: 0.4834, Acc: 0.8536, Val Acc: 0.8040\n",
      "Epoch 250, Loss: 0.4070, Val Loss: 0.4833, Acc: 0.8486, Val Acc: 0.7955\n",
      "Epoch 260, Loss: 0.3853, Val Loss: 0.4805, Acc: 0.8571, Val Acc: 0.8040\n",
      "Epoch 270, Loss: 0.3822, Val Loss: 0.4809, Acc: 0.8600, Val Acc: 0.7983\n",
      "Epoch 280, Loss: 0.3946, Val Loss: 0.4794, Acc: 0.8543, Val Acc: 0.7926\n",
      "Epoch 290, Loss: 0.3934, Val Loss: 0.4829, Acc: 0.8536, Val Acc: 0.7955\n",
      "Best Validation Loss: 0.4785\n",
      "Fold Test Accuracy: 0.8196, F1-score: 0.8174, Homogeneity: 0.7999, NMI: 0.8036, ARI: 0.6980\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6839, Val Loss: 2.5622, Acc: 0.0924, Val Acc: 0.0966\n",
      "Epoch 10, Loss: 2.4654, Val Loss: 2.4187, Acc: 0.1613, Val Acc: 0.3011\n",
      "Epoch 20, Loss: 2.1729, Val Loss: 2.0880, Acc: 0.2331, Val Acc: 0.3011\n",
      "Epoch 30, Loss: 1.9128, Val Loss: 1.8020, Acc: 0.2985, Val Acc: 0.3949\n",
      "Epoch 40, Loss: 1.6874, Val Loss: 1.5612, Acc: 0.3397, Val Acc: 0.4205\n",
      "Epoch 50, Loss: 1.5019, Val Loss: 1.3327, Acc: 0.4144, Val Acc: 0.5426\n",
      "Epoch 60, Loss: 1.3173, Val Loss: 1.1409, Acc: 0.4542, Val Acc: 0.6193\n",
      "Epoch 70, Loss: 1.1844, Val Loss: 0.9823, Acc: 0.5338, Val Acc: 0.6506\n",
      "Epoch 80, Loss: 1.0313, Val Loss: 0.8425, Acc: 0.5814, Val Acc: 0.6932\n",
      "Epoch 90, Loss: 0.9395, Val Loss: 0.7456, Acc: 0.6041, Val Acc: 0.7102\n",
      "Epoch 100, Loss: 0.8122, Val Loss: 0.7466, Acc: 0.6766, Val Acc: 0.6875\n",
      "Epoch 110, Loss: 0.7599, Val Loss: 0.7377, Acc: 0.6923, Val Acc: 0.6989\n",
      "Epoch 120, Loss: 0.7415, Val Loss: 0.6650, Acc: 0.6937, Val Acc: 0.7216\n",
      "Epoch 130, Loss: 0.7007, Val Loss: 0.6094, Acc: 0.7214, Val Acc: 0.7528\n",
      "Epoch 140, Loss: 0.6321, Val Loss: 0.5901, Acc: 0.7427, Val Acc: 0.7699\n",
      "Epoch 150, Loss: 0.6548, Val Loss: 0.5774, Acc: 0.7335, Val Acc: 0.7585\n",
      "Epoch 160, Loss: 0.6014, Val Loss: 0.5553, Acc: 0.7669, Val Acc: 0.7784\n",
      "Epoch 170, Loss: 0.5702, Val Loss: 0.5361, Acc: 0.7704, Val Acc: 0.7983\n",
      "Epoch 180, Loss: 0.5436, Val Loss: 0.5253, Acc: 0.7896, Val Acc: 0.7898\n",
      "Epoch 190, Loss: 0.5068, Val Loss: 0.5092, Acc: 0.8109, Val Acc: 0.7955\n",
      "Epoch 200, Loss: 0.5132, Val Loss: 0.4863, Acc: 0.7982, Val Acc: 0.8125\n",
      "Epoch 210, Loss: 0.4680, Val Loss: 0.5044, Acc: 0.8266, Val Acc: 0.7869\n",
      "Epoch 220, Loss: 0.4746, Val Loss: 0.4970, Acc: 0.8152, Val Acc: 0.7898\n",
      "Epoch 230, Loss: 0.4596, Val Loss: 0.4845, Acc: 0.8216, Val Acc: 0.8040\n",
      "Epoch 240, Loss: 0.4693, Val Loss: 0.4716, Acc: 0.8181, Val Acc: 0.8153\n",
      "Epoch 250, Loss: 0.4543, Val Loss: 0.4633, Acc: 0.8223, Val Acc: 0.8239\n",
      "Epoch 260, Loss: 0.4526, Val Loss: 0.4688, Acc: 0.8216, Val Acc: 0.8153\n",
      "Epoch 270, Loss: 0.4524, Val Loss: 0.4685, Acc: 0.8202, Val Acc: 0.8182\n",
      "Epoch 280, Loss: 0.4520, Val Loss: 0.4687, Acc: 0.8316, Val Acc: 0.8125\n",
      "Epoch 290, Loss: 0.4635, Val Loss: 0.4676, Acc: 0.8230, Val Acc: 0.8125\n",
      "Best Validation Loss: 0.4627\n",
      "Fold Test Accuracy: 0.8103, F1-score: 0.8047, Homogeneity: 0.7946, NMI: 0.8003, ARI: 0.6908\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7336, Val Loss: 2.5606, Acc: 0.0703, Val Acc: 0.0912\n",
      "Epoch 10, Loss: 2.4635, Val Loss: 2.4309, Acc: 0.1534, Val Acc: 0.2393\n",
      "Epoch 20, Loss: 2.2016, Val Loss: 2.1306, Acc: 0.2358, Val Acc: 0.3020\n",
      "Epoch 30, Loss: 1.9766, Val Loss: 1.8543, Acc: 0.2798, Val Acc: 0.3732\n",
      "Epoch 40, Loss: 1.7540, Val Loss: 1.5762, Acc: 0.3303, Val Acc: 0.4672\n",
      "Epoch 50, Loss: 1.5433, Val Loss: 1.3386, Acc: 0.4091, Val Acc: 0.5470\n",
      "Epoch 60, Loss: 1.3392, Val Loss: 1.1367, Acc: 0.4858, Val Acc: 0.6011\n",
      "Epoch 70, Loss: 1.1767, Val Loss: 0.9771, Acc: 0.5206, Val Acc: 0.6809\n",
      "Epoch 80, Loss: 1.0504, Val Loss: 0.8506, Acc: 0.5966, Val Acc: 0.6980\n",
      "Epoch 90, Loss: 0.9331, Val Loss: 0.7621, Acc: 0.6214, Val Acc: 0.7208\n",
      "Epoch 100, Loss: 0.8676, Val Loss: 0.6853, Acc: 0.6435, Val Acc: 0.7379\n",
      "Epoch 110, Loss: 0.7509, Val Loss: 0.6349, Acc: 0.6996, Val Acc: 0.7521\n",
      "Epoch 120, Loss: 0.6745, Val Loss: 0.5887, Acc: 0.7180, Val Acc: 0.7749\n",
      "Epoch 130, Loss: 0.6286, Val Loss: 0.5666, Acc: 0.7486, Val Acc: 0.7692\n",
      "Epoch 140, Loss: 0.5792, Val Loss: 0.5109, Acc: 0.7621, Val Acc: 0.7892\n",
      "Epoch 150, Loss: 0.5409, Val Loss: 0.4805, Acc: 0.7685, Val Acc: 0.8063\n",
      "Epoch 160, Loss: 0.5168, Val Loss: 0.4801, Acc: 0.7827, Val Acc: 0.8148\n",
      "Epoch 170, Loss: 0.4686, Val Loss: 0.4681, Acc: 0.8189, Val Acc: 0.8319\n",
      "Epoch 180, Loss: 0.4546, Val Loss: 0.4304, Acc: 0.8310, Val Acc: 0.8291\n",
      "Epoch 190, Loss: 0.4246, Val Loss: 0.4220, Acc: 0.8366, Val Acc: 0.8462\n",
      "Epoch 200, Loss: 0.4238, Val Loss: 0.3987, Acc: 0.8345, Val Acc: 0.8376\n",
      "Epoch 210, Loss: 0.4199, Val Loss: 0.3862, Acc: 0.8374, Val Acc: 0.8405\n",
      "Epoch 220, Loss: 0.3632, Val Loss: 0.3768, Acc: 0.8693, Val Acc: 0.8519\n",
      "Epoch 230, Loss: 0.3495, Val Loss: 0.3743, Acc: 0.8643, Val Acc: 0.8547\n",
      "Epoch 240, Loss: 0.3178, Val Loss: 0.3703, Acc: 0.8757, Val Acc: 0.8604\n",
      "Epoch 250, Loss: 0.3418, Val Loss: 0.3653, Acc: 0.8565, Val Acc: 0.8718\n",
      "Epoch 260, Loss: 0.3227, Val Loss: 0.3606, Acc: 0.8757, Val Acc: 0.8661\n",
      "Epoch 270, Loss: 0.3183, Val Loss: 0.3508, Acc: 0.8899, Val Acc: 0.8746\n",
      "Epoch 280, Loss: 0.2669, Val Loss: 0.3555, Acc: 0.8984, Val Acc: 0.8746\n",
      "Epoch 290, Loss: 0.2914, Val Loss: 0.3504, Acc: 0.8942, Val Acc: 0.8661\n",
      "Best Validation Loss: 0.3459\n",
      "Fold Test Accuracy: 0.8607, F1-score: 0.8616, Homogeneity: 0.8362, NMI: 0.8373, ARI: 0.7540\n",
      "  Average Metrics for Threshold 0.15: {'test_accuracy': 0.8246684350132625, 'f1_score': 0.821457292727463, 'homogeneity': 0.8047022605747933, 'nmi': 0.8089550344816325, 'ari': 0.7060501268178164}\n",
      "\n",
      "Best Threshold: 0.11, Best Average Test Accuracy: 0.8350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.05: {'test_accuracy': 0.83315649867374,\n",
       "  'f1_score': 0.8308637093296243,\n",
       "  'homogeneity': 0.8141059242287847,\n",
       "  'nmi': 0.8181186599402427,\n",
       "  'ari': 0.7172208226752821},\n",
       " 0.060000000000000005: {'test_accuracy': 0.8201591511936339,\n",
       "  'f1_score': 0.8161126123801405,\n",
       "  'homogeneity': 0.8096019053065527,\n",
       "  'nmi': 0.8145921231620598,\n",
       "  'ari': 0.7062671719817598},\n",
       " 0.07: {'test_accuracy': 0.8209549071618036,\n",
       "  'f1_score': 0.8161785833741744,\n",
       "  'homogeneity': 0.8067399040598993,\n",
       "  'nmi': 0.8120419746911871,\n",
       "  'ari': 0.7041854207446072},\n",
       " 0.08000000000000002: {'test_accuracy': 0.8196286472148542,\n",
       "  'f1_score': 0.8133218716156844,\n",
       "  'homogeneity': 0.8050171970294876,\n",
       "  'nmi': 0.8113834367711013,\n",
       "  'ari': 0.702723960809965},\n",
       " 0.09000000000000001: {'test_accuracy': 0.8225464190981432,\n",
       "  'f1_score': 0.8170486219640237,\n",
       "  'homogeneity': 0.8073525852989297,\n",
       "  'nmi': 0.8132736260411777,\n",
       "  'ari': 0.7061775795412505},\n",
       " 0.1: {'test_accuracy': 0.8305039787798408,\n",
       "  'f1_score': 0.8270866334632359,\n",
       "  'homogeneity': 0.8109318240224498,\n",
       "  'nmi': 0.8152595877389042,\n",
       "  'ari': 0.7134363392038657},\n",
       " 0.11000000000000001: {'test_accuracy': 0.8350132625994695,\n",
       "  'f1_score': 0.8319859130331559,\n",
       "  'homogeneity': 0.813306898961288,\n",
       "  'nmi': 0.8177960166511999,\n",
       "  'ari': 0.7189394486859011},\n",
       " 0.12000000000000001: {'test_accuracy': 0.8302387267904511,\n",
       "  'f1_score': 0.8268156844863487,\n",
       "  'homogeneity': 0.8139876810125685,\n",
       "  'nmi': 0.8186019029155048,\n",
       "  'ari': 0.7166031583204625},\n",
       " 0.13: {'test_accuracy': 0.8302387267904509,\n",
       "  'f1_score': 0.8272590056510148,\n",
       "  'homogeneity': 0.8131702131759772,\n",
       "  'nmi': 0.8174629120792046,\n",
       "  'ari': 0.7153237972299523},\n",
       " 0.14: {'test_accuracy': 0.826525198938992,\n",
       "  'f1_score': 0.8214264516619924,\n",
       "  'homogeneity': 0.8078577030529314,\n",
       "  'nmi': 0.8133153042788477,\n",
       "  'ari': 0.7093959840703302},\n",
       " 0.15000000000000002: {'test_accuracy': 0.8246684350132625,\n",
       "  'f1_score': 0.821457292727463,\n",
       "  'homogeneity': 0.8047022605747933,\n",
       "  'nmi': 0.8089550344816325,\n",
       "  'ari': 0.7060501268178164}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_graphs_and_setup_model_cv(embedding, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T09:14:36.892801Z",
     "iopub.status.busy": "2025-02-22T09:14:36.892509Z",
     "iopub.status.idle": "2025-02-22T09:14:38.708690Z",
     "shell.execute_reply": "2025-02-22T09:14:38.707740Z",
     "shell.execute_reply.started": "2025-02-22T09:14:36.892780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped file saved at: /kaggle/working/kaggle_working_dir.zip\n"
     ]
    }
   ],
   "source": [
    "zip_filename = \"/kaggle/working/kaggle_working_dir.zip\"\n",
    "\n",
    "# Zip the entire working directory\n",
    "shutil.make_archive(zip_filename.replace(\".zip\", \"\"), 'zip', \"/kaggle/working\")\n",
    "\n",
    "print(f\"Zipped file saved at: {zip_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6527063,
     "sourceId": 10549111,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6594881,
     "sourceId": 10650466,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6670472,
     "sourceId": 10754815,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
